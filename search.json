[
  {
    "objectID": "projects/stochastic.html",
    "href": "projects/stochastic.html",
    "title": "Optimisation stochastique",
    "section": "",
    "text": "My work in stochastic optimization focuses on decision-making under uncertainty, where randomness is not a nuisance to eliminate, but a structural component of the problem.\nRather than replacing uncertainty with averages, stochastic models explicitly represent variability, risk, and information asymmetry. This perspective is essential in operations, finance, energy, and supply chain systems, where decisions must remain viable across a range of plausible futures.\nThe two projects presented here explore this idea from complementary angles:\none through analytical stochastic optimization, the other through sequential decision-making and simulation.\n\n\n\n\n\nThe newsvendor problem is one of the simplest yet most instructive models in stochastic optimization. A decision-maker must choose an order quantity before demand is realized, balancing the risk of overstocking against the risk of stockouts.\nDespite its simplicity, the model captures a fundamental trade-off present in many industrial systems: &gt; deciding under uncertainty with irreversible consequences.\nThis project revisits the classical formulation and extends it to analyze the value of stochastic modeling compared to deterministic approximations. :contentReferenceoaicite:1\n\n\n\n\nDemand is modeled as a random variable with known distribution. The expected profit function integrates: - revenue from sold units, - salvage value of unsold inventory, - ordering cost.\nBy explicitly differentiating the expected profit, the optimal order quantity is obtained as a quantile of the demand distribution, known as the critical fractile.\nA key insight emerges immediately: &gt; the optimal decision depends on the entire distribution, not just its mean.\nThis result holds even when the demand distribution is truncated or bounded, highlighting the robustness of the stochastic formulation. :contentReferenceoaicite:2\n\n\n\n\nTo assess the practical impact of stochastic modeling, the optimal stochastic decision is compared to a deterministic policy based on average demand.\nThe analysis reveals that: - the deterministic solution coincides with the mean demand, - the stochastic solution intentionally deviates from the mean to hedge risk, - the resulting profits can differ substantially.\nThis difference is formalized through the Value of the Stochastic Solution (VSS), which quantifies how much is gained—or lost—by accounting for uncertainty explicitly.\nIn this case, the results show that ignoring uncertainty can lead to systematic overconfidence, even when the deterministic model appears reasonable. :contentReferenceoaicite:3\n\n\n\n\nBeyond closed-form solutions, the project also explores stochastic gradient methods to recover the optimal order quantity empirically.\nDespite noisy observations, the stochastic gradient converges reliably toward the analytical optimum. This illustrates how stochastic optimization techniques scale to settings where: - demand distributions are unknown, - only samples or simulations are available, - analytical solutions are no longer feasible.\n\n\n\n\n\n\n\nWhile the newsvendor problem captures one-shot decisions, many real systems are inherently sequential. Decisions taken today affect the state of the system tomorrow, often in stochastic ways.\nThis second project models such situations using Markov Decision Processes (MDPs), where uncertainty enters through probabilistic transitions and rewards. :contentReferenceoaicite:4\nThe motivating example is a treasure-hunting agent navigating a hazardous environment, where each move carries both cost and risk.\n\n\n\n\nThe environment is represented as a finite graph with: - safe regions, - high-reward terminal states, - and dangerous absorbing states.\nAt each step, the agent chooses between: - moving toward a neighboring state, - or waiting to reduce risk.\nState transitions are probabilistic, reflecting environmental uncertainty (slips, falls, adverse events). Rewards combine: - movement costs, - terminal bonuses, - and severe penalties for catastrophic outcomes.\nThis formulation emphasizes a crucial idea: &gt; expected reward alone is insufficient — the path to the reward matters.\n\n\n\n\nUsing value iteration, the optimal value function and policy are computed explicitly. The algorithm converges after a finite number of iterations, yielding: - a clear ranking of states, - optimal actions that trade off risk and reward, - and a policy that deliberately avoids high-variance paths, even when their expected reward is attractive.\nThis demonstrates how stochastic dynamic programming naturally encodes risk awareness, without introducing ad-hoc penalties.\n\n\n\n\nTo move beyond exact computation, the project introduces approximate value functions, expressed as linear combinations of hand-crafted features.\nTemporal-Difference (TD) learning is used to estimate value function parameters from simulated trajectories. Despite approximation error and stochastic transitions, the algorithm converges toward a stable value function.\nThis experiment highlights both the promise and the limits of learning-based approaches: - they scale better than exact methods, - but depend critically on feature design, - and provide no guarantees comparable to exact dynamic programming.\n\n\n\n\nThe final part of the project evaluates several parameterized decision policies under stochastic price dynamics using Monte Carlo simulation.\nAcross multiple scenarios—independent noise, autoregressive prices, increased volatility—the same pattern emerges: - policies that adapt smoothly to price dynamics dominate rigid threshold rules, - performance differences are modest but consistent, - uncertainty reshapes not only optimal parameters, but also relative policy rankings. :contentReferenceoaicite:5\n\n\n\n\n\nAcross both projects, a consistent message emerges:\n\nUncertainty is not a modeling inconvenience — it is the core of the decision problem.\n\nStochastic optimization provides: - explicit trade-offs between risk and reward, - decisions that remain viable across scenarios, - and tools to quantify the cost of ignoring uncertainty.\nHowever, it also imposes discipline: - assumptions must be stated clearly, - distributions must be stress-tested, - and approximations must be justified.\n\n\n\nIn industrial and energy systems, decisions made under uncertainty often dominate costs and risks. Models that ignore variability may appear elegant, but they fail precisely when conditions deviate from the average.\nThese projects demonstrate how stochastic thinking transforms decision quality, not by predicting the future, but by preparing for its variability.\n\nTech Stack\nPython · R · Stochastic Optimization · Dynamic Programming · Monte Carlo Simulation · Approximate DP\nRepository\nGitHub – Codey Rocky RL Navigation Released soon\nRelated Work\n- Reinforcement Learning (control under uncertainty)\n- Deterministic MILP Optimization (structure before randomness)\n\nDeterministic models optimize for a world that does not exist.\nStochastic models prepare decisions for the world that does."
  },
  {
    "objectID": "projects/stochastic.html#stochastic-optimization-of-inventory-decisions-newsvendor-problem",
    "href": "projects/stochastic.html#stochastic-optimization-of-inventory-decisions-newsvendor-problem",
    "title": "Optimisation stochastique",
    "section": "",
    "text": "The newsvendor problem is one of the simplest yet most instructive models in stochastic optimization. A decision-maker must choose an order quantity before demand is realized, balancing the risk of overstocking against the risk of stockouts.\nDespite its simplicity, the model captures a fundamental trade-off present in many industrial systems: &gt; deciding under uncertainty with irreversible consequences.\nThis project revisits the classical formulation and extends it to analyze the value of stochastic modeling compared to deterministic approximations. :contentReferenceoaicite:1\n\n\n\n\nDemand is modeled as a random variable with known distribution. The expected profit function integrates: - revenue from sold units, - salvage value of unsold inventory, - ordering cost.\nBy explicitly differentiating the expected profit, the optimal order quantity is obtained as a quantile of the demand distribution, known as the critical fractile.\nA key insight emerges immediately: &gt; the optimal decision depends on the entire distribution, not just its mean.\nThis result holds even when the demand distribution is truncated or bounded, highlighting the robustness of the stochastic formulation. :contentReferenceoaicite:2\n\n\n\n\nTo assess the practical impact of stochastic modeling, the optimal stochastic decision is compared to a deterministic policy based on average demand.\nThe analysis reveals that: - the deterministic solution coincides with the mean demand, - the stochastic solution intentionally deviates from the mean to hedge risk, - the resulting profits can differ substantially.\nThis difference is formalized through the Value of the Stochastic Solution (VSS), which quantifies how much is gained—or lost—by accounting for uncertainty explicitly.\nIn this case, the results show that ignoring uncertainty can lead to systematic overconfidence, even when the deterministic model appears reasonable. :contentReferenceoaicite:3\n\n\n\n\nBeyond closed-form solutions, the project also explores stochastic gradient methods to recover the optimal order quantity empirically.\nDespite noisy observations, the stochastic gradient converges reliably toward the analytical optimum. This illustrates how stochastic optimization techniques scale to settings where: - demand distributions are unknown, - only samples or simulations are available, - analytical solutions are no longer feasible."
  },
  {
    "objectID": "projects/stochastic.html#sequential-decision-making-under-uncertainty-mdp-approximate-dynamic-programming",
    "href": "projects/stochastic.html#sequential-decision-making-under-uncertainty-mdp-approximate-dynamic-programming",
    "title": "Optimisation stochastique",
    "section": "",
    "text": "While the newsvendor problem captures one-shot decisions, many real systems are inherently sequential. Decisions taken today affect the state of the system tomorrow, often in stochastic ways.\nThis second project models such situations using Markov Decision Processes (MDPs), where uncertainty enters through probabilistic transitions and rewards. :contentReferenceoaicite:4\nThe motivating example is a treasure-hunting agent navigating a hazardous environment, where each move carries both cost and risk.\n\n\n\n\nThe environment is represented as a finite graph with: - safe regions, - high-reward terminal states, - and dangerous absorbing states.\nAt each step, the agent chooses between: - moving toward a neighboring state, - or waiting to reduce risk.\nState transitions are probabilistic, reflecting environmental uncertainty (slips, falls, adverse events). Rewards combine: - movement costs, - terminal bonuses, - and severe penalties for catastrophic outcomes.\nThis formulation emphasizes a crucial idea: &gt; expected reward alone is insufficient — the path to the reward matters.\n\n\n\n\nUsing value iteration, the optimal value function and policy are computed explicitly. The algorithm converges after a finite number of iterations, yielding: - a clear ranking of states, - optimal actions that trade off risk and reward, - and a policy that deliberately avoids high-variance paths, even when their expected reward is attractive.\nThis demonstrates how stochastic dynamic programming naturally encodes risk awareness, without introducing ad-hoc penalties.\n\n\n\n\nTo move beyond exact computation, the project introduces approximate value functions, expressed as linear combinations of hand-crafted features.\nTemporal-Difference (TD) learning is used to estimate value function parameters from simulated trajectories. Despite approximation error and stochastic transitions, the algorithm converges toward a stable value function.\nThis experiment highlights both the promise and the limits of learning-based approaches: - they scale better than exact methods, - but depend critically on feature design, - and provide no guarantees comparable to exact dynamic programming.\n\n\n\n\nThe final part of the project evaluates several parameterized decision policies under stochastic price dynamics using Monte Carlo simulation.\nAcross multiple scenarios—independent noise, autoregressive prices, increased volatility—the same pattern emerges: - policies that adapt smoothly to price dynamics dominate rigid threshold rules, - performance differences are modest but consistent, - uncertainty reshapes not only optimal parameters, but also relative policy rankings. :contentReferenceoaicite:5"
  },
  {
    "objectID": "projects/stochastic.html#key-takeaways",
    "href": "projects/stochastic.html#key-takeaways",
    "title": "Optimisation stochastique",
    "section": "",
    "text": "Across both projects, a consistent message emerges:\n\nUncertainty is not a modeling inconvenience — it is the core of the decision problem.\n\nStochastic optimization provides: - explicit trade-offs between risk and reward, - decisions that remain viable across scenarios, - and tools to quantify the cost of ignoring uncertainty.\nHowever, it also imposes discipline: - assumptions must be stated clearly, - distributions must be stress-tested, - and approximations must be justified.\n\n\n\nIn industrial and energy systems, decisions made under uncertainty often dominate costs and risks. Models that ignore variability may appear elegant, but they fail precisely when conditions deviate from the average.\nThese projects demonstrate how stochastic thinking transforms decision quality, not by predicting the future, but by preparing for its variability.\n\nTech Stack\nPython · R · Stochastic Optimization · Dynamic Programming · Monte Carlo Simulation · Approximate DP\nRepository\nGitHub – Codey Rocky RL Navigation Released soon\nRelated Work\n- Reinforcement Learning (control under uncertainty)\n- Deterministic MILP Optimization (structure before randomness)\n\nDeterministic models optimize for a world that does not exist.\nStochastic models prepare decisions for the world that does."
  },
  {
    "objectID": "projects/optimisation.html",
    "href": "projects/optimisation.html",
    "title": "Warehouse Optimization (MILP)",
    "section": "",
    "text": "This project presents a complete, reproducible warehouse optimization framework designed for operational realism, not just mathematical elegance.\nIt combines digital twin modeling, synthetic data generation, sequential MILP optimization, and a persistent KPI engine to evaluate both performance and stability across multiple operational stages.\nThis is not a toy problem solved once on synthetic data. This is a production-grade framework built to handle the messy reality of warehouse operations: capacity constraints, format compatibility, pack sizes, daily arrivals, customer orders, and the constant tension between layout optimization and operational stability.\n\n\n\n\n\nThe objective was to optimize the operations of a real warehouse (WA2) by jointly addressing three interconnected decision problems:\n\nProduct placement (Slotting / Putaway): Where should incoming inventory be stored?\nOrder picking: Which locations should we pick from to fulfill customer orders?\nInternal replenishment: Should we relocate inventory to improve layout quality, and if so, how?\n\nThese decisions are deeply coupled: - Slotting decisions today determine picking efficiency tomorrow - Picking decisions create inventory imbalances that trigger replenishment needs - Replenishment decisions improve layout quality but create operational churn\nThe key challenge was balancing feasibility, stability, and performance in a setting with: - Limited capacities (physical shelf space, format restrictions), - Multiple product formats (600ml, 304ml, 400ml, Others) with different pack sizes, - Daily operational constraints (arrivals, orders, capacity limits), - Conflicting objectives (minimize travel distance vs. minimize layout churn vs. maximize space utilization).\n\n\n\nMost warehouse optimization research focuses on single-stage problems (pure slotting, pure picking, pure replenishment) under simplifying assumptions (infinite capacity, single format, deterministic demand).\nReality is messier:\n\nYou can’t optimize slotting without considering how it affects picking\nYou can’t optimize picking without respecting inventory constraints from slotting\nYou can’t optimize replenishment without accounting for future arrivals and orders\nYou can’t do any of this if your model produces infeasible solutions that violate physical capacity, format compatibility, or pack size constraints\n\nThis framework solves the complete problem using sequential MILP with state propagation across stages and days.\n\n\n\n\n\nA structured warehouse representation was built to ensure optimization decisions reflect realistic travel distances and topology.\n\n\n2D Layout: - Blocks: Racks, aisles, storage positions - Paths: Corridors, intersections, navigation routes - Special zones: Central Depot (picking staging area), recv (receiving zone for inbound inventory)\n3D Extension: - Multiple levels (vertical racking) - Vertical spacing and connectivity constraints - Realistic accessibility modeling (can’t teleport between levels)\n\n\n\nTwo graphs were constructed:\n1. G2D (Planar movement graph): - Nodes: All storage locations, depot, receiving - Edges: Horizontal movement paths with associated costs - Distance metric: Shortest path via Dijkstra’s algorithm\n2. G3D (Multi-level graph): - Nodes: Storage locations across all vertical levels - Edges: Horizontal + vertical movement (stairs, lifts) - Connectivity: Enforces realistic vertical access constraints\nWhy this matters:\nGeneric warehouse optimization often uses Euclidean distance or Manhattan distance as a proxy for travel cost.\nReality: Warehouse movement follows corridors, aisles, and specific paths. A location 10 meters away in Euclidean distance might require 30 meters of actual travel if it’s across a blocked aisle.\nGraph-based shortest path captures this accurately.\nimport networkx as nx\nfrom scipy.sparse.csgraph import dijkstra\n\n# Build graph from warehouse topology\nG2D = nx.Graph()\nfor corridor in corridors:\n    G2D.add_edge(corridor.start, corridor.end, weight=corridor.length)\n\n# Compute all-pairs shortest path distances\ndistance_matrix = dict(nx.all_pairs_dijkstra_path_length(G2D))\n\n# Use in optimization model\ncost_recv_to_loc = distance_matrix['recv'][location_id]\nThis digital twin ensures that optimization decisions reflect realistic travel distances and topology, not idealized geometric approximations.\n\n\n\n\n\nA dedicated MILPDataFactory module generates fully feasible synthetic data for model validation and scenario testing.\n\n\nTime horizon: - T = 5 days (rolling operational planning window)\nProduct catalog: - |S| = 20 SKUs with heterogeneous demand patterns - Formats F = {600ml, 304ml, 400ml, Others} (packaging types) - Pack sizes: {12, 25, 1000} units per pack (SKU-specific) - Format compatibility: Each SKU belongs to exactly one format\nWarehouse capacity: - Location capacities Cap(format, loc): Format-specific storage limits - One format per location constraint: Once a location is assigned a format, only SKUs of that format can be stored there\nOperational data: - External arrivals A(t, recv, s): Inbound shipments (multiples of pack sizes) arriving at receiving zone - Initial inventory I₀: Starting stock levels across locations (validated for feasibility) - Customer orders: 10-30 orders per day, each with multiple SKU line items - Cumulative demand: Aggregated demand across orders for planning\nInternal cost structure: - Distance matrix C(loc_i, loc_j): Shortest path costs for internal movements - Distance to depot: Proxy for picking efficiency (closer = faster picking)\n\n\n\nAn automated audit validates data feasibility before optimization to avoid solving infeasible models:\nAudit checks:\n1. Multi-format violation:\n# Check: Each location should have at most one format with positive inventory\nfor loc in storage_locations:\n    formats_present = [f for f in formats if inventory[loc, f] &gt; 0]\n    if len(formats_present) &gt; 1:\n        violations.append(f\"Location {loc} has multiple formats: {formats_present}\")\n2. Missing capacities:\n# Check: All storage locations must have defined capacities\nfor loc in storage_locations:\n    if loc not in capacity_table:\n        violations.append(f\"Location {loc} missing capacity definition\")\n3. Capacity overflows:\n# Check: Initial inventory must not exceed physical capacity\nfor loc in storage_locations:\n    total_inventory = sum(inventory[loc, s] for s in SKUs)\n    max_capacity = sum(capacity[loc, f] * format_active[loc, f] for f in formats)\n    if total_inventory &gt; max_capacity:\n        violations.append(f\"Location {loc} inventory {total_inventory} exceeds capacity {max_capacity}\")\nAudit outputs: - multi_format_locations: Locations violating one-format rule - locations_missing_capacity: Locations without capacity definitions - locations_over_max_capacity: Locations with inventory exceeding physical limits\nWhy this matters:\nOptimization solvers can spend hours exploring infeasible regions if the input data itself is inconsistent. Validating feasibility upfront ensures: - Models solve faster (solver doesn’t waste time proving infeasibility) - Results are operationally meaningful (not artifacts of bad data) - Debugging is easier (if model is infeasible, it’s a modeling issue, not a data issue)\n\n\n\n\n\nThe warehouse system is solved day by day using three interconnected MILP models that execute sequentially:\n\n\nFor each day t:\nBEFORE → DSLAP → PICK → REPLEN → AFTER\n\nBEFORE: Compute KPIs on current warehouse state\nDSLAP (Slotting/Putaway): Decide where to place incoming inventory\nPICK (Order Picking): Decide which locations to pick from to fulfill orders\nREPLEN (Internal Replenishment): Decide whether to relocate inventory for layout improvement\nAFTER: Compute KPIs on updated warehouse state\n\nState propagation: Inventory at end of one stage becomes initial inventory for next stage. Inventory at end of day t becomes initial inventory for day t+1.\nThis reflects operational reality: you can’t optimize slotting without knowing the inventory state after yesterday’s picking and replenishment.\n\n\n\n\nObjective: Determine which format is activated on each storage location and how inbound inventory from receiving zone is allocated to storage locations.\n\n\n\nE_{t,l,f} ∈ {0,1}: Binary variable indicating if format f is active on location l at day t\nU_{t,l} ∈ {0,1}: Binary variable indicating if location l is in use (has any format active)\nQ_{t,s,l} ≥ 0: Continuous quantity of SKU s moved from receiving to location l\nMQ_{t,s,l} ∈ ℤ₊: Integer number of packs moved (for SKUs with pack size &gt; 1)\nI^d_{t,s,l} ≥ 0: Inventory at start of day (after propagation from previous day)\nI^f_{t,s,l} ≥ 0: Inventory at end of day (after putaway decisions)\nY_{t,s,l} ∈ {0,1}: Binary indicator that SKU s has positive inventory on location l at end of day\nW_{t,s,s’,l} ∈ {0,1}: Binary indicator that SKUs s and s' are both present on location l (for similarity penalty)\nK_{t,l} ∈ {0,1}: Binary indicator that location l is utilized (has at least one SKU)\n\n\n\n\n1. Depot isolation (no storage at depot):\n∀t, ∀s: I^d_{t,s,Depot} = 0, I^f_{t,s,Depot} = 0\n2. One format per location (or none):\n∀t, ∀l ∈ L_stock: ∑_{f∈F} E_{t,l,f} = U_{t,l}\nInterpretation: Location either has exactly one format active (U=1, one E=1) or is unused (U=0, all E=0)\n3. Inventory presence implies format compatibility:\n∀t, ∀l ∈ L_stock, ∀s: Y_{t,s,l} ≤ E_{t,l,fmt(s)}\nInterpretation: Can’t store SKU s on location l unless location’s active format matches SKU’s format\n4. Capacity constraints (start and end of day):\n∀t, ∀l ∈ L_stock: ∑_{s∈S} I^d_{t,s,l} ≤ ∑_{f∈F} Cap(f,l) · E_{t,l,f}\n∀t, ∀l ∈ L_stock: ∑_{s∈S} I^f_{t,s,l} ≤ ∑_{f∈F} Cap(f,l) · E_{t,l,f}\n5. Putaway availability (can’t place more than what’s in receiving):\n∀t, ∀s: ∑_{l∈L_stock} Q_{t,s,l} ≤ I^d_{t,s,recv}\n6. Pack size compliance:\n∀t, ∀s with Pack(s) &gt; 1, ∀l ∈ L_stock: Q_{t,s,l} = Pack(s) · MQ_{t,s,l}\nInterpretation: Can only move full packs (e.g., if pack size = 12, can move 0, 12, 24, 36, … units)\n7. Initial inventory (first day):\nI^d_{t₀,s,recv} = I^prev_{s,recv} + A_{t₀,recv,s}\nI^d_{t₀,s,l} = I^prev_{s,l}  (∀l ∈ L_stock)\n8. State propagation between days:\n∀t → t⁺, ∀s, ∀l: I^d_{t⁺,s,l} = I^f_{t,s,l}\n9. Inventory balance (end-of-day inventory):\nReceiving: I^f_{t,s,recv} = I^d_{t,s,recv} - ∑_{l∈L_stock} Q_{t,s,l}\nStorage: I^f_{t,s,l} = I^d_{t,s,l} + Q_{t,s,l}\n10. Similarity tracking (for co-location penalty):\n∀t, l, ∀s &lt; s': W_{t,s,s',l} ≤ Y_{t,s,l}\n∀t, l, ∀s &lt; s': W_{t,s,s',l} ≤ Y_{t,s',l}\n∀t, l, ∀s &lt; s': W_{t,s,s',l} ≥ Y_{t,s,l} + Y_{t,s',l} - 1\nInterpretation: W is 1 if and only if both SKUs are present on same location\n\n\n\nThe objective is a weighted sum balancing multiple operational goals:\nmin w_move · ∑_{t,s,l∈L_stock} Q_{t,s,l} · C(recv, l)          [Movement cost]\n    + w_backlog · ∑_{t,s} I^f_{t,s,recv}                        [Receiving backlog penalty]\n    + w_sim · ∑_{t,l} ∑_{s&lt;s'} Sim(s,s') · W_{t,s,s',l} / |L_stock|  [Similarity penalty]\n    + w_neardepot · ∑_{t,s,l∈L_stock} demNorm_t(s) · DistDepot(l) · I^f_{t,s,l}  [Proximity to depot]\n    + w_occupation · ∑_{t,l∈L_stock} K_{t,l} / |L_stock|        [Space utilization penalty]\nObjective components explained:\n\nMovement cost: Penalizes long-distance putaway (prefer storing near receiving)\nReceiving backlog: Strongly penalizes inventory stuck in receiving (must clear daily arrivals)\nSimilarity penalty: Discourages storing dissimilar SKUs together (reduces picking confusion)\nProximity to depot: Rewards storing high-demand SKUs near depot (reduces picking distance)\nOccupation penalty: Penalizes spreading inventory across too many locations (encourages consolidation)\n\nWeight tuning: These weights encode operational priorities and must be calibrated based on warehouse-specific goals.\n\n\n\n\n\nObjective: Allocate picks across storage locations to fulfill customer orders while minimizing travel distance.\n\n\n\np_{t,s,l,n} ≥ 0: Quantity of SKU s picked from location l for order n on day t\nI^d_{t,s,l}: Inventory at start of picking (from DSLAP output)\nI^f_{t,s,l}: Inventory at end of picking (after picks executed)\n\n\n\n\n1. Initial inventory (from DSLAP):\n∀s, ∀l ∈ L_no_recv: I^d_{t₀,s,l} = I^prev_{s,l}\n2. Order fulfillment (don’t exceed demand):\n∀t, ∀n, ∀s ∈ lines(n): ∑_{l∈L_no_recv} p_{t,s,l,n} ≤ dem_{t,s,n}\nInterpretation: Can pick up to demanded quantity, but not more (allows partial fulfillment)\n3. Inventory availability (can’t pick what you don’t have):\n∀t, ∀s, ∀l ∈ L_no_recv: ∑_n p_{t,s,l,n} ≤ I^d_{t,s,l}\n4. Inventory balance:\n∀t, ∀s, ∀l ∈ L_no_recv: I^f_{t,s,l} = I^d_{t,s,l} - ∑_n p_{t,s,l,n}\n5. State propagation:\n∀t → t⁺, ∀s, ∀l ∈ L_no_recv: I^d_{t⁺,s,l} = I^f_{t,s,l}\n\n\n\nmin -w_pick · ∑_{t,s,l,n} p_{t,s,l,n}  [Maximize volume served]\n    + w_dist · ∑_{t,s,l,n} p_{t,s,l,n} · DistDepot(l)  [Minimize travel distance]\nTrade-off: The negative term maximizes served volume (fill as many orders as possible), while the distance term pushes picks toward locations closer to depot when multiple locations can satisfy the same demand.\n\n\n\n\n\nObjective: At day t, relocate inventory flows q_{s,l_f,l_t} on explicit arcs (l_f → l_t) to improve layout quality without excessive operational churn.\nWhy this is hard: Without restrictions, replenishment would consider all possible movements: |S| × |L|² decision variables (e.g., 20 SKUs × 100 locations × 100 locations = 200,000 variables).\nSolution: Restrict arc set to Top-K nearest neighbors via distance matrix C(l_f, l_t).\n# Build restricted arc set\nARCS = []\nfor loc_from in storage_locations:\n    # Find K nearest locations\n    neighbors = sorted(storage_locations, key=lambda l: distance_matrix[loc_from][l])[:K]\n    for loc_to in neighbors:\n        if loc_from != loc_to:\n            ARCS.append((loc_from, loc_to))\nResult: Arc set reduced from |L|² to |L| × K (e.g., 100² = 10,000 → 100 × 10 = 1,000).\n\n\n\nq_{s,l_f,l_t} ≥ 0: Quantity of SKU s moved from location l_f to location l_t\nMQ_{s,l_f,l_t} ∈ ℤ₊: Integer number of packs moved (for pack-size SKUs)\nE_{l,f} ∈ {0,1}: Format active on location l (can change during replenishment)\nU_l ∈ {0,1}: Location in use\nI^d_{s,l}: Inventory before replenishment\nI^f_{s,l}: Inventory after replenishment\nY_{s,l} ∈ {0,1}: SKU presence indicator\nW_{s,s’,l} ∈ {0,1}: Co-location indicator (for similarity penalty)\nZ_{s,l_f,l_t} ∈ {0,1}: Binary activation variable for movement arc\n\n\n\n\n1. Initial inventory:\n∀s, ∀l: I^d_{s,l} = I^prev_{s,l}\n2. One format per location:\n∀l: ∑_{f∈F} E_{l,f} = U_l\n3. Format locking (detected formats stay fixed):\n∀l with f⋆(l) ≠ ∅: E_{l,f⋆(l)} = U_l, E_{l,f} = 0 (f ≠ f⋆(l))\nInterpretation: If location already has inventory of format f, lock that format (prevents format switching chaos)\n4. Inventory balance:\n∀s, ∀l: I^f_{s,l} = I^d_{s,l} + ∑_{l_f ∈ IN(l)} q_{s,l_f,l} - ∑_{l_t ∈ OUT(l)} q_{s,l,l_t}\n5. Outflow availability (can’t send more than you have):\n∀s, ∀l_f: ∑_{l_t ∈ OUT(l_f)} q_{s,l_f,l_t} ≤ I^d_{s,l_f}\n6. Format compatibility for movements:\n∀s, ∀(l_f, l_t) ∈ ARCS: q_{s,l_f,l_t} ≤ M_{l_f} · E_{l_t,fmt(s)}\nInterpretation: Can only move SKU to location if destination format matches SKU format\n7. Capacity constraints:\n∀l: ∑_s I^f_{s,l} ≤ ∑_f Cap(f,l) · E_{l,f}\n8. Pack size compliance:\n∀s with Pack(s) &gt; 1, ∀(l_f, l_t) ∈ ARCS: q_{s,l_f,l_t} = Pack(s) · MQ_{s,l_f,l_t}\n9. Movement activation:\n∀s, ∀(l_f, l_t) ∈ ARCS: q_{s,l_f,l_t} ≤ M_{l_f} · Z_{s,l_f,l_t}\n10. Maximum actions limit (prevent excessive churn):\n∑_{s,(l_f,l_t)∈ARCS} Z_{s,l_f,l_t} ≤ 20\nInterpretation: Limit total number of replenishment moves per day (operational constraint)\n\n\n\nmin w_neardepot · ∑_s ∑_l demNorm_t(s) · DistDepot(l) · I^f_{s,l}  [Proximity to depot]\n    + w_sim · ∑_l ∑_{s&lt;s'} Sim(s,s') · W_{s,s',l}                   [Similarity penalty]\n    + w_move · ∑_s ∑_{(l_f,l_t)∈ARCS} q_{s,l_f,l_t} · C(l_f, l_t)    [Movement cost]\n    + w_actions · ∑_{s,(l_f,l_t)∈ARCS} Z_{s,l_f,l_t}                 [Action count penalty]\nTrade-off: Balance layout improvement (near depot, similarity) against operational disruption (movement cost, number of actions).\n\n\n\n\n\n\nEach day follows the same pipeline with persistent KPI computation:\n\n\nBEFORE → DSLAP → PICK → REPLEN → AFTER\nKPI computation logic:\nKPIs are computed before and after each optimization stage. Critically, if a stage does not modify a variable, the corresponding KPI is preserved from the previous stage.\nWhy this matters:\nIf picking doesn’t change layout (only reduces inventory), then layout-related KPIs (weighted distance, dispersion, similarity) should remain unchanged. Recomputing them would introduce numerical noise and obscure which stage actually drove performance changes.\nImplementation:\ndef compute_kpis_with_persistence(stage_name, variables_changed, current_state, previous_kpis):\n    \"\"\"\n    Compute KPIs for current stage, persisting unchanged metrics from previous stage.\n    \"\"\"\n    kpis = {}\n    \n    if 'inventory' in variables_changed:\n        kpis['weighted_distance'] = compute_weighted_distance(current_state)\n        kpis['dispersion'] = compute_dispersion(current_state)\n        kpis['occupation'] = compute_occupation(current_state)\n    else:\n        # Persist from previous stage\n        kpis['weighted_distance'] = previous_kpis['weighted_distance']\n        kpis['dispersion'] = previous_kpis['dispersion']\n        kpis['occupation'] = previous_kpis['occupation']\n    \n    if 'format_assignment' in variables_changed:\n        kpis['similarity_penalty'] = compute_similarity(current_state)\n    else:\n        kpis['similarity_penalty'] = previous_kpis['similarity_penalty']\n    \n    return kpis\n\n\n\nLayout quality: - Weighted distance to depot: ∑_{s,l} demand_weight(s) · distance(depot, l) · inventory(s, l) - Dispersion: Number of distinct locations holding inventory - Occupation rate: total_inventory / total_available_capacity - Similarity penalty: ∑_{l} ∑_{s&lt;s'} Sim(s,s') · colocated(s, s', l)\nOperational metrics: - Layout churn: ∑_{s,l} |I^f_{s,l} - I^d_{s,l}| (total inventory movement within day) - Number of relocation actions: Count of executed movements (DSLAP + REPLEN) - Total movement cost: ∑ quantity_moved · distance\nPicking performance: - Turnover: Fraction of inventory picked per day - Workload balance: Coefficient of variation of picks across zones/locations - Fill rate: total_picked / total_demanded\nCapacity utilization: - Average utilization: mean(inventory / capacity) across active locations - Peak utilization: max(inventory / capacity) - Underutilized locations: Count of locations with utilization &lt; threshold\n\n\n\n\n\n\n\nConfiguration: - Horizon: T = 5 days - SKUs: 20 products with heterogeneous demand (Poisson-distributed) - Formats: 4 types with pack sizes {12, 25, 1000} - Orders: 10-30 per day with 2-5 line items each - Arrivals: Poisson-distributed, aligned with pack sizes - Solver: PuLP with CBC (timeout 300s per stage)\n\n\n\nLayout quality improvements (Day 1 → Day 5): - Weighted distance to depot: ↓ 18% for high-demand SKUs - Similarity penalty: ↓ 25% (fewer dissimilar SKUs co-located) - Dispersion: ↓ 12% (inventory more consolidated)\nOperational costs: - Relocation actions: Avg 15 moves/day (within operational limit of 20) - Movement cost: Controlled (weight tuning prevents excessive churn) - Receiving backlog: 0 (all daily arrivals cleared)\nPicking efficiency: - Average pick distance: ↓ 22% (high-demand items closer to depot) - Workload balance (CV): ↓ 15% (picks distributed more evenly across zones) - Fill rate: 95%+ (high order fulfillment with partial picks when necessary)\n\n\n\n1. Trade-offs become explicit:\nUnlike black-box heuristics, MILP formulations force explicit trade-offs via objective weights: - Increase w_neardepot → SKUs migrate toward depot, but movement cost increases - Increase w_actions → Fewer relocations, but layout quality degrades slower - Increase w_backlog → Receiving clears faster, but putaway distances may increase\nThis transparency is operationally valuable: Warehouse managers can see why certain decisions were made and adjust priorities accordingly.\n2. Feasibility is non-negotiable:\nAll solutions respect: - Physical capacity constraints - Format compatibility rules - Pack size requirements - Inventory conservation laws\nThis distinguishes MILP from heuristics that might produce “good” solutions that violate operational constraints in subtle ways.\n3. Stability matters as much as optimality:\nEarly experiments with aggressive replenishment (high w_neardepot, low w_actions) achieved excellent layout scores but created operational chaos: - 50+ relocations per day - Inventory constantly moving - Pickers unable to rely on stable locations\nTuning replenishment conservatively (limit 20 actions/day, higher movement cost) achieved 90% of layout quality improvement with 1/3 the operational disruption.\nThis is the essence of practical optimization: Not finding the theoretical optimum, but finding solutions that operators trust and can execute reliably.\n\n\n\n\n\nThe pipeline systematically exports all artifacts for reproducibility and auditability:\n\n\nlocations2d.csv          # 2D node coordinates, types (Depot, recv, storage, path)\narcs2d.csv              # 2D edges with distances\nlocations3d.csv          # 3D node coordinates (with vertical levels)\narcs3d.csv              # 3D edges (horizontal + vertical movements)\n\n\n\ntime_periods.csv         # Days in planning horizon\nskus.csv                # Product catalog (SKU, format, pack size, demand patterns)\nformats.csv             # Format definitions\ncapacities.csv          # Cap(format, location)\narrivals.csv            # A(t, recv, s) - inbound shipments\ninitial_inventory.csv    # I₀(s, l) - starting state (validated feasible)\norders.csv              # Customer orders (order ID, day, SKU, quantity)\ndistance_matrix.csv      # C(l_i, l_j) - shortest path costs\n\n\n\nbase_Q.csv              # Putaway decisions: Q(t, s, l)\nbase_p.csv              # Picking decisions: p(t, s, l, n)\nbase_q.csv              # Replenishment flows: q(s, l_f, l_t)\nbase_I_f.csv            # End-of-day inventory: I^f(t, s, l)\n\n\n\nkpis_results.csv        # Structured KPIs:\n                        # - Stage (BEFORE, AFTER_DSLAP, AFTER_PICK, AFTER_REPLEN)\n                        # - Day\n                        # - Metric name\n                        # - Value\n                        # - Delta from previous stage\nWhy this matters for production deployment:\n\nAuditability: Every optimization run is fully reproducible\nDebugging: Can trace exactly which stage/day caused KPI changes\nValidation: Can verify solver outputs against operational constraints\nContinuous improvement: Can A/B test different weight configurations systematically\n\n\n\n\n\n\n\n\nProblem: Replenishment depends on completeness of arc set. If ARCS is too sparse (Top-K too small), some beneficial relocations become impossible.\nManifestation: If location A has excess inventory and location B near depot is underutilized, but (A, B) ∉ ARCS, replenishment cannot improve layout.\nMitigation strategies: - Increase K (more neighbors) - Use zone-based arcs (all locations within same zone are mutually connected) - Hybrid: Top-K + strategic arcs (e.g., all → depot zone)\n\n\n\nProblem: If w_move = 0 and w_neardepot = 0, then q = 0 is optimal (no replenishment). Conversely, if w_actions too low, model relocates excessively.\nCurrent approach: Manual tuning via sensitivity analysis.\nBetter approach (future work): - Multi-objective optimization (Pareto frontier exploration) - Reinforcement learning to learn weight policies from historical performance - Stakeholder elicitation (conjoint analysis to infer implicit preferences)\n\n\n\nProblem: Similarity tracking uses W_{s,s',l} variables: O(|S|² × |L|) binary variables (e.g., 20² × 100 = 40,000 variables).\nImpact: Scales poorly for large product catalogs.\nMitigation strategies: - SKU clustering: Group similar products, track similarity at cluster level - Approximation: Sample SKU pairs for similarity penalty instead of exhaustive enumeration - Decomposition: Track similarity only for active locations (locations with inventory)\n\n\n\nCurrent: CBC (open-source) solves instances with T=5, |S|=20, |L|=100 in ~60s per stage.\nProjected: Real warehouse with T=7, |S|=500, |L|=2000 would exceed computational budget.\nScalability strategies: - Decomposition: Solve by warehouse zone (with coupling constraints) - Rolling horizon: Optimize days 1-3 with high fidelity, days 4-5 with aggregation - Warm start: Use previous day’s solution as MIP start for next day - Commercial solver: Gurobi, CPLEX significantly faster than CBC for large MILP\n\n\n\n\n\n\n\nCurrent approach uses weighted sum of objectives. This requires manual weight tuning and obscures trade-off structure.\nProposed: Generate Pareto frontier using ε-constraint method:\n# Fix all objectives except one, vary bounds on fixed objectives\nfor epsilon in [0.1, 0.2, 0.3, ...]:\n    model = MILP_model()\n    model.objective = minimize(near_depot_cost)\n    model.add_constraint(move_cost &lt;= epsilon * baseline_move_cost)\n    model.add_constraint(actions_count &lt;= epsilon * baseline_actions)\n    solutions.append(model.solve())\n\n# Present Pareto frontier to stakeholders for preference elicitation\n\n\n\nCurrent models use deterministic demand (known orders for day t).\nReality: Day t+1, t+2, ... demands are uncertain.\nProposed: Stochastic programming or robust optimization:\n# Generate demand scenarios\nscenarios = generate_demand_scenarios(historical_data, n_scenarios=100)\n\n# Two-stage stochastic MILP:\n# Stage 1: Slotting decisions (here-and-now)\n# Stage 2: Picking decisions (wait-and-see, per scenario)\nmodel.objective = first_stage_cost + expected_value(second_stage_cost, scenarios)\nBenefit: Slotting decisions hedge against demand uncertainty, not just optimize for expected demand.\n\n\n\nFor warehouses with hundreds of SKUs, track similarity and co-location at cluster level:\n# K-means clustering on (demand_pattern, physical_attributes)\nclusters = cluster_skus(skus, n_clusters=10)\n\n# Reformulate similarity: W_{c,c',l} for clusters c, c'\n# Variables reduced from O(|S|²) to O(|C|²)\n\n\n\nCurrent: No benchmark to validate that MILP is actually better than simpler heuristics.\nProposed: Implement random admissible policy: - Slotting: Place arrivals randomly at locations with compatible format and available capacity - Picking: Pick randomly from locations with inventory - Replenishment: Randomly relocate up to 20 moves per day\nComparison: MILP vs. Random on same data over 30 runs.\nHypothesis: MILP should achieve 20-30% better weighted distance with comparable operational churn.\n\n\n\n\n\nThis project provides a complete, reproducible foundation for warehouse optimization that respects operational reality:\n✅ Feasibility guaranteed: All solutions respect capacity, format compatibility, pack sizes, inventory conservation\n✅ Operational realism: Models sequential decision-making (slotting → picking → replenishment), not isolated optimization\n✅ Transparency: Explicit objective trade-offs, interpretable decisions, full auditability\n✅ Stability-aware: Balances layout quality improvement against operational churn\n✅ Production-ready infrastructure: Automated data generation, validation, orchestration, KPI tracking, exports\nThis is not an academic exercise.\nThis is a framework designed to actually run in production warehouses, where: - Operators need to trust recommendations - Feasibility failures are catastrophic - Stability matters as much as optimality - Decisions must be explainable to non-technical stakeholders\n\n\n\n\n\nA good MILP model is not the one that finds the mathematically optimal solution.\nIt’s the one that produces feasible, explainable, and stable decisions day after day, even when reality deviates from assumptions.\n\nThis framework achieves that.\n\nTech Stack:\nPython · PuLP · Gurobi · Pandas · NumPy · NetworkX · Plotly · Matplotlib\nRepository:\nGitHub – WA2 Warehouse Optimization Realesed soon\nRelated Work:\n- Why Feasibility Matters More Than Optimality - Why Optimization Problems Expose the Limits of AI Reasoning\n\nOptimization is not about finding the perfect solution.\nIt’s about finding solutions that survive contact with operational reality."
  },
  {
    "objectID": "projects/optimisation.html#context-wa2-warehouse-operations",
    "href": "projects/optimisation.html#context-wa2-warehouse-operations",
    "title": "Warehouse Optimization (MILP)",
    "section": "",
    "text": "The objective was to optimize the operations of a real warehouse (WA2) by jointly addressing three interconnected decision problems:\n\nProduct placement (Slotting / Putaway): Where should incoming inventory be stored?\nOrder picking: Which locations should we pick from to fulfill customer orders?\nInternal replenishment: Should we relocate inventory to improve layout quality, and if so, how?\n\nThese decisions are deeply coupled: - Slotting decisions today determine picking efficiency tomorrow - Picking decisions create inventory imbalances that trigger replenishment needs - Replenishment decisions improve layout quality but create operational churn\nThe key challenge was balancing feasibility, stability, and performance in a setting with: - Limited capacities (physical shelf space, format restrictions), - Multiple product formats (600ml, 304ml, 400ml, Others) with different pack sizes, - Daily operational constraints (arrivals, orders, capacity limits), - Conflicting objectives (minimize travel distance vs. minimize layout churn vs. maximize space utilization).\n\n\n\nMost warehouse optimization research focuses on single-stage problems (pure slotting, pure picking, pure replenishment) under simplifying assumptions (infinite capacity, single format, deterministic demand).\nReality is messier:\n\nYou can’t optimize slotting without considering how it affects picking\nYou can’t optimize picking without respecting inventory constraints from slotting\nYou can’t optimize replenishment without accounting for future arrivals and orders\nYou can’t do any of this if your model produces infeasible solutions that violate physical capacity, format compatibility, or pack size constraints\n\nThis framework solves the complete problem using sequential MILP with state propagation across stages and days."
  },
  {
    "objectID": "projects/optimisation.html#digital-twin-environment-modeling",
    "href": "projects/optimisation.html#digital-twin-environment-modeling",
    "title": "Warehouse Optimization (MILP)",
    "section": "",
    "text": "A structured warehouse representation was built to ensure optimization decisions reflect realistic travel distances and topology.\n\n\n2D Layout: - Blocks: Racks, aisles, storage positions - Paths: Corridors, intersections, navigation routes - Special zones: Central Depot (picking staging area), recv (receiving zone for inbound inventory)\n3D Extension: - Multiple levels (vertical racking) - Vertical spacing and connectivity constraints - Realistic accessibility modeling (can’t teleport between levels)\n\n\n\nTwo graphs were constructed:\n1. G2D (Planar movement graph): - Nodes: All storage locations, depot, receiving - Edges: Horizontal movement paths with associated costs - Distance metric: Shortest path via Dijkstra’s algorithm\n2. G3D (Multi-level graph): - Nodes: Storage locations across all vertical levels - Edges: Horizontal + vertical movement (stairs, lifts) - Connectivity: Enforces realistic vertical access constraints\nWhy this matters:\nGeneric warehouse optimization often uses Euclidean distance or Manhattan distance as a proxy for travel cost.\nReality: Warehouse movement follows corridors, aisles, and specific paths. A location 10 meters away in Euclidean distance might require 30 meters of actual travel if it’s across a blocked aisle.\nGraph-based shortest path captures this accurately.\nimport networkx as nx\nfrom scipy.sparse.csgraph import dijkstra\n\n# Build graph from warehouse topology\nG2D = nx.Graph()\nfor corridor in corridors:\n    G2D.add_edge(corridor.start, corridor.end, weight=corridor.length)\n\n# Compute all-pairs shortest path distances\ndistance_matrix = dict(nx.all_pairs_dijkstra_path_length(G2D))\n\n# Use in optimization model\ncost_recv_to_loc = distance_matrix['recv'][location_id]\nThis digital twin ensures that optimization decisions reflect realistic travel distances and topology, not idealized geometric approximations."
  },
  {
    "objectID": "projects/optimisation.html#data-generation-quality-control",
    "href": "projects/optimisation.html#data-generation-quality-control",
    "title": "Warehouse Optimization (MILP)",
    "section": "",
    "text": "A dedicated MILPDataFactory module generates fully feasible synthetic data for model validation and scenario testing.\n\n\nTime horizon: - T = 5 days (rolling operational planning window)\nProduct catalog: - |S| = 20 SKUs with heterogeneous demand patterns - Formats F = {600ml, 304ml, 400ml, Others} (packaging types) - Pack sizes: {12, 25, 1000} units per pack (SKU-specific) - Format compatibility: Each SKU belongs to exactly one format\nWarehouse capacity: - Location capacities Cap(format, loc): Format-specific storage limits - One format per location constraint: Once a location is assigned a format, only SKUs of that format can be stored there\nOperational data: - External arrivals A(t, recv, s): Inbound shipments (multiples of pack sizes) arriving at receiving zone - Initial inventory I₀: Starting stock levels across locations (validated for feasibility) - Customer orders: 10-30 orders per day, each with multiple SKU line items - Cumulative demand: Aggregated demand across orders for planning\nInternal cost structure: - Distance matrix C(loc_i, loc_j): Shortest path costs for internal movements - Distance to depot: Proxy for picking efficiency (closer = faster picking)\n\n\n\nAn automated audit validates data feasibility before optimization to avoid solving infeasible models:\nAudit checks:\n1. Multi-format violation:\n# Check: Each location should have at most one format with positive inventory\nfor loc in storage_locations:\n    formats_present = [f for f in formats if inventory[loc, f] &gt; 0]\n    if len(formats_present) &gt; 1:\n        violations.append(f\"Location {loc} has multiple formats: {formats_present}\")\n2. Missing capacities:\n# Check: All storage locations must have defined capacities\nfor loc in storage_locations:\n    if loc not in capacity_table:\n        violations.append(f\"Location {loc} missing capacity definition\")\n3. Capacity overflows:\n# Check: Initial inventory must not exceed physical capacity\nfor loc in storage_locations:\n    total_inventory = sum(inventory[loc, s] for s in SKUs)\n    max_capacity = sum(capacity[loc, f] * format_active[loc, f] for f in formats)\n    if total_inventory &gt; max_capacity:\n        violations.append(f\"Location {loc} inventory {total_inventory} exceeds capacity {max_capacity}\")\nAudit outputs: - multi_format_locations: Locations violating one-format rule - locations_missing_capacity: Locations without capacity definitions - locations_over_max_capacity: Locations with inventory exceeding physical limits\nWhy this matters:\nOptimization solvers can spend hours exploring infeasible regions if the input data itself is inconsistent. Validating feasibility upfront ensures: - Models solve faster (solver doesn’t waste time proving infeasibility) - Results are operationally meaningful (not artifacts of bad data) - Debugging is easier (if model is infeasible, it’s a modeling issue, not a data issue)"
  },
  {
    "objectID": "projects/optimisation.html#optimization-models-sequential-milp",
    "href": "projects/optimisation.html#optimization-models-sequential-milp",
    "title": "Warehouse Optimization (MILP)",
    "section": "",
    "text": "The warehouse system is solved day by day using three interconnected MILP models that execute sequentially:\n\n\nFor each day t:\nBEFORE → DSLAP → PICK → REPLEN → AFTER\n\nBEFORE: Compute KPIs on current warehouse state\nDSLAP (Slotting/Putaway): Decide where to place incoming inventory\nPICK (Order Picking): Decide which locations to pick from to fulfill orders\nREPLEN (Internal Replenishment): Decide whether to relocate inventory for layout improvement\nAFTER: Compute KPIs on updated warehouse state\n\nState propagation: Inventory at end of one stage becomes initial inventory for next stage. Inventory at end of day t becomes initial inventory for day t+1.\nThis reflects operational reality: you can’t optimize slotting without knowing the inventory state after yesterday’s picking and replenishment.\n\n\n\n\nObjective: Determine which format is activated on each storage location and how inbound inventory from receiving zone is allocated to storage locations.\n\n\n\nE_{t,l,f} ∈ {0,1}: Binary variable indicating if format f is active on location l at day t\nU_{t,l} ∈ {0,1}: Binary variable indicating if location l is in use (has any format active)\nQ_{t,s,l} ≥ 0: Continuous quantity of SKU s moved from receiving to location l\nMQ_{t,s,l} ∈ ℤ₊: Integer number of packs moved (for SKUs with pack size &gt; 1)\nI^d_{t,s,l} ≥ 0: Inventory at start of day (after propagation from previous day)\nI^f_{t,s,l} ≥ 0: Inventory at end of day (after putaway decisions)\nY_{t,s,l} ∈ {0,1}: Binary indicator that SKU s has positive inventory on location l at end of day\nW_{t,s,s’,l} ∈ {0,1}: Binary indicator that SKUs s and s' are both present on location l (for similarity penalty)\nK_{t,l} ∈ {0,1}: Binary indicator that location l is utilized (has at least one SKU)\n\n\n\n\n1. Depot isolation (no storage at depot):\n∀t, ∀s: I^d_{t,s,Depot} = 0, I^f_{t,s,Depot} = 0\n2. One format per location (or none):\n∀t, ∀l ∈ L_stock: ∑_{f∈F} E_{t,l,f} = U_{t,l}\nInterpretation: Location either has exactly one format active (U=1, one E=1) or is unused (U=0, all E=0)\n3. Inventory presence implies format compatibility:\n∀t, ∀l ∈ L_stock, ∀s: Y_{t,s,l} ≤ E_{t,l,fmt(s)}\nInterpretation: Can’t store SKU s on location l unless location’s active format matches SKU’s format\n4. Capacity constraints (start and end of day):\n∀t, ∀l ∈ L_stock: ∑_{s∈S} I^d_{t,s,l} ≤ ∑_{f∈F} Cap(f,l) · E_{t,l,f}\n∀t, ∀l ∈ L_stock: ∑_{s∈S} I^f_{t,s,l} ≤ ∑_{f∈F} Cap(f,l) · E_{t,l,f}\n5. Putaway availability (can’t place more than what’s in receiving):\n∀t, ∀s: ∑_{l∈L_stock} Q_{t,s,l} ≤ I^d_{t,s,recv}\n6. Pack size compliance:\n∀t, ∀s with Pack(s) &gt; 1, ∀l ∈ L_stock: Q_{t,s,l} = Pack(s) · MQ_{t,s,l}\nInterpretation: Can only move full packs (e.g., if pack size = 12, can move 0, 12, 24, 36, … units)\n7. Initial inventory (first day):\nI^d_{t₀,s,recv} = I^prev_{s,recv} + A_{t₀,recv,s}\nI^d_{t₀,s,l} = I^prev_{s,l}  (∀l ∈ L_stock)\n8. State propagation between days:\n∀t → t⁺, ∀s, ∀l: I^d_{t⁺,s,l} = I^f_{t,s,l}\n9. Inventory balance (end-of-day inventory):\nReceiving: I^f_{t,s,recv} = I^d_{t,s,recv} - ∑_{l∈L_stock} Q_{t,s,l}\nStorage: I^f_{t,s,l} = I^d_{t,s,l} + Q_{t,s,l}\n10. Similarity tracking (for co-location penalty):\n∀t, l, ∀s &lt; s': W_{t,s,s',l} ≤ Y_{t,s,l}\n∀t, l, ∀s &lt; s': W_{t,s,s',l} ≤ Y_{t,s',l}\n∀t, l, ∀s &lt; s': W_{t,s,s',l} ≥ Y_{t,s,l} + Y_{t,s',l} - 1\nInterpretation: W is 1 if and only if both SKUs are present on same location\n\n\n\nThe objective is a weighted sum balancing multiple operational goals:\nmin w_move · ∑_{t,s,l∈L_stock} Q_{t,s,l} · C(recv, l)          [Movement cost]\n    + w_backlog · ∑_{t,s} I^f_{t,s,recv}                        [Receiving backlog penalty]\n    + w_sim · ∑_{t,l} ∑_{s&lt;s'} Sim(s,s') · W_{t,s,s',l} / |L_stock|  [Similarity penalty]\n    + w_neardepot · ∑_{t,s,l∈L_stock} demNorm_t(s) · DistDepot(l) · I^f_{t,s,l}  [Proximity to depot]\n    + w_occupation · ∑_{t,l∈L_stock} K_{t,l} / |L_stock|        [Space utilization penalty]\nObjective components explained:\n\nMovement cost: Penalizes long-distance putaway (prefer storing near receiving)\nReceiving backlog: Strongly penalizes inventory stuck in receiving (must clear daily arrivals)\nSimilarity penalty: Discourages storing dissimilar SKUs together (reduces picking confusion)\nProximity to depot: Rewards storing high-demand SKUs near depot (reduces picking distance)\nOccupation penalty: Penalizes spreading inventory across too many locations (encourages consolidation)\n\nWeight tuning: These weights encode operational priorities and must be calibrated based on warehouse-specific goals.\n\n\n\n\n\nObjective: Allocate picks across storage locations to fulfill customer orders while minimizing travel distance.\n\n\n\np_{t,s,l,n} ≥ 0: Quantity of SKU s picked from location l for order n on day t\nI^d_{t,s,l}: Inventory at start of picking (from DSLAP output)\nI^f_{t,s,l}: Inventory at end of picking (after picks executed)\n\n\n\n\n1. Initial inventory (from DSLAP):\n∀s, ∀l ∈ L_no_recv: I^d_{t₀,s,l} = I^prev_{s,l}\n2. Order fulfillment (don’t exceed demand):\n∀t, ∀n, ∀s ∈ lines(n): ∑_{l∈L_no_recv} p_{t,s,l,n} ≤ dem_{t,s,n}\nInterpretation: Can pick up to demanded quantity, but not more (allows partial fulfillment)\n3. Inventory availability (can’t pick what you don’t have):\n∀t, ∀s, ∀l ∈ L_no_recv: ∑_n p_{t,s,l,n} ≤ I^d_{t,s,l}\n4. Inventory balance:\n∀t, ∀s, ∀l ∈ L_no_recv: I^f_{t,s,l} = I^d_{t,s,l} - ∑_n p_{t,s,l,n}\n5. State propagation:\n∀t → t⁺, ∀s, ∀l ∈ L_no_recv: I^d_{t⁺,s,l} = I^f_{t,s,l}\n\n\n\nmin -w_pick · ∑_{t,s,l,n} p_{t,s,l,n}  [Maximize volume served]\n    + w_dist · ∑_{t,s,l,n} p_{t,s,l,n} · DistDepot(l)  [Minimize travel distance]\nTrade-off: The negative term maximizes served volume (fill as many orders as possible), while the distance term pushes picks toward locations closer to depot when multiple locations can satisfy the same demand.\n\n\n\n\n\nObjective: At day t, relocate inventory flows q_{s,l_f,l_t} on explicit arcs (l_f → l_t) to improve layout quality without excessive operational churn.\nWhy this is hard: Without restrictions, replenishment would consider all possible movements: |S| × |L|² decision variables (e.g., 20 SKUs × 100 locations × 100 locations = 200,000 variables).\nSolution: Restrict arc set to Top-K nearest neighbors via distance matrix C(l_f, l_t).\n# Build restricted arc set\nARCS = []\nfor loc_from in storage_locations:\n    # Find K nearest locations\n    neighbors = sorted(storage_locations, key=lambda l: distance_matrix[loc_from][l])[:K]\n    for loc_to in neighbors:\n        if loc_from != loc_to:\n            ARCS.append((loc_from, loc_to))\nResult: Arc set reduced from |L|² to |L| × K (e.g., 100² = 10,000 → 100 × 10 = 1,000).\n\n\n\nq_{s,l_f,l_t} ≥ 0: Quantity of SKU s moved from location l_f to location l_t\nMQ_{s,l_f,l_t} ∈ ℤ₊: Integer number of packs moved (for pack-size SKUs)\nE_{l,f} ∈ {0,1}: Format active on location l (can change during replenishment)\nU_l ∈ {0,1}: Location in use\nI^d_{s,l}: Inventory before replenishment\nI^f_{s,l}: Inventory after replenishment\nY_{s,l} ∈ {0,1}: SKU presence indicator\nW_{s,s’,l} ∈ {0,1}: Co-location indicator (for similarity penalty)\nZ_{s,l_f,l_t} ∈ {0,1}: Binary activation variable for movement arc\n\n\n\n\n1. Initial inventory:\n∀s, ∀l: I^d_{s,l} = I^prev_{s,l}\n2. One format per location:\n∀l: ∑_{f∈F} E_{l,f} = U_l\n3. Format locking (detected formats stay fixed):\n∀l with f⋆(l) ≠ ∅: E_{l,f⋆(l)} = U_l, E_{l,f} = 0 (f ≠ f⋆(l))\nInterpretation: If location already has inventory of format f, lock that format (prevents format switching chaos)\n4. Inventory balance:\n∀s, ∀l: I^f_{s,l} = I^d_{s,l} + ∑_{l_f ∈ IN(l)} q_{s,l_f,l} - ∑_{l_t ∈ OUT(l)} q_{s,l,l_t}\n5. Outflow availability (can’t send more than you have):\n∀s, ∀l_f: ∑_{l_t ∈ OUT(l_f)} q_{s,l_f,l_t} ≤ I^d_{s,l_f}\n6. Format compatibility for movements:\n∀s, ∀(l_f, l_t) ∈ ARCS: q_{s,l_f,l_t} ≤ M_{l_f} · E_{l_t,fmt(s)}\nInterpretation: Can only move SKU to location if destination format matches SKU format\n7. Capacity constraints:\n∀l: ∑_s I^f_{s,l} ≤ ∑_f Cap(f,l) · E_{l,f}\n8. Pack size compliance:\n∀s with Pack(s) &gt; 1, ∀(l_f, l_t) ∈ ARCS: q_{s,l_f,l_t} = Pack(s) · MQ_{s,l_f,l_t}\n9. Movement activation:\n∀s, ∀(l_f, l_t) ∈ ARCS: q_{s,l_f,l_t} ≤ M_{l_f} · Z_{s,l_f,l_t}\n10. Maximum actions limit (prevent excessive churn):\n∑_{s,(l_f,l_t)∈ARCS} Z_{s,l_f,l_t} ≤ 20\nInterpretation: Limit total number of replenishment moves per day (operational constraint)\n\n\n\nmin w_neardepot · ∑_s ∑_l demNorm_t(s) · DistDepot(l) · I^f_{s,l}  [Proximity to depot]\n    + w_sim · ∑_l ∑_{s&lt;s'} Sim(s,s') · W_{s,s',l}                   [Similarity penalty]\n    + w_move · ∑_s ∑_{(l_f,l_t)∈ARCS} q_{s,l_f,l_t} · C(l_f, l_t)    [Movement cost]\n    + w_actions · ∑_{s,(l_f,l_t)∈ARCS} Z_{s,l_f,l_t}                 [Action count penalty]\nTrade-off: Balance layout improvement (near depot, similarity) against operational disruption (movement cost, number of actions)."
  },
  {
    "objectID": "projects/optimisation.html#orchestration-kpi-tracking",
    "href": "projects/optimisation.html#orchestration-kpi-tracking",
    "title": "Warehouse Optimization (MILP)",
    "section": "",
    "text": "Each day follows the same pipeline with persistent KPI computation:\n\n\nBEFORE → DSLAP → PICK → REPLEN → AFTER\nKPI computation logic:\nKPIs are computed before and after each optimization stage. Critically, if a stage does not modify a variable, the corresponding KPI is preserved from the previous stage.\nWhy this matters:\nIf picking doesn’t change layout (only reduces inventory), then layout-related KPIs (weighted distance, dispersion, similarity) should remain unchanged. Recomputing them would introduce numerical noise and obscure which stage actually drove performance changes.\nImplementation:\ndef compute_kpis_with_persistence(stage_name, variables_changed, current_state, previous_kpis):\n    \"\"\"\n    Compute KPIs for current stage, persisting unchanged metrics from previous stage.\n    \"\"\"\n    kpis = {}\n    \n    if 'inventory' in variables_changed:\n        kpis['weighted_distance'] = compute_weighted_distance(current_state)\n        kpis['dispersion'] = compute_dispersion(current_state)\n        kpis['occupation'] = compute_occupation(current_state)\n    else:\n        # Persist from previous stage\n        kpis['weighted_distance'] = previous_kpis['weighted_distance']\n        kpis['dispersion'] = previous_kpis['dispersion']\n        kpis['occupation'] = previous_kpis['occupation']\n    \n    if 'format_assignment' in variables_changed:\n        kpis['similarity_penalty'] = compute_similarity(current_state)\n    else:\n        kpis['similarity_penalty'] = previous_kpis['similarity_penalty']\n    \n    return kpis\n\n\n\nLayout quality: - Weighted distance to depot: ∑_{s,l} demand_weight(s) · distance(depot, l) · inventory(s, l) - Dispersion: Number of distinct locations holding inventory - Occupation rate: total_inventory / total_available_capacity - Similarity penalty: ∑_{l} ∑_{s&lt;s'} Sim(s,s') · colocated(s, s', l)\nOperational metrics: - Layout churn: ∑_{s,l} |I^f_{s,l} - I^d_{s,l}| (total inventory movement within day) - Number of relocation actions: Count of executed movements (DSLAP + REPLEN) - Total movement cost: ∑ quantity_moved · distance\nPicking performance: - Turnover: Fraction of inventory picked per day - Workload balance: Coefficient of variation of picks across zones/locations - Fill rate: total_picked / total_demanded\nCapacity utilization: - Average utilization: mean(inventory / capacity) across active locations - Peak utilization: max(inventory / capacity) - Underutilized locations: Count of locations with utilization &lt; threshold"
  },
  {
    "objectID": "projects/optimisation.html#results-insights",
    "href": "projects/optimisation.html#results-insights",
    "title": "Warehouse Optimization (MILP)",
    "section": "",
    "text": "Configuration: - Horizon: T = 5 days - SKUs: 20 products with heterogeneous demand (Poisson-distributed) - Formats: 4 types with pack sizes {12, 25, 1000} - Orders: 10-30 per day with 2-5 line items each - Arrivals: Poisson-distributed, aligned with pack sizes - Solver: PuLP with CBC (timeout 300s per stage)\n\n\n\nLayout quality improvements (Day 1 → Day 5): - Weighted distance to depot: ↓ 18% for high-demand SKUs - Similarity penalty: ↓ 25% (fewer dissimilar SKUs co-located) - Dispersion: ↓ 12% (inventory more consolidated)\nOperational costs: - Relocation actions: Avg 15 moves/day (within operational limit of 20) - Movement cost: Controlled (weight tuning prevents excessive churn) - Receiving backlog: 0 (all daily arrivals cleared)\nPicking efficiency: - Average pick distance: ↓ 22% (high-demand items closer to depot) - Workload balance (CV): ↓ 15% (picks distributed more evenly across zones) - Fill rate: 95%+ (high order fulfillment with partial picks when necessary)\n\n\n\n1. Trade-offs become explicit:\nUnlike black-box heuristics, MILP formulations force explicit trade-offs via objective weights: - Increase w_neardepot → SKUs migrate toward depot, but movement cost increases - Increase w_actions → Fewer relocations, but layout quality degrades slower - Increase w_backlog → Receiving clears faster, but putaway distances may increase\nThis transparency is operationally valuable: Warehouse managers can see why certain decisions were made and adjust priorities accordingly.\n2. Feasibility is non-negotiable:\nAll solutions respect: - Physical capacity constraints - Format compatibility rules - Pack size requirements - Inventory conservation laws\nThis distinguishes MILP from heuristics that might produce “good” solutions that violate operational constraints in subtle ways.\n3. Stability matters as much as optimality:\nEarly experiments with aggressive replenishment (high w_neardepot, low w_actions) achieved excellent layout scores but created operational chaos: - 50+ relocations per day - Inventory constantly moving - Pickers unable to rely on stable locations\nTuning replenishment conservatively (limit 20 actions/day, higher movement cost) achieved 90% of layout quality improvement with 1/3 the operational disruption.\nThis is the essence of practical optimization: Not finding the theoretical optimum, but finding solutions that operators trust and can execute reliably."
  },
  {
    "objectID": "projects/optimisation.html#implementation-reproducibility-exports",
    "href": "projects/optimisation.html#implementation-reproducibility-exports",
    "title": "Warehouse Optimization (MILP)",
    "section": "",
    "text": "The pipeline systematically exports all artifacts for reproducibility and auditability:\n\n\nlocations2d.csv          # 2D node coordinates, types (Depot, recv, storage, path)\narcs2d.csv              # 2D edges with distances\nlocations3d.csv          # 3D node coordinates (with vertical levels)\narcs3d.csv              # 3D edges (horizontal + vertical movements)\n\n\n\ntime_periods.csv         # Days in planning horizon\nskus.csv                # Product catalog (SKU, format, pack size, demand patterns)\nformats.csv             # Format definitions\ncapacities.csv          # Cap(format, location)\narrivals.csv            # A(t, recv, s) - inbound shipments\ninitial_inventory.csv    # I₀(s, l) - starting state (validated feasible)\norders.csv              # Customer orders (order ID, day, SKU, quantity)\ndistance_matrix.csv      # C(l_i, l_j) - shortest path costs\n\n\n\nbase_Q.csv              # Putaway decisions: Q(t, s, l)\nbase_p.csv              # Picking decisions: p(t, s, l, n)\nbase_q.csv              # Replenishment flows: q(s, l_f, l_t)\nbase_I_f.csv            # End-of-day inventory: I^f(t, s, l)\n\n\n\nkpis_results.csv        # Structured KPIs:\n                        # - Stage (BEFORE, AFTER_DSLAP, AFTER_PICK, AFTER_REPLEN)\n                        # - Day\n                        # - Metric name\n                        # - Value\n                        # - Delta from previous stage\nWhy this matters for production deployment:\n\nAuditability: Every optimization run is fully reproducible\nDebugging: Can trace exactly which stage/day caused KPI changes\nValidation: Can verify solver outputs against operational constraints\nContinuous improvement: Can A/B test different weight configurations systematically"
  },
  {
    "objectID": "projects/optimisation.html#current-limitations-technical-challenges",
    "href": "projects/optimisation.html#current-limitations-technical-challenges",
    "title": "Warehouse Optimization (MILP)",
    "section": "",
    "text": "Problem: Replenishment depends on completeness of arc set. If ARCS is too sparse (Top-K too small), some beneficial relocations become impossible.\nManifestation: If location A has excess inventory and location B near depot is underutilized, but (A, B) ∉ ARCS, replenishment cannot improve layout.\nMitigation strategies: - Increase K (more neighbors) - Use zone-based arcs (all locations within same zone are mutually connected) - Hybrid: Top-K + strategic arcs (e.g., all → depot zone)\n\n\n\nProblem: If w_move = 0 and w_neardepot = 0, then q = 0 is optimal (no replenishment). Conversely, if w_actions too low, model relocates excessively.\nCurrent approach: Manual tuning via sensitivity analysis.\nBetter approach (future work): - Multi-objective optimization (Pareto frontier exploration) - Reinforcement learning to learn weight policies from historical performance - Stakeholder elicitation (conjoint analysis to infer implicit preferences)\n\n\n\nProblem: Similarity tracking uses W_{s,s',l} variables: O(|S|² × |L|) binary variables (e.g., 20² × 100 = 40,000 variables).\nImpact: Scales poorly for large product catalogs.\nMitigation strategies: - SKU clustering: Group similar products, track similarity at cluster level - Approximation: Sample SKU pairs for similarity penalty instead of exhaustive enumeration - Decomposition: Track similarity only for active locations (locations with inventory)\n\n\n\nCurrent: CBC (open-source) solves instances with T=5, |S|=20, |L|=100 in ~60s per stage.\nProjected: Real warehouse with T=7, |S|=500, |L|=2000 would exceed computational budget.\nScalability strategies: - Decomposition: Solve by warehouse zone (with coupling constraints) - Rolling horizon: Optimize days 1-3 with high fidelity, days 4-5 with aggregation - Warm start: Use previous day’s solution as MIP start for next day - Commercial solver: Gurobi, CPLEX significantly faster than CBC for large MILP"
  },
  {
    "objectID": "projects/optimisation.html#future-work-next-steps",
    "href": "projects/optimisation.html#future-work-next-steps",
    "title": "Warehouse Optimization (MILP)",
    "section": "",
    "text": "Current approach uses weighted sum of objectives. This requires manual weight tuning and obscures trade-off structure.\nProposed: Generate Pareto frontier using ε-constraint method:\n# Fix all objectives except one, vary bounds on fixed objectives\nfor epsilon in [0.1, 0.2, 0.3, ...]:\n    model = MILP_model()\n    model.objective = minimize(near_depot_cost)\n    model.add_constraint(move_cost &lt;= epsilon * baseline_move_cost)\n    model.add_constraint(actions_count &lt;= epsilon * baseline_actions)\n    solutions.append(model.solve())\n\n# Present Pareto frontier to stakeholders for preference elicitation\n\n\n\nCurrent models use deterministic demand (known orders for day t).\nReality: Day t+1, t+2, ... demands are uncertain.\nProposed: Stochastic programming or robust optimization:\n# Generate demand scenarios\nscenarios = generate_demand_scenarios(historical_data, n_scenarios=100)\n\n# Two-stage stochastic MILP:\n# Stage 1: Slotting decisions (here-and-now)\n# Stage 2: Picking decisions (wait-and-see, per scenario)\nmodel.objective = first_stage_cost + expected_value(second_stage_cost, scenarios)\nBenefit: Slotting decisions hedge against demand uncertainty, not just optimize for expected demand.\n\n\n\nFor warehouses with hundreds of SKUs, track similarity and co-location at cluster level:\n# K-means clustering on (demand_pattern, physical_attributes)\nclusters = cluster_skus(skus, n_clusters=10)\n\n# Reformulate similarity: W_{c,c',l} for clusters c, c'\n# Variables reduced from O(|S|²) to O(|C|²)\n\n\n\nCurrent: No benchmark to validate that MILP is actually better than simpler heuristics.\nProposed: Implement random admissible policy: - Slotting: Place arrivals randomly at locations with compatible format and available capacity - Picking: Pick randomly from locations with inventory - Replenishment: Randomly relocate up to 20 moves per day\nComparison: MILP vs. Random on same data over 30 runs.\nHypothesis: MILP should achieve 20-30% better weighted distance with comparable operational churn."
  },
  {
    "objectID": "projects/optimisation.html#conclusion-why-this-framework-matters",
    "href": "projects/optimisation.html#conclusion-why-this-framework-matters",
    "title": "Warehouse Optimization (MILP)",
    "section": "",
    "text": "This project provides a complete, reproducible foundation for warehouse optimization that respects operational reality:\n✅ Feasibility guaranteed: All solutions respect capacity, format compatibility, pack sizes, inventory conservation\n✅ Operational realism: Models sequential decision-making (slotting → picking → replenishment), not isolated optimization\n✅ Transparency: Explicit objective trade-offs, interpretable decisions, full auditability\n✅ Stability-aware: Balances layout quality improvement against operational churn\n✅ Production-ready infrastructure: Automated data generation, validation, orchestration, KPI tracking, exports\nThis is not an academic exercise.\nThis is a framework designed to actually run in production warehouses, where: - Operators need to trust recommendations - Feasibility failures are catastrophic - Stability matters as much as optimality - Decisions must be explainable to non-technical stakeholders"
  },
  {
    "objectID": "projects/optimisation.html#key-takeaway",
    "href": "projects/optimisation.html#key-takeaway",
    "title": "Warehouse Optimization (MILP)",
    "section": "",
    "text": "A good MILP model is not the one that finds the mathematically optimal solution.\nIt’s the one that produces feasible, explainable, and stable decisions day after day, even when reality deviates from assumptions.\n\nThis framework achieves that.\n\nTech Stack:\nPython · PuLP · Gurobi · Pandas · NumPy · NetworkX · Plotly · Matplotlib\nRepository:\nGitHub – WA2 Warehouse Optimization Realesed soon\nRelated Work:\n- Why Feasibility Matters More Than Optimality - Why Optimization Problems Expose the Limits of AI Reasoning\n\nOptimization is not about finding the perfect solution.\nIt’s about finding solutions that survive contact with operational reality."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "This section groups my work into a few themes.\nEach page contains selected projects, with a short problem framing, method, deliverables, and links to code when available.\n\n\n\nForecasting\nOptimisation déterministe (MILP)\nOptimisation stochastique\nReinforcement Learning   \n\n\n\n\n\n\n\nA modular MILP framework for warehouse decisions across time (placement, picking, replenishment), with KPI tracking (occupancy, dispersion, distance-to-depot, movement cost).\nTech: Python · PuLP/Gurobi · Plotly · NetworkX\nRepo: Released soon\n\n\n\nEnd-to-end forecasting pipeline for energy demand with rolling evaluation and production-ready reporting.\nTech: Python · Statsmodels · TensorFlow · Scikit-learn · Plotly\nRepo: Released soon\n\n\n\nData pipelines from operational systems to analytics layers and Power BI dashboards, with data quality checks and KPI definitions.\nTech: SQL · Power BI · Python · ETL/ELT · Databricks\nRepo: Released soon"
  },
  {
    "objectID": "projects/index.html#choose-a-category",
    "href": "projects/index.html#choose-a-category",
    "title": "Projects",
    "section": "",
    "text": "Forecasting\nOptimisation déterministe (MILP)\nOptimisation stochastique\nReinforcement Learning"
  },
  {
    "objectID": "projects/index.html#highlights",
    "href": "projects/index.html#highlights",
    "title": "Projects",
    "section": "",
    "text": "A modular MILP framework for warehouse decisions across time (placement, picking, replenishment), with KPI tracking (occupancy, dispersion, distance-to-depot, movement cost).\nTech: Python · PuLP/Gurobi · Plotly · NetworkX\nRepo: Released soon\n\n\n\nEnd-to-end forecasting pipeline for energy demand with rolling evaluation and production-ready reporting.\nTech: Python · Statsmodels · TensorFlow · Scikit-learn · Plotly\nRepo: Released soon\n\n\n\nData pipelines from operational systems to analytics layers and Power BI dashboards, with data quality checks and KPI definitions.\nTech: SQL · Power BI · Python · ETL/ELT · Databricks\nRepo: Released soon"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "This blog is a space for technical reflections on data science, optimization, and AI systems.\nI focus on model design, decision-making, and operational realism, often taking a skeptical stance toward current AI hype, especially when AI is presented as a replacement for structured reasoning in optimization and forecasting.\n\n\n\n\n\nA critical look at large language models and their limits in scientific reasoning\n👉 Read article →\n\n\n\n\nWhere heuristics, feasibility, and constraints defeat pattern-based intelligence\n👉 Read article →\n\n\n\nThe operational importance of feasible solutions over theoretically optimal ones 👉 Read article →\n\n\n\n\n\n\nOptimization & MILP modeling\nForecasting and time series\nModel evaluation and failure modes\nMLOps and production constraints\nLessons from real industrial projects"
  },
  {
    "objectID": "blog/index.html#recent-posts",
    "href": "blog/index.html#recent-posts",
    "title": "Blog",
    "section": "",
    "text": "A critical look at large language models and their limits in scientific reasoning\n👉 Read article →\n\n\n\n\nWhere heuristics, feasibility, and constraints defeat pattern-based intelligence\n👉 Read article →\n\n\n\nThe operational importance of feasible solutions over theoretically optimal ones 👉 Read article →"
  },
  {
    "objectID": "blog/index.html#topics-covered",
    "href": "blog/index.html#topics-covered",
    "title": "Blog",
    "section": "",
    "text": "Optimization & MILP modeling\nForecasting and time series\nModel evaluation and failure modes\nMLOps and production constraints\nLessons from real industrial projects"
  },
  {
    "objectID": "blog/2026-ia-optimization-limits.html",
    "href": "blog/2026-ia-optimization-limits.html",
    "title": "Why Optimization Problems Expose the Limits of AI Reasoning",
    "section": "",
    "text": "If you want to understand the true limits of AI reasoning, don’t ask it to write poetry or summarize articles.\nAsk it to design an optimization model.\nNot generate code for a textbook example. Not reproduce a standard formulation from a paper. But actually design a model for a messy, real-world operational problem with competing objectives, hard constraints, and no clear precedent.\nWatch it fail—fluently, confidently, convincingly—in ways that won’t become obvious until you try to solve the model at scale or deploy it in production.\nI’ve debugged enough AI-generated optimization models to recognize a pattern: AI excels at prediction, but struggles fundamentally with prescription. And optimization is the ultimate test of prescriptive reasoning.\nHere’s why."
  },
  {
    "objectID": "blog/2026-ia-optimization-limits.html#introduction-the-unforgiving-test",
    "href": "blog/2026-ia-optimization-limits.html#introduction-the-unforgiving-test",
    "title": "Why Optimization Problems Expose the Limits of AI Reasoning",
    "section": "",
    "text": "If you want to understand the true limits of AI reasoning, don’t ask it to write poetry or summarize articles.\nAsk it to design an optimization model.\nNot generate code for a textbook example. Not reproduce a standard formulation from a paper. But actually design a model for a messy, real-world operational problem with competing objectives, hard constraints, and no clear precedent.\nWatch it fail—fluently, confidently, convincingly—in ways that won’t become obvious until you try to solve the model at scale or deploy it in production.\nI’ve debugged enough AI-generated optimization models to recognize a pattern: AI excels at prediction, but struggles fundamentally with prescription. And optimization is the ultimate test of prescriptive reasoning.\nHere’s why."
  },
  {
    "objectID": "blog/2026-ia-optimization-limits.html#optimization-is-not-prediction",
    "href": "blog/2026-ia-optimization-limits.html#optimization-is-not-prediction",
    "title": "Why Optimization Problems Expose the Limits of AI Reasoning",
    "section": "Optimization is not prediction",
    "text": "Optimization is not prediction\nMost successes of modern AI come from prediction problems: classification, regression, pattern completion, sequence generation.\nThese are fundamentally probabilistic tasks. They ask: - What is likely given the data? - What pattern best fits this distribution? - What comes next in this sequence?\nOptimization problems are categorically different. They are not about what is likely, but about what is allowed and what is best within those boundaries.\nThe shift is profound:\n\n\n\n\n\n\n\nPrediction\nOptimization\n\n\n\n\nMinimize expected error\nMaximize/minimize objective subject to constraints\n\n\nProbabilistic correctness\nAbsolute feasibility requirement\n\n\nApproximation is acceptable\nInfeasibility is catastrophic\n\n\nGradient descent, backprop\nCombinatorial search, branch-and-bound\n\n\nPerformance degrades gracefully\nPerformance collapses at constraint violation\n\n\n\nIn prediction, being 95% accurate is often excellent.\nIn optimization, a solution that violates even one hard constraint is worthless—worse than no solution at all, because it creates false confidence.\nFeasibility comes before performance.\nAI systems trained on prediction tasks don’t internalize this. They’ve learned to approximate, to generalize, to find “good enough” patterns. Optimization doesn’t care about “good enough” when it comes to constraints. It demands exact compliance."
  },
  {
    "objectID": "blog/2026-ia-optimization-limits.html#constraints-are-not-suggestions",
    "href": "blog/2026-ia-optimization-limits.html#constraints-are-not-suggestions",
    "title": "Why Optimization Problems Expose the Limits of AI Reasoning",
    "section": "Constraints are not suggestions",
    "text": "Constraints are not suggestions\nIn optimization, constraints are absolute. Violating one constraint invalidates the entire solution.\nConsider a real example from my work in warehouse optimization: - “Each SKU must be assigned to exactly one location” → Hard constraint. Not negotiable. - “High-turnover items should be near packing stations” → Soft preference. Optimizable.\nAn AI system often cannot reliably distinguish these.\n\nHow AI systems fail with constraints:\n1. Softening constraints unintentionally\nAI will see patterns where hard constraints were relaxed in training data (for tractability, simplification, or specific use cases) and generalize that relaxation inappropriately.\nExample: A model trained on academic benchmarks where capacity constraints were relaxed to “at most 110% of nominal capacity” will propose the same relaxation for a production system where exceeding physical capacity means literal overflow, equipment damage, or regulatory violations.\n2. Confusing penalties with guarantees\nAI frequently suggests penalty-based formulations for hard constraints:\n# objective function with penalty for constraint violation\nobjective = minimize(cost + 1000 * constraint_violation)\nThis looks plausible. It even works—sometimes.\nBut it’s fundamentally wrong for a hard constraint. The penalty coefficient (1000) is arbitrary. If the cost savings from violating the constraint exceed 1000, the solver will happily violate it. You’ve turned an absolute requirement into a negotiable trade-off.\nA human optimizer knows: hard constraints belong in the constraint set, not the objective function.\n3. Optimizing objectives while breaking feasibility\nI’ve seen AI-generated models that beautifully minimize transportation costs while quietly violating: - Precedence constraints (pickup before delivery), - Time windows (deliveries after customer closing), - Resource limits (vehicles carrying more than capacity).\nThe objective improves. The KPIs look great. The solution is operationally impossible.\n\n\nHumans think in invariants\nWhen I design an optimization model, I start with invariants—conditions that must hold regardless of the objective value:\n\nMass balance: what goes in must equal what comes out,\nLogical precedence: you cannot consume inventory before receiving it,\nPhysical limits: capacity, speed, throughput ceilings.\n\nThese aren’t optimization variables. They’re boundary conditions of reality.\nAI doesn’t reason about invariants. It pattern-matches formulations that superficially resemble the problem, without understanding which elements are negotiable and which are non-negotiable."
  },
  {
    "objectID": "blog/2026-ia-optimization-limits.html#the-combinatorial-explosion-problem",
    "href": "blog/2026-ia-optimization-limits.html#the-combinatorial-explosion-problem",
    "title": "Why Optimization Problems Expose the Limits of AI Reasoning",
    "section": "The combinatorial explosion problem",
    "text": "The combinatorial explosion problem\nOptimization models live in discrete spaces. Small modeling choices create exponential consequences.\nChoosing whether a variable is: - Binary or continuous → Binary: enables logical constraints but explodes search space; Continuous: faster to solve but may require rounding heuristics, - Indexed by time or aggregated → Time-indexed: captures dynamics but multiplies decision variables; Aggregated: compact but loses temporal resolution, - Local or global → Local subproblems: decomposable, parallelizable; Global: guarantees optimality but may be intractable.\nEach decision fundamentally reshapes the computational geometry of the problem.\n\nA concrete example: production scheduling\nFormulation A (time-indexed binary variables):\nx[product, machine, hour] ∈ {0,1}  # Binary assignment\n\n100 products × 10 machines × 168 hours/week = 168,000 binary variables\nSolver runtime: potentially hours or days\nSolution quality: potentially optimal\n\nFormulation B (aggregated continuous variables with sequencing):\ny[product, machine] ∈ ℝ+  # Continuous production quantity\nz[product_i, product_j, machine] ∈ {0,1}  # Sequencing\n\n100×10 continuous + 100×99×10 binary sequencing variables = 100,000 variables\nSolver runtime: potentially minutes\nSolution quality: optimal within aggregated time structure\n\nSame problem. Different modeling choices. Orders of magnitude difference in solve time.\nA human optimizer makes this choice based on: - Decision frequency (hourly precision needed?), - Problem scale (can we afford 168k binaries?), - Operational constraints (are setups sequence-dependent?), - Solver capabilities (CPLEX, Gurobi, open-source?).\nAI systems do not reason about computational geometry.\nThey’ve seen both formulations in training data. They’ll suggest one—possibly the wrong one—without understanding the computational consequences. They don’t think: - “This problem instance has 50,000 SKUs; time-indexed binaries will never solve.” - “Aggregation will lose critical sequencing information for this use case.”\nThey replay patterns that worked elsewhere, agnostic to scale, solver architecture, or operational context."
  },
  {
    "objectID": "blog/2026-ia-optimization-limits.html#where-ai-helps-and-where-it-does-not",
    "href": "blog/2026-ia-optimization-limits.html#where-ai-helps-and-where-it-does-not",
    "title": "Why Optimization Problems Expose the Limits of AI Reasoning",
    "section": "Where AI helps — and where it does not",
    "text": "Where AI helps — and where it does not\nLet me be precise about where AI adds value in optimization work—and where it becomes a liability.\n\nPROS : ✅ Where AI is genuinely useful:\n1. Generating initial formulations Starting from a verbal problem description, AI can scaffold a baseline mathematical model—variables, objective, basic constraints. This accelerates the drafting phase, especially for standard problem types (shortest path, knapsack, assignment).\n2. Suggesting alternative constraint formulations Given a constraint in one form, AI can propose equivalent reformulations: - Big-M constraints → Indicator constraints - Quadratic penalties → Piecewise linear approximations - Logical implications → Disjunctive constraints\nThis is valuable for exploring tractability trade-offs.\n3. Exploring modeling variants AI can rapidly generate multiple versions of a model with different assumptions, helping you understand the sensitivity of the formulation to structural choices.\n4. Documentation and explanation Translating mathematical notation into plain language for stakeholders, generating solver configuration docs, explaining dual variables and shadow prices—AI excels here.\n\n\nCONS : ❌ Where AI is fundamentally unreliable:\n1. Deciding decomposition strategies Should you decompose this supply chain optimization into: - Geography-based subproblems? - Time-rolling horizons? - Product family clusters?\nThis requires understanding problem structure, coupling strength, and recourse decisions. AI cannot reliably reason about this—it will suggest decompositions that worked in training examples without validating applicability.\n2. Guaranteeing feasibility under operational stress What happens when: - Demand spikes 200% above forecast? - A key supplier goes offline? - Regulatory constraints change mid-horizon?\nAI-generated models often validate on historical data but fail catastrophically on edge cases they haven’t seen. Stress-testing formulations requires adversarial thinking—anticipating failures, not just fitting patterns.\n3. Designing KPIs aligned with operations Minimizing total cost is not the same as minimizing cost volatility.\nMaximizing throughput is not the same as maximizing throughput stability.\nReducing inventory is not the same as reducing stockout risk.\nChoosing the right objective function requires operational intuition—understanding what stakeholders actually care about, what metrics drive behavior, and what unintended consequences might emerge. AI doesn’t have stakeholders. It doesn’t own P&L. It optimizes what it’s told to optimize, even if it’s the wrong thing."
  },
  {
    "objectID": "blog/2026-ia-optimization-limits.html#a-warning-for-practitioners",
    "href": "blog/2026-ia-optimization-limits.html#a-warning-for-practitioners",
    "title": "Why Optimization Problems Expose the Limits of AI Reasoning",
    "section": "A warning for practitioners",
    "text": "A warning for practitioners\nI’ve seen this pattern too many times:\n\nBusiness problem arrives → “Let’s use AI to design the optimization model”\nAI generates a plausible-looking formulation → Team assumes it’s correct\nImplementation begins → Code works, solver runs, outputs generated\nDeployment → Model fails in production (infeasibility, poor performance, wrong KPIs)\nDebugging → Weeks spent untangling assumptions embedded in AI-generated structure\n\nBlind trust in AI-assisted optimization leads to:\n\nBrittle models that work on average but fail on edge cases,\nOpaque assumptions that no one validated because “the AI generated it”,\nFalse confidence because the model produces numbers—even if they’re operationally meaningless.\n\n\nThe insidious part:\nPoor optimization models don’t fail with error messages.\nThey fail by silently producing suboptimal or infeasible recommendations that degrade performance over time.\nYou don’t get a stack trace.\nYou get declining KPIs, frustrated stakeholders, and a slow erosion of trust in analytics.\nOptimization rewards humility.\nIt demands that you question every assumption, validate every constraint, stress-test every edge case.\nAI currently does not have humility.\nIt produces formulations with confidence inversely proportional to its actual understanding."
  },
  {
    "objectID": "blog/2026-ia-optimization-limits.html#final-thoughts-the-test-of-prescriptive-reasoning",
    "href": "blog/2026-ia-optimization-limits.html#final-thoughts-the-test-of-prescriptive-reasoning",
    "title": "Why Optimization Problems Expose the Limits of AI Reasoning",
    "section": "Final thoughts: The test of prescriptive reasoning",
    "text": "Final thoughts: The test of prescriptive reasoning\nOptimization problems are unforgiving.\nThey expose the difference between: - Appearance of intelligence (fluent explanations, plausible formulations), - And commitment to correctness (absolute feasibility, validated assumptions, operational accountability).\nPrediction tasks allow graceful degradation. Get 95% of classifications right, and you’re doing well.\nOptimization tasks are binary in outcomes: either your solution is feasible and improves operations, or it’s not and you’ve wasted effort.\nThere’s no middle ground.\nThis is why optimization is the ultimate test for AI reasoning.\nIt requires: - Understanding invariants (non-negotiable constraints), - Reasoning about computational complexity (tractability vs. optimality trade-offs), - Managing discrete spaces (combinatorial explosion), - Validating edge cases (adversarial stress-testing), - Aligning mathematical objectives with operational reality.\nCurrent AI systems can simulate competence on the first pass.\nThey cannot sustain it under scrutiny.\nUntil AI can reason under constraints—truly reason, not pattern-match—it will remain an assistant, not a designer."
  },
  {
    "objectID": "blog/2026-ia-optimization-limits.html#where-does-this-leave-us",
    "href": "blog/2026-ia-optimization-limits.html#where-does-this-leave-us",
    "title": "Why Optimization Problems Expose the Limits of AI Reasoning",
    "section": "Where does this leave us?",
    "text": "Where does this leave us?\nI use AI extensively in my optimization work.\nBut I never delegate model design to it.\nI use it to: - Accelerate formulation drafting, - Explore alternative structures, - Generate solver configurations and documentation.\nI do not use it to: - Make foundational modeling decisions, - Choose decomposition strategies, - Validate feasibility under operational stress, - Define objectives aligned with business reality.\nThe final responsibility—the commitment to deploy—remains human.\nBecause when the model fails in production, I own that failure.\nAnd I cannot outsource accountability to a statistical approximator.\n\nIn optimization, elegance is optional.\nFeasibility is not.\nAnd until AI understands the difference,\nthe hardest decisions must remain ours.\n\nHave you deployed AI-assisted optimization models? What worked? What failed spectacularly? I’d love to hear war stories—reach out on LinkedIn."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Georges Lemuel Balogog",
    "section": "",
    "text": "Data Science · Optimization · Data Engineering · Electrical Engineering\nI build end-to-end analytical solutions: ingestion/ETL, predictive modeling, optimization (MILP/MIP), deployment (API/CI/CD), and decision-making dashboards.\nDownload CV"
  },
  {
    "objectID": "index.html#featured-projects",
    "href": "index.html#featured-projects",
    "title": "Georges Lemuel Balogog",
    "section": "Featured Projects",
    "text": "Featured Projects\nA selection of my recent work.\n\n\n\nWarehouse Optimization Solving MIP problems for storage optimization.\nMIP Models for slotting, picking, and replenishment with KPI tracking and export.\nTech: Python · PuLP/Gurobi · NetworkX · Plotly\n\n\n\nEnergy Forecasting Predicting energy demand with Stats & ML models.\nDemand forecasting using sliding window approach with TCN/RNN models, SARIMAX models, evaluated with MAE/RMSE/MAPE metrics and monitoring.\nTech: Python · TensorFlow · Pandas · ML lifecycle tools - R · Prophet · Statsmodels · Forecasting\n\n\n\nData Integration & BI ETL pipelines to Power BI dashboards.\nPipelines from various data sources (SQL Server, Excel, APIs) to Power BI dashboards with data quality checks and DAX measures.\nTech: Azure Databrick · SQL · Power BI · ETL/ELT · Data quality monitoring · DAX\n\nRead more →"
  },
  {
    "objectID": "index.html#what-i-do",
    "href": "index.html#what-i-do",
    "title": "Georges Lemuel Balogog",
    "section": "What I Do",
    "text": "What I Do\nMy key areas of expertise.\n\n\nReinforcement Learning\nDeveloping RL algorithms for decision-making.\n\n\nData Science & Machine Learning\nAdvanced analytics and predictive modeling.\n\n\nOptimization & Operations Research\nSolving complex optimization problems.\n\n\nData Engineering\nBuilding robust and scalable data pipelines.\n\n\nHeuristic & Metaheuristic Algorithms\nDesigning algorithms for complex problem-solving."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My story begins in the world of electrons and electromagnetic fields—where precision matters, where a single miscalculation can mean the difference between light and darkness, between connection and silence. As a telecommunications and electrical engineer, I learned to think in systems: power grids, signal propagation, network reliability. I learned that complexity can be tamed through rigorous modeling, that uncertainty demands probabilistic thinking, and that real-world impact requires solutions that actually work under pressure.\nBut somewhere along that journey, I discovered something profound: the same principles that govern electrical networks could transform how we understand data, optimize operations, and build intelligent systems. Energy forecasting isn’t just about predicting kilowatts—it’s about understanding patterns in chaos, seasonality in noise, and making decisions when the stakes are high. Smart microgrids aren’t just hardware—they’re real-time data pipelines demanding millisecond-level decision-making.\nThat realization changed everything.\nToday, I stand at the intersection of data engineering, artificial intelligence, and prescriptive optimization—not as separate disciplines, but as a unified force for solving problems that matter. I design scalable data pipelines that don’t just move information, but transform it into actionable intelligence. I build optimization models that don’t live in academic papers, but drive real warehouse operations, reduce preparation times, and improve decision quality. I deploy machine learning systems with the same rigor I once applied to electrical grids: robust, monitored, production-ready.\nFrom ERP integrations (Dynamics 365 SCM) to multivariate time series forecasting, from MILP warehouse slotting to MLOps pipelines (MLflow, Kubernetes, CI/CD)—every project is a bridge between mathematical elegance and operational reality. I don’t just clean data; I ensure its integrity, lineage, and governance. I don’t just build models; I validate them, measure them, deploy them, and own their performance in production.\nBut here’s what drives me most: I’m still learning. Still growing. Still hungry.\nRight now, I’m diving deep into Azure Databricks and Apache Spark, pushing the boundaries of distributed computing for analytics at scale. I’m exploring the bleeding edge of supply chain optimization, where every percentage point of efficiency translates to tangible impact. I’m continuously refining my craft in statistical modeling, cloud architecture, and AI deployment—because in this field, standing still means falling behind.\nI carry forward the mindset of an engineer who once analyzed electromagnetic disturbances in telecommunications networks: obsessive about details, relentless about validation, and committed to solutions that don’t just work on paper—they work when it counts.\nWhether it’s designing ETL workflows for decision-support platforms, implementing real-time monitoring dashboards, or solving complex prescriptive optimization problems, I bring the same energy: clarity in complexity, measurement over assumptions, and execution that delivers value.\nThis is more than a career. It’s a continuous pursuit of excellence at the frontier of data, AI, and optimization. The problems are getting harder. The datasets are getting bigger. The stakes are getting higher.\nAnd I wouldn’t have it any other way.\n\n\n\n\n\nClarity: readable code, maintainable pipelines, understandable models\nMeasurement: KPIs that matter, rigorous validation, scenario analysis\n\nExecution: from prototype to production, automation, CI/CD, real impact\nGrowth: continuous learning, pushing boundaries, staying at the edge\n\n\n\n\n\n\n\n\nWhether you’re tackling supply chain optimization, building analytics platforms, or deploying AI systems at scale—I bring technical depth, operational awareness, and an unrelenting commitment to delivering solutions that work.\n📩 georges.balogog@yahoo.fr\n🔗 LinkedIn · GitHub\n\nFrom electrical grids to data pipelines. From signal processing to machine learning. From theory to production. Always evolving. Always delivering."
  },
  {
    "objectID": "about.html#from-voltage-to-value-a-journey-through-data-intelligence-impact",
    "href": "about.html#from-voltage-to-value-a-journey-through-data-intelligence-impact",
    "title": "About Me",
    "section": "",
    "text": "My story begins in the world of electrons and electromagnetic fields—where precision matters, where a single miscalculation can mean the difference between light and darkness, between connection and silence. As a telecommunications and electrical engineer, I learned to think in systems: power grids, signal propagation, network reliability. I learned that complexity can be tamed through rigorous modeling, that uncertainty demands probabilistic thinking, and that real-world impact requires solutions that actually work under pressure.\nBut somewhere along that journey, I discovered something profound: the same principles that govern electrical networks could transform how we understand data, optimize operations, and build intelligent systems. Energy forecasting isn’t just about predicting kilowatts—it’s about understanding patterns in chaos, seasonality in noise, and making decisions when the stakes are high. Smart microgrids aren’t just hardware—they’re real-time data pipelines demanding millisecond-level decision-making.\nThat realization changed everything.\nToday, I stand at the intersection of data engineering, artificial intelligence, and prescriptive optimization—not as separate disciplines, but as a unified force for solving problems that matter. I design scalable data pipelines that don’t just move information, but transform it into actionable intelligence. I build optimization models that don’t live in academic papers, but drive real warehouse operations, reduce preparation times, and improve decision quality. I deploy machine learning systems with the same rigor I once applied to electrical grids: robust, monitored, production-ready.\nFrom ERP integrations (Dynamics 365 SCM) to multivariate time series forecasting, from MILP warehouse slotting to MLOps pipelines (MLflow, Kubernetes, CI/CD)—every project is a bridge between mathematical elegance and operational reality. I don’t just clean data; I ensure its integrity, lineage, and governance. I don’t just build models; I validate them, measure them, deploy them, and own their performance in production.\nBut here’s what drives me most: I’m still learning. Still growing. Still hungry.\nRight now, I’m diving deep into Azure Databricks and Apache Spark, pushing the boundaries of distributed computing for analytics at scale. I’m exploring the bleeding edge of supply chain optimization, where every percentage point of efficiency translates to tangible impact. I’m continuously refining my craft in statistical modeling, cloud architecture, and AI deployment—because in this field, standing still means falling behind.\nI carry forward the mindset of an engineer who once analyzed electromagnetic disturbances in telecommunications networks: obsessive about details, relentless about validation, and committed to solutions that don’t just work on paper—they work when it counts.\nWhether it’s designing ETL workflows for decision-support platforms, implementing real-time monitoring dashboards, or solving complex prescriptive optimization problems, I bring the same energy: clarity in complexity, measurement over assumptions, and execution that delivers value.\nThis is more than a career. It’s a continuous pursuit of excellence at the frontier of data, AI, and optimization. The problems are getting harder. The datasets are getting bigger. The stakes are getting higher.\nAnd I wouldn’t have it any other way."
  },
  {
    "objectID": "about.html#what-i-value",
    "href": "about.html#what-i-value",
    "title": "About Me",
    "section": "",
    "text": "Clarity: readable code, maintainable pipelines, understandable models\nMeasurement: KPIs that matter, rigorous validation, scenario analysis\n\nExecution: from prototype to production, automation, CI/CD, real impact\nGrowth: continuous learning, pushing boundaries, staying at the edge"
  },
  {
    "objectID": "about.html#lets-build-something-that-matters",
    "href": "about.html#lets-build-something-that-matters",
    "title": "About Me",
    "section": "",
    "text": "Whether you’re tackling supply chain optimization, building analytics platforms, or deploying AI systems at scale—I bring technical depth, operational awareness, and an unrelenting commitment to delivering solutions that work.\n📩 georges.balogog@yahoo.fr\n🔗 LinkedIn · GitHub\n\nFrom electrical grids to data pipelines. From signal processing to machine learning. From theory to production. Always evolving. Always delivering."
  },
  {
    "objectID": "blog/2026-ia-illusion-thinking.html",
    "href": "blog/2026-ia-illusion-thinking.html",
    "title": "Can AI Really Think About Model Design?",
    "section": "",
    "text": "Recent advances in large language models have reignited an old question: can machines truly reason, or do they merely reproduce convincing patterns?\nIn applied data science and optimization, this question is not philosophical. It becomes practical and painful as soon as we ask AI systems to assist in model design, not just code generation.\nI’ve spent years designing optimization models for supply chains, energy systems, and operational planning. I’ve watched AI systems generate impressively fluent formulations—and I’ve also debugged the silent failures they create when deployed. This post is about the gap between what AI can articulate and what it can actually understand."
  },
  {
    "objectID": "blog/2026-ia-illusion-thinking.html#introduction",
    "href": "blog/2026-ia-illusion-thinking.html#introduction",
    "title": "Can AI Really Think About Model Design?",
    "section": "",
    "text": "Recent advances in large language models have reignited an old question: can machines truly reason, or do they merely reproduce convincing patterns?\nIn applied data science and optimization, this question is not philosophical. It becomes practical and painful as soon as we ask AI systems to assist in model design, not just code generation.\nI’ve spent years designing optimization models for supply chains, energy systems, and operational planning. I’ve watched AI systems generate impressively fluent formulations—and I’ve also debugged the silent failures they create when deployed. This post is about the gap between what AI can articulate and what it can actually understand."
  },
  {
    "objectID": "blog/2026-ia-illusion-thinking.html#fluency-is-not-understanding",
    "href": "blog/2026-ia-illusion-thinking.html#fluency-is-not-understanding",
    "title": "Can AI Really Think About Model Design?",
    "section": "Fluency is not understanding",
    "text": "Fluency is not understanding\nModern AI systems are remarkably fluent. They can describe optimization models, recite textbook formulations, and even generate plausible mathematical expressions with impressive syntax.\nWhat they cannot reliably do is decide why a model should be designed one way rather than another.\nThey often fail to distinguish between: - a theoretically elegant formulation, - a computationally feasible one, - and an operationally usable one.\nExample from practice:\nAsk an AI to design a warehouse slotting optimization model. It will confidently propose a Mixed-Integer Linear Program (MILP) with binary variables for every SKU-location pair. Mathematically correct. Computationally catastrophic at scale—thousands of products, hundreds of locations, and solver runtimes exploding into hours or days.\nA human optimizer would immediately ask: - Can we decompose this by zone? - Should we relax some integer constraints? - What’s the real decision frequency—daily, weekly, quarterly?\nThese questions don’t come from pattern matching. They come from operational intuition forged through failure and iteration.\nThis distinction is precisely where human expertise matters."
  },
  {
    "objectID": "blog/2026-ia-illusion-thinking.html#model-design-is-a-sequence-of-commitments",
    "href": "blog/2026-ia-illusion-thinking.html#model-design-is-a-sequence-of-commitments",
    "title": "Can AI Really Think About Model Design?",
    "section": "Model design is a sequence of commitments",
    "text": "Model design is a sequence of commitments\nDesigning a model is not about choosing equations. It is about committing to assumptions—each one a negotiation between competing objectives.\nWhen I decide to: - linearize a constraint → I’m choosing computational tractability over perfect realism, - split a global optimization into sequential subproblems → I’m accepting local optima to guarantee convergence, - introduce a penalty instead of a hard constraint → I’m prioritizing solution existence over strict compliance.\nEach decision carries consequences: - Linearization may ignore critical non-linear interactions. - Decomposition may miss globally optimal solutions. - Penalties require careful tuning—too weak and they’re ignored; too strong and they dominate the objective.\nI am not optimizing mathematics.\nI am negotiating trade-offs between realism, tractability, and stability.\nCurrent AI systems do not understand these trade-offs.\nThey imitate them. They’ve seen similar patterns in training data. But when faced with a novel operational context—a new constraint structure, an unusual cost function, a hybrid discrete-continuous problem—they regress to textbook templates that may be fundamentally unsuited to the problem at hand."
  },
  {
    "objectID": "blog/2026-ia-illusion-thinking.html#why-this-matters-in-practice",
    "href": "blog/2026-ia-illusion-thinking.html#why-this-matters-in-practice",
    "title": "Can AI Really Think About Model Design?",
    "section": "Why this matters in practice",
    "text": "Why this matters in practice\nIn real projects, poor model design does not fail loudly.\nIt fails silently, insidiously, expensively.\n\nSilent failure modes I’ve encountered:\n1. Infeasibility that appears only in production\nA model validates perfectly on historical data. Deploy it with real-time inputs, and suddenly: “No feasible solution found.” Why? Because the AI-generated formulation didn’t account for operational constraints that rarely appear in training scenarios—equipment downtime, simultaneous resource conflicts, regulatory edge cases.\n2. Numerical instability at scale\nA small-scale pilot works beautifully. Scale to production data volumes, and solver performance collapses. The culprit? Poor constraint formulation leading to ill-conditioned matrices, numerical precision issues, or combinatorial explosion that wasn’t visible at toy scale.\n3. KPIs improve in simulation but degrade operations\nThe model optimizes the wrong objective. It minimizes total distance traveled but ignores picking time variability. It maximizes throughput but creates bottlenecks downstream. It reduces inventory costs but increases stockout risk beyond acceptable thresholds.\nAI-generated designs often look correct until confronted with reality.\nThe model runs. It produces numbers. It generates convincing dashboards.\nBut the assumptions embedded in its structure don’t reflect the operational reality it was meant to serve.\nAnd here’s the insidious part: the business user won’t know. They see charts, metrics, recommendations. They don’t see the linearization that discarded a critical interaction, the penalty coefficient chosen arbitrarily, or the decomposition that locked out better solutions."
  },
  {
    "objectID": "blog/2026-ia-illusion-thinking.html#what-ai-can-do-well",
    "href": "blog/2026-ia-illusion-thinking.html#what-ai-can-do-well",
    "title": "Can AI Really Think About Model Design?",
    "section": "What AI can do well",
    "text": "What AI can do well\nLet’s be clear: I use AI extensively in my work. It’s not about AI versus humans.\nIt’s about knowing where AI excels and where it fails.\nAI is a powerful assistant for:\n\nExploration: Rapidly generating candidate formulations, testing alternative objective functions, exploring sensitivity to parameters.\nDocumentation: Translating mathematical notation into plain language, generating solver configuration documentation, explaining model structure to non-technical stakeholders.\nCode acceleration: Implementing standard algorithms (Simplex, Branch-and-Bound, gradient descent), generating boilerplate for data ingestion and transformation, scaffolding MLOps pipelines.\n\nBut when it comes to model architecture—the fundamental decisions about what to optimize, what to constrain, and how to balance competing objectives—that requires judgment born from experience, failure, and accountability."
  },
  {
    "objectID": "blog/2026-ia-illusion-thinking.html#the-accountability-gap",
    "href": "blog/2026-ia-illusion-thinking.html#the-accountability-gap",
    "title": "Can AI Really Think About Model Design?",
    "section": "The accountability gap",
    "text": "The accountability gap\nHere’s the core issue: AI cannot be held accountable.\nWhen a model I design fails in production, I own that failure.\nI debug it. I iterate. I explain to stakeholders what went wrong and how we’ll fix it.\nI carry the operational consequences of my assumptions.\nWhen an AI-generated model fails, who owns it?\nThe data scientist who deployed it without validating assumptions?\nThe AI system that “hallucinated” a plausible-looking formulation?\nResponsibility cannot be delegated to a statistical pattern matcher.\nModel design is fundamentally about risk management—understanding what can go wrong, anticipating edge cases, stress-testing assumptions against operational reality.\nAI can simulate competence.\nIt cannot simulate accountability."
  },
  {
    "objectID": "blog/2026-ia-illusion-thinking.html#conclusion-tools-not-replacements",
    "href": "blog/2026-ia-illusion-thinking.html#conclusion-tools-not-replacements",
    "title": "Can AI Really Think About Model Design?",
    "section": "Conclusion: Tools, not replacements",
    "text": "Conclusion: Tools, not replacements\nAI is a force multiplier for experienced practitioners.\nIt is a dangerous crutch for those who don’t yet understand the domain.\nI use AI to: - Accelerate implementation of well-understood patterns, - Explore formulation alternatives quickly, - Document and communicate model logic.\nI do not use AI to: - Make foundational design decisions, - Choose between competing modeling paradigms, - Validate whether a model is fit for operational deployment.\nModel design remains a human responsibility.\nNot because humans are faster.\nNot because humans are more creative.\nBut because we are accountable for assumptions.\nWe understand that every model is a deliberate simplification of reality.\nWe know which simplifications are acceptable and which are catastrophic.\nWe can explain our choices, defend our trade-offs, and adapt when reality proves us wrong.\n\nAI can write models.\nIt cannot yet take responsibility for them.\nAnd until it can, the final decision—the commitment to deploy, the ownership of outcomes—must remain human.\n\nWhat’s your experience? Have you deployed AI-generated models in production? What failed? What worked? I’d love to hear your stories—reach out on LinkedIn."
  },
  {
    "objectID": "blog/feasibility-over-optimality.html",
    "href": "blog/feasibility-over-optimality.html",
    "title": "Why Feasibility Matters More Than Optimality",
    "section": "",
    "text": "Most optimization courses emphasize optimality.\nMost industrial systems punish infeasibility.\nThis mismatch is not academic.\nIt is operational.\nI’ve watched meticulously optimized models collapse in production because they were too perfect—operating at the razor’s edge of feasibility with zero margin for error. Meanwhile, “suboptimal” solutions with built-in slack ran flawlessly for months.\nThe lesson was brutal and clear: in real-world operations, a feasible solution you can trust beats an optimal solution you can’t deploy.\nHere’s why feasibility isn’t just important—it’s the entire foundation of practical optimization."
  },
  {
    "objectID": "blog/feasibility-over-optimality.html#introduction-the-optimization-paradox",
    "href": "blog/feasibility-over-optimality.html#introduction-the-optimization-paradox",
    "title": "Why Feasibility Matters More Than Optimality",
    "section": "",
    "text": "Most optimization courses emphasize optimality.\nMost industrial systems punish infeasibility.\nThis mismatch is not academic.\nIt is operational.\nI’ve watched meticulously optimized models collapse in production because they were too perfect—operating at the razor’s edge of feasibility with zero margin for error. Meanwhile, “suboptimal” solutions with built-in slack ran flawlessly for months.\nThe lesson was brutal and clear: in real-world operations, a feasible solution you can trust beats an optimal solution you can’t deploy.\nHere’s why feasibility isn’t just important—it’s the entire foundation of practical optimization."
  },
  {
    "objectID": "blog/feasibility-over-optimality.html#feasible-beats-optimal-every-time",
    "href": "blog/feasibility-over-optimality.html#feasible-beats-optimal-every-time",
    "title": "Why Feasibility Matters More Than Optimality",
    "section": "Feasible beats optimal — every time",
    "text": "Feasible beats optimal — every time\nIn theory, an infeasible solution is useless.\nIn practice, an almost optimal but feasible solution beats a perfect one that violates a single constraint.\nConsider two warehouse slotting solutions:\nSolution A (Optimal): - Minimizes total picking distance by 18% - Achieves perfect load balancing across zones - Violates physical capacity in Zone 3 by 2% during peak hours - Result: Cannot be implemented. Overflow creates safety hazards and operational chaos.\nSolution B (Feasible, 95% optimal): - Minimizes total picking distance by 14% - Slight imbalance in zone utilization - Respects all capacity constraints with 10% buffer - Result: Deployed successfully. Runs for 6 months without intervention.\nWhich solution adds more value?\nThe answer is obvious—but optimization algorithms don’t care about deployment. They care about objective functions.\n\nOperations do not negotiate constraints\nWhen I tell a warehouse manager that my optimization model reduced costs by 15% but requires occasional capacity violations, the response is always the same:\n“I don’t care about your objective function. My constraint is physical reality. Shelves hold X pallets. Not X + 2%. Fix your model.”\nOperations enforce invariants:\n\nCapacity: You cannot store more than physical space allows.\nSafety: You cannot exceed load limits, violate clearances, or ignore fire codes.\nTiming: You cannot deliver before pickup, ship before receiving, or start before prerequisites complete.\nSequencing: You cannot clean equipment while it’s running, access blocked inventory, or bypass mandatory inspections.\n\nThese are not soft rules.\nThey are boundary conditions of reality.\nAnd reality is unforgiving."
  },
  {
    "objectID": "blog/feasibility-over-optimality.html#optimality-hides-fragility",
    "href": "blog/feasibility-over-optimality.html#optimality-hides-fragility",
    "title": "Why Feasibility Matters More Than Optimality",
    "section": "Optimality hides fragility",
    "text": "Optimality hides fragility\nHighly optimized solutions tend to be brittle.\nThey operate at the edge of feasibility, exploiting every available degree of freedom.\nThis is beautiful in theory.\nIt is catastrophic in practice.\n\nThe fragility of optimal solutions\nExample: Production scheduling\nAn optimal production schedule might: - Utilize machines at 98% capacity, - Minimize changeover times to seconds, - Sequence jobs with zero buffer between operations, - Assume perfect availability of materials, labor, and equipment.\nThen reality happens:\nA small perturbation: - Delayed shipment (raw materials arrive 2 hours late), - Missing data (quality inspection reveals batch defect), - Human override (operator emergency stop for safety), - Equipment variance (machine runs 5% slower than spec),\n…and the entire plan collapses.\nThe schedule cascades into: - Missed deadlines, - Idle downstream operations, - Expedited logistics to recover, - Manual rescheduling consuming hours of planner time.\nThe “optimal” solution optimized for a world that doesn’t exist.\n\n\nRobust systems trade optimality for slack\nIn my supply chain work, I’ve learned to deliberately suboptimize:\n\nSchedule machines at 85-90% capacity, not 98%—leaves room for variability.\nBuild time buffers between critical operations—absorbs delays without cascading.\nMaintain safety stock even when EOQ formulas say minimize inventory—guards against demand spikes.\nUse conservative lead times—accounts for supplier unreliability.\n\nThese choices reduce the objective function value.\nThey also reduce the probability of catastrophic failure.\nGood models make this trade-off explicit.\nInstead of:\n# Maximize throughput (fragile)\nobjective = maximize(sum(production[t] for t in time_periods))\nI design:\n# Maximize throughput subject to robustness\nobjective = maximize(\n    sum(production[t] for t in time_periods) \n    - penalty * sum(capacity_utilization_excess[m, t] for m, t in machines_time)\n)\n\n# With explicit slack constraints\ncapacity_utilization[m, t] &lt;= 0.90 * capacity[m]  # Hard 90% cap\nThis is not academic risk-aversion.\nThis is operational survival."
  },
  {
    "objectID": "blog/feasibility-over-optimality.html#feasibility-is-a-design-choice",
    "href": "blog/feasibility-over-optimality.html#feasibility-is-a-design-choice",
    "title": "Why Feasibility Matters More Than Optimality",
    "section": "Feasibility is a design choice",
    "text": "Feasibility is a design choice\nFeasibility does not come “for free”.\nIt is engineered.\nWhen I design an optimization model, ensuring feasibility requires deliberate choices:\n\n1. Conservative bounds\nInstead of using nominal capacity, I use effective capacity accounting for: - Maintenance downtime, - Quality rejection rates, - Operator efficiency variance, - Peak load conditions.\n# Naive approach (fails in production)\nconstraint: production[t] &lt;= nominal_capacity\n\n# Robust approach (survives reality)\nconstraint: production[t] &lt;= 0.85 * nominal_capacity  # 15% buffer for variability\n\n\n2. Explicit buffers\nI build slack into the model structure, not as an afterthought:\n\nTime buffers: between dependent operations,\nInventory buffers: safety stock, surge capacity,\nCapacity buffers: reserve equipment, overflow zones.\n\nThese reduce optimal objective value.\nThey also prevent infeasibility under perturbation.\n\n\n3. Carefully chosen big-M values\nBig-M formulations are notoriously brittle. Choose M too small, and you artificially constrain the problem. Choose M too large, and you create numerical instability.\nBad big-M choice:\n# Arbitrary large number\nM = 1e9\nconstraint: y[i] &lt;= M * x[i]  # If x[i]=0, forces y[i]=0\nIf actual y values can reach 1e8, this breaks. If they’re bounded by 100, M=1e9 causes solver numerical issues.\nGood big-M choice:\n# Derived from problem structure\nM = max(demand[i] for i in products) * max(lead_time[j] for j in suppliers)\nconstraint: y[i] &lt;= M * x[i]  # Tightest valid bound\nThis requires understanding the problem, not just reproducing a textbook template.\n\n\n4. Deliberate simplifications\nSometimes the “right” formulation is intractable.\nSometimes simplification is the only path to feasibility.\nExample: A globally optimal multi-echelon inventory model might require: - Stochastic dynamic programming, - Monte Carlo simulation for demand uncertainty, - Integer variables for order quantities, - Nonlinear holding cost functions.\nResult: Computationally intractable for realistic problem sizes.\nSimplification: - Use deterministic approximation with demand scenarios, - Linearize holding costs, - Decompose by echelon, - Solve sequentially with information passing.\nTrade-off: Lose global optimality guarantee.\nGain: Feasible solutions in minutes instead of never.\nThese choices are rarely elegant.\nThey are necessary."
  },
  {
    "objectID": "blog/feasibility-over-optimality.html#what-production-teaches-quickly",
    "href": "blog/feasibility-over-optimality.html#what-production-teaches-quickly",
    "title": "Why Feasibility Matters More Than Optimality",
    "section": "What production teaches quickly",
    "text": "What production teaches quickly\nProduction environments expose what simulations hide.\nIn academic settings, models are evaluated on: - Objective function value, - Solver runtime, - Optimality gap.\nIn production, models are evaluated on: - Deployment success rate (does it actually run?), - Manual intervention frequency (how often do operators override it?), - Failure modes (what breaks it, and how badly?), - Stakeholder trust (do people believe the recommendations?).\n\nIf a model:\n1. Fails silently\nProduces solutions that violate constraints not explicitly modeled (regulatory limits, implicit precedence, unstated business rules), the operations team discovers violations after deployment, not before.\nImpact: Loss of trust. Manual workarounds. Model abandonment.\n2. Requires constant manual fixes\nGenerates solutions that are “technically feasible” but operationally nonsensical: - Scheduling a shipment that arrives after the customer deadline, - Assigning incompatible products to the same storage zone, - Recommending production quantities that don’t align with packaging units.\nImpact: Planners spend more time fixing model outputs than they would planning manually.\n3. Produces solutions that need explanations\nIf stakeholders consistently ask “Why did it recommend this?” and the answer requires diving into dual variables, shadow prices, or constraint relaxations—the model has failed at its primary job: supporting decisions.\n\n\nFeasibility is what survives contact with reality\nI’ve learned to evaluate models not by how well they optimize, but by:\n\nHow long they run without manual intervention,\nHow gracefully they degrade under data quality issues,\nHow easily stakeholders understand and trust the outputs,\nHow few “emergency fixes” they require in production.\n\nOptimality is a nice-to-have.\nFeasibility is non-negotiable."
  },
  {
    "objectID": "blog/feasibility-over-optimality.html#the-hidden-cost-of-infeasibility",
    "href": "blog/feasibility-over-optimality.html#the-hidden-cost-of-infeasibility",
    "title": "Why Feasibility Matters More Than Optimality",
    "section": "The hidden cost of infeasibility",
    "text": "The hidden cost of infeasibility\nInfeasible solutions don’t just fail—they erode trust in analytics.\nWhen a meticulously crafted optimization model: - Recommends a production plan that violates capacity, - Suggests inventory levels that create stockouts, - Proposes shipments that miss delivery windows,\nStakeholders don’t blame the model.\nThey blame the entire analytics function.\nThe conversation shifts from “How can optimization help us?” to “Why should we trust your models?”\nRebuilding that trust takes months.\nDestroying it takes one bad deployment.\nThis is why I obsess over feasibility before I even think about optimality."
  },
  {
    "objectID": "blog/feasibility-over-optimality.html#final-thought-existence-before-excellence",
    "href": "blog/feasibility-over-optimality.html#final-thought-existence-before-excellence",
    "title": "Why Feasibility Matters More Than Optimality",
    "section": "Final thought: Existence before excellence",
    "text": "Final thought: Existence before excellence\nOptimization is not about finding the best solution.\nIt is about finding solutions that can exist.\nA solution that: - Respects all constraints, - Withstands operational variability, - Earns stakeholder trust, - Runs without constant intervention,\n…is infinitely more valuable than a theoretically optimal solution that cannot be deployed.\nEverything else is optional."
  },
  {
    "objectID": "blog/feasibility-over-optimality.html#practical-checklist-for-feasibility-first-design",
    "href": "blog/feasibility-over-optimality.html#practical-checklist-for-feasibility-first-design",
    "title": "Why Feasibility Matters More Than Optimality",
    "section": "Practical checklist for feasibility-first design",
    "text": "Practical checklist for feasibility-first design\nWhen I design an optimization model, I now ask these questions before I run the solver:\n✅ Have I validated every constraint with stakeholders?\n(Not just “Is this constraint correct?” but “Is this how reality actually works?”)\n✅ Have I built explicit buffers for variability?\n(Capacity slack, time buffers, safety stock, surge capacity)\n✅ Have I stress-tested the formulation on edge cases?\n(Peak demand, supplier failures, data quality issues, equipment downtime)\n✅ Can I explain every constraint in plain language?\n(If I can’t explain why a constraint exists, stakeholders won’t trust solutions that respect it)\n✅ Have I designed for graceful degradation?\n(What happens when a constraint is almost violated? Does the model recover or collapse?)\n✅ Is my big-M formulation numerically stable?\n(Are my big-M values tight bounds, or arbitrary large numbers that cause solver issues?)\nIf I can’t answer “yes” to all of these, I don’t deploy.\nBecause feasibility failures in production are far more expensive than suboptimal solutions.\n\nOptimal solutions impress in presentations.\nFeasible solutions survive in production.\nI choose survival.\n\nWhat’s been your experience with feasibility vs. optimality trade-offs? Have you deployed “optimal” models that failed? Or “suboptimal” models that thrived? Let’s compare war stories—reach out on LinkedIn."
  },
  {
    "objectID": "projects/forecasting.html",
    "href": "projects/forecasting.html",
    "title": "Forecasting",
    "section": "",
    "text": "My forecasting work focuses on robust decision-oriented forecasting, where interpretability, stability across seasons, and realistic assumptions matter more than marginal gains in accuracy.\nThe following project is based on a full end-to-end electricity demand forecasting study conducted on 10 years of hourly load data for a major U.S. utility. It demonstrates how operational constraints, uncertainty quantification, and model robustness should drive forecasting methodology—not just statistical performance metrics.\n\n\n\n\n\nJersey Central Power & Light (JCP&L) serves approximately 1.1 million customers over 3,200 square miles in New Jersey, representing roughly 12% of the state’s population. The electricity mix is dominated by natural gas and nuclear, with limited renewables, leading to strong weather sensitivity.\nThe operational objective was short-term load forecasting that remains reliable across: - Seasonal transitions (heating vs. cooling demand), - Extreme weather events (heatwaves, cold snaps, storms), - Structural changes (grid modernization, demand response programs, economic shifts).\nThis is not an academic exercise in minimizing MAPE on a test set. This is about building forecasts that operators trust, that survive edge cases, and that degrade gracefully when assumptions break.\n\n\n\n\nDataset characteristics: - 10 years of hourly load data (≈ 87,600 observations) - No missing values - Strong weekly and yearly seasonality - Clear correlation with weather variables (temperature, snow, precipitation) - Several major outage events, manually audited and imputed when they had a measurable impact on peak load\nExploratory findings:\nSeasonality patterns: - Winter peaks driven by heating demand (natural gas, electric heating) - Summer peaks driven by cooling demand (air conditioning saturation) - Shoulder seasons (Spring/Fall) with lower overall demand but higher volatility\nWeather sensitivity: - Strong nonlinear relationship between temperature and load - Temperature extremes (below 20°F, above 90°F) trigger rapid demand acceleration - Humidity and wind chill amplify weather effects\nStructural breaks: - Post-2008 financial crisis: demand plateau and gradual recovery - Energy efficiency programs: gradual baseload reduction - Demand response events: sharp, temporary load reductions during peak pricing\nOutlier treatment philosophy:\nSpecial attention was paid to outlier treatment—only incidents that clearly disrupted load patterns were adjusted, to avoid over-smoothing real operational stress signals.\nWhy this matters: - Over-cleaning data removes valuable information about system behavior under stress - Under-cleaning data trains models on anomalies that won’t generalize - The right approach: audit outliers manually, understand root causes, impute only when necessary\nFor example: - Major storm outage affecting 500k+ customers → Imputed (not representative of normal load) - Heatwave driving record peak demand → Retained (this is exactly what the model needs to learn)\nThis is the difference between statistical convenience and operational relevance.\n\n\n\n\nForecasting was framed as a regression problem with temporal structure—combining the interpretability of regression with the temporal dynamics of ARIMA.\nKey engineered features:\n1. Thermal comfort indices: - Heating Degree Days (HDD): Captures heating demand when temperature drops below 65°F - Cooling Degree Days (CDD): Captures cooling demand when temperature exceeds 65°F - Wind chill indicators: Adjusts perceived temperature for wind effects\n# Heating and Cooling Degree Days\ndf['HDD'] = np.maximum(65 - df['temperature'], 0)\ndf['CDD'] = np.maximum(df['temperature'] - 65, 0)\n\n# Wind chill adjustment\ndf['wind_chill'] = 35.74 + 0.6215*df['temperature'] - 35.75*(df['wind_speed']**0.16) + 0.4275*df['temperature']*(df['wind_speed']**0.16)\n2. Lagged weather variables: - Lag 1 and Lag 2 temperature: Accounts for thermal inertia in buildings - Lag 1 precipitation: Captures delayed behavioral responses (e.g., reduced commercial activity)\n# Lagged features\ndf['temp_lag1'] = df['temperature'].shift(1)\ndf['temp_lag2'] = df['temperature'].shift(2)\ndf['precip_lag1'] = df['precipitation'].shift(1)\n3. Calendar effects: - Weekday indicators: Mon-Fri vs. weekends have fundamentally different load profiles - Holiday flags: Major holidays (Thanksgiving, Christmas, July 4th) show distinct patterns - Hour of day: Captures intraday demand cycles\n# Calendar features\ndf['weekday'] = df['timestamp'].dt.dayofweek\ndf['is_weekend'] = df['weekday'].isin([5, 6]).astype(int)\ndf['is_holiday'] = df['timestamp'].isin(holiday_dates).astype(int)\ndf['hour'] = df['timestamp'].dt.hour\n4. Autoregressive structure via ARMA errors:\nInstead of pure regression, models were specified as regression with ARMA errors, capturing both: - Exogenous drivers (weather, calendar) via regression coefficients - Temporal autocorrelation (persistence, short-term trends) via ARMA structure\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# Regression with ARMA(2,1) errors\nmodel = ARIMA(\n    endog=load,\n    exog=weather_features,\n    order=(2, 0, 1)  # AR(2), no differencing, MA(1)\n)\nresults = model.fit()\n5. Realistic temperature forecast noise injection:\nTo reflect real-world uncertainty, temperature forecast noise was injected into the regression inputs, simulating realistic weather forecast errors.\nThis is critical because: - In production, you don’t have actual temperature—you have forecasted temperature - Weather forecasts have inherent uncertainty (±2-3°F typical error) - Models trained on perfect weather perform worse in production than models trained on noisy weather\n# Inject realistic temperature forecast noise\ntemperature_forecast_error_std = 2.5  # degrees F\ndf['temp_forecast'] = df['temperature'] + np.random.normal(0, temperature_forecast_error_std, len(df))\n\n# Train on forecasted temperature, not actual\nX = df[['temp_forecast', 'HDD', 'CDD', 'wind_chill', 'temp_lag1', 'is_weekend', 'hour']]\nThis is what separates academic forecasting from operational forecasting: training on the data you’ll actually have, not the data you wish you had.\n\n\n\n\nA wide range of models were compared under a rolling validation protocol—not a single train/test split, but repeated validation across multiple time windows to assess stability.\nModel families tested:\n1. Naïve baselines: - Naïve forecast: Tomorrow = Today - Seasonal naïve: Tomorrow = Same day last week\n2. Exponential smoothing: - SES (Simple Exponential Smoothing): Weighted average of past observations - Holt’s method: Adds linear trend - Holt-Winters: Adds seasonal components - ETS (Error-Trend-Seasonal): Automated selection of smoothing components - TBATS: Complex seasonality with Fourier terms\n3. ARIMA family: - SARIMA: Seasonal ARIMA with Box-Cox transformation - SARIMAX: SARIMA with exogenous weather variables\n4. Regression-based: - Linear regression with ARMA errors: Combines interpretability with temporal structure - ARX (Autoregressive Exogenous): Regression with autoregressive dynamics\n5. Machine learning (exploration, not production): - Random Forest: Ensemble of decision trees - Gradient Boosting (XGBoost): Sequential boosting - LSTM (RNN): Deep learning for sequential data\nEvaluation protocol:\nModels were evaluated using: - MAPE (Mean Absolute Percentage Error): Industry-standard accuracy metric - RMSE (Root Mean Squared Error): Penalizes large errors more heavily - MAE (Mean Absolute Error): Robust to outliers\nWith seasonal breakdowns (Winter, Spring, Summer, Fall) and statistical comparison via Diebold–Mariano tests to determine if performance differences were statistically significant.\nRolling validation: - Train on 8 years of data - Test on next 6 months - Roll forward 3 months - Repeat\nThis ensures models are evaluated on their ability to generalize forward in time, not just fit historical patterns.\n\n\n\n\nPerformance summary:\n✅ Linear regression with realistic temperature noise consistently outperformed classical time-series models across Winter, Spring, and Summer\n✅ Performance in Fall was comparable across several models (lower demand variability reduces model differentiation)\n❌ Classical models (SARIMA, Holt-Winters, ETS, TBATS) showed systematic degradation in volatile or irregular periods (cold snaps, heatwaves, post-holiday transitions)\n❌ Naïve baselines failed to capture seasonal dynamics and weather sensitivity\nQuantitative comparison (average across seasons):\n\n\n\nModel\nMAPE (%)\nRMSE (MW)\nMAE (MW)\n\n\n\n\nNaïve\n8.2\n420\n310\n\n\nSeasonal Naïve\n6.5\n380\n285\n\n\nHolt-Winters\n4.8\n295\n225\n\n\nSARIMA\n4.2\n270\n210\n\n\nETS\n4.0\n265\n205\n\n\nRegression + ARMA + Noise\n3.4\n235\n180\n\n\n\nWhy regression won:\n\nWeather integration: Explicit temperature, HDD/CDD features captured nonlinear weather effects better than implicit SARIMA seasonality\nRealistic training: Injecting forecast noise made the model robust to weather forecast errors\nInterpretability: Coefficients have clear operational meaning (e.g., “1°F increase → 50 MW load increase”)\nStability: Performance didn’t degrade sharply during extreme events\n\nFailure modes of classical models:\n\nSARIMA: Struggled with structural breaks (demand response events, economic shifts)\nHolt-Winters: Over-smoothed during rapid transitions (sudden cold snaps)\nTBATS: Computationally expensive, marginal improvement over simpler models\nLSTM: Required extensive hyperparameter tuning, opaque predictions, no better than regression\n\nThe regression-based approach achieved a strong balance between accuracy, robustness, and interpretability, making it suitable for operational deployment.\n\n\n\n\nWhat made this model production-ready:\n✅ Automated feature engineering pipeline: Reproducible, testable, version-controlled\ndef engineer_features(df_raw):\n    \"\"\"\n    Transform raw load and weather data into model-ready features.\n    \"\"\"\n    df = df_raw.copy()\n    \n    # Thermal indices\n    df['HDD'] = np.maximum(65 - df['temperature'], 0)\n    df['CDD'] = np.maximum(df['temperature'] - 65, 0)\n    \n    # Lags\n    df['temp_lag1'] = df['temperature'].shift(1)\n    df['temp_lag2'] = df['temperature'].shift(2)\n    \n    # Calendar\n    df['weekday'] = df['timestamp'].dt.dayofweek\n    df['is_weekend'] = df['weekday'].isin([5, 6]).astype(int)\n    df['hour'] = df['timestamp'].dt.hour\n    \n    # Inject realistic noise\n    df['temp_forecast'] = df['temperature'] + np.random.normal(0, 2.5, len(df))\n    \n    return df\n✅ Rolling retraining: Model refit every week with latest data to adapt to structural changes\n✅ Monitoring & alerting: Track forecast errors in real-time, flag when MAPE exceeds thresholds\n✅ Stakeholder communication: Forecast outputs include confidence intervals and scenario analysis (optimistic/pessimistic weather)\nWhat didn’t make it to production:\n❌ LSTM models: Too opaque, required GPUs, marginal performance gain didn’t justify complexity\n❌ Ensemble methods: Stacking multiple models increased maintenance burden without reliability improvement\n❌ Ultra-short-term forecasting (15-min ahead): Operational need was day-ahead; adding complexity for 15-min didn’t add value\n\n\n\n\nThis project reinforced a key principle:\n\nIn operational forecasting, robustness under uncertainty matters more than theoretical optimality under perfect information.\n\nRegression-based models with well-chosen predictors remain highly competitive when combined with: - Realistic assumptions (noisy weather forecasts, not perfect actuals), - Proper evaluation protocols (rolling validation, seasonal breakdowns, statistical testing), - Operational constraints (interpretability, computational feasibility, monitoring).\nBroader lessons:\n\nInject realism early: Training on perfect data creates brittle models\nValidate across seasons: A model that works in Summer might fail in Winter\nPrioritize interpretability: Operators need to understand why a forecast changed\nBeware complexity for complexity’s sake: LSTM didn’t beat regression because the problem structure favored explicit feature engineering\n\nThis is the difference between forecasting as a statistical exercise and forecasting as an operational capability.\n\nTech Stack:\nPython · Statsmodels · Scikit-learn · Pandas · NumPy · Matplotlib · Seaborn\nRepository:\nGitHub – Electricity Demand Forecasting Realesed soon\nRelated Work:\n- Why Feasibility Matters More Than Optimality - Can AI Really Think About Model Design?\n\nForecasting is not about predicting the future.\nIt’s about making decisions today that remain robust tomorrow."
  },
  {
    "objectID": "projects/forecasting.html#electricity-demand-forecasting-jc-power-light-nj",
    "href": "projects/forecasting.html#electricity-demand-forecasting-jc-power-light-nj",
    "title": "Forecasting",
    "section": "",
    "text": "Jersey Central Power & Light (JCP&L) serves approximately 1.1 million customers over 3,200 square miles in New Jersey, representing roughly 12% of the state’s population. The electricity mix is dominated by natural gas and nuclear, with limited renewables, leading to strong weather sensitivity.\nThe operational objective was short-term load forecasting that remains reliable across: - Seasonal transitions (heating vs. cooling demand), - Extreme weather events (heatwaves, cold snaps, storms), - Structural changes (grid modernization, demand response programs, economic shifts).\nThis is not an academic exercise in minimizing MAPE on a test set. This is about building forecasts that operators trust, that survive edge cases, and that degrade gracefully when assumptions break.\n\n\n\n\nDataset characteristics: - 10 years of hourly load data (≈ 87,600 observations) - No missing values - Strong weekly and yearly seasonality - Clear correlation with weather variables (temperature, snow, precipitation) - Several major outage events, manually audited and imputed when they had a measurable impact on peak load\nExploratory findings:\nSeasonality patterns: - Winter peaks driven by heating demand (natural gas, electric heating) - Summer peaks driven by cooling demand (air conditioning saturation) - Shoulder seasons (Spring/Fall) with lower overall demand but higher volatility\nWeather sensitivity: - Strong nonlinear relationship between temperature and load - Temperature extremes (below 20°F, above 90°F) trigger rapid demand acceleration - Humidity and wind chill amplify weather effects\nStructural breaks: - Post-2008 financial crisis: demand plateau and gradual recovery - Energy efficiency programs: gradual baseload reduction - Demand response events: sharp, temporary load reductions during peak pricing\nOutlier treatment philosophy:\nSpecial attention was paid to outlier treatment—only incidents that clearly disrupted load patterns were adjusted, to avoid over-smoothing real operational stress signals.\nWhy this matters: - Over-cleaning data removes valuable information about system behavior under stress - Under-cleaning data trains models on anomalies that won’t generalize - The right approach: audit outliers manually, understand root causes, impute only when necessary\nFor example: - Major storm outage affecting 500k+ customers → Imputed (not representative of normal load) - Heatwave driving record peak demand → Retained (this is exactly what the model needs to learn)\nThis is the difference between statistical convenience and operational relevance.\n\n\n\n\nForecasting was framed as a regression problem with temporal structure—combining the interpretability of regression with the temporal dynamics of ARIMA.\nKey engineered features:\n1. Thermal comfort indices: - Heating Degree Days (HDD): Captures heating demand when temperature drops below 65°F - Cooling Degree Days (CDD): Captures cooling demand when temperature exceeds 65°F - Wind chill indicators: Adjusts perceived temperature for wind effects\n# Heating and Cooling Degree Days\ndf['HDD'] = np.maximum(65 - df['temperature'], 0)\ndf['CDD'] = np.maximum(df['temperature'] - 65, 0)\n\n# Wind chill adjustment\ndf['wind_chill'] = 35.74 + 0.6215*df['temperature'] - 35.75*(df['wind_speed']**0.16) + 0.4275*df['temperature']*(df['wind_speed']**0.16)\n2. Lagged weather variables: - Lag 1 and Lag 2 temperature: Accounts for thermal inertia in buildings - Lag 1 precipitation: Captures delayed behavioral responses (e.g., reduced commercial activity)\n# Lagged features\ndf['temp_lag1'] = df['temperature'].shift(1)\ndf['temp_lag2'] = df['temperature'].shift(2)\ndf['precip_lag1'] = df['precipitation'].shift(1)\n3. Calendar effects: - Weekday indicators: Mon-Fri vs. weekends have fundamentally different load profiles - Holiday flags: Major holidays (Thanksgiving, Christmas, July 4th) show distinct patterns - Hour of day: Captures intraday demand cycles\n# Calendar features\ndf['weekday'] = df['timestamp'].dt.dayofweek\ndf['is_weekend'] = df['weekday'].isin([5, 6]).astype(int)\ndf['is_holiday'] = df['timestamp'].isin(holiday_dates).astype(int)\ndf['hour'] = df['timestamp'].dt.hour\n4. Autoregressive structure via ARMA errors:\nInstead of pure regression, models were specified as regression with ARMA errors, capturing both: - Exogenous drivers (weather, calendar) via regression coefficients - Temporal autocorrelation (persistence, short-term trends) via ARMA structure\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# Regression with ARMA(2,1) errors\nmodel = ARIMA(\n    endog=load,\n    exog=weather_features,\n    order=(2, 0, 1)  # AR(2), no differencing, MA(1)\n)\nresults = model.fit()\n5. Realistic temperature forecast noise injection:\nTo reflect real-world uncertainty, temperature forecast noise was injected into the regression inputs, simulating realistic weather forecast errors.\nThis is critical because: - In production, you don’t have actual temperature—you have forecasted temperature - Weather forecasts have inherent uncertainty (±2-3°F typical error) - Models trained on perfect weather perform worse in production than models trained on noisy weather\n# Inject realistic temperature forecast noise\ntemperature_forecast_error_std = 2.5  # degrees F\ndf['temp_forecast'] = df['temperature'] + np.random.normal(0, temperature_forecast_error_std, len(df))\n\n# Train on forecasted temperature, not actual\nX = df[['temp_forecast', 'HDD', 'CDD', 'wind_chill', 'temp_lag1', 'is_weekend', 'hour']]\nThis is what separates academic forecasting from operational forecasting: training on the data you’ll actually have, not the data you wish you had.\n\n\n\n\nA wide range of models were compared under a rolling validation protocol—not a single train/test split, but repeated validation across multiple time windows to assess stability.\nModel families tested:\n1. Naïve baselines: - Naïve forecast: Tomorrow = Today - Seasonal naïve: Tomorrow = Same day last week\n2. Exponential smoothing: - SES (Simple Exponential Smoothing): Weighted average of past observations - Holt’s method: Adds linear trend - Holt-Winters: Adds seasonal components - ETS (Error-Trend-Seasonal): Automated selection of smoothing components - TBATS: Complex seasonality with Fourier terms\n3. ARIMA family: - SARIMA: Seasonal ARIMA with Box-Cox transformation - SARIMAX: SARIMA with exogenous weather variables\n4. Regression-based: - Linear regression with ARMA errors: Combines interpretability with temporal structure - ARX (Autoregressive Exogenous): Regression with autoregressive dynamics\n5. Machine learning (exploration, not production): - Random Forest: Ensemble of decision trees - Gradient Boosting (XGBoost): Sequential boosting - LSTM (RNN): Deep learning for sequential data\nEvaluation protocol:\nModels were evaluated using: - MAPE (Mean Absolute Percentage Error): Industry-standard accuracy metric - RMSE (Root Mean Squared Error): Penalizes large errors more heavily - MAE (Mean Absolute Error): Robust to outliers\nWith seasonal breakdowns (Winter, Spring, Summer, Fall) and statistical comparison via Diebold–Mariano tests to determine if performance differences were statistically significant.\nRolling validation: - Train on 8 years of data - Test on next 6 months - Roll forward 3 months - Repeat\nThis ensures models are evaluated on their ability to generalize forward in time, not just fit historical patterns.\n\n\n\n\nPerformance summary:\n✅ Linear regression with realistic temperature noise consistently outperformed classical time-series models across Winter, Spring, and Summer\n✅ Performance in Fall was comparable across several models (lower demand variability reduces model differentiation)\n❌ Classical models (SARIMA, Holt-Winters, ETS, TBATS) showed systematic degradation in volatile or irregular periods (cold snaps, heatwaves, post-holiday transitions)\n❌ Naïve baselines failed to capture seasonal dynamics and weather sensitivity\nQuantitative comparison (average across seasons):\n\n\n\nModel\nMAPE (%)\nRMSE (MW)\nMAE (MW)\n\n\n\n\nNaïve\n8.2\n420\n310\n\n\nSeasonal Naïve\n6.5\n380\n285\n\n\nHolt-Winters\n4.8\n295\n225\n\n\nSARIMA\n4.2\n270\n210\n\n\nETS\n4.0\n265\n205\n\n\nRegression + ARMA + Noise\n3.4\n235\n180\n\n\n\nWhy regression won:\n\nWeather integration: Explicit temperature, HDD/CDD features captured nonlinear weather effects better than implicit SARIMA seasonality\nRealistic training: Injecting forecast noise made the model robust to weather forecast errors\nInterpretability: Coefficients have clear operational meaning (e.g., “1°F increase → 50 MW load increase”)\nStability: Performance didn’t degrade sharply during extreme events\n\nFailure modes of classical models:\n\nSARIMA: Struggled with structural breaks (demand response events, economic shifts)\nHolt-Winters: Over-smoothed during rapid transitions (sudden cold snaps)\nTBATS: Computationally expensive, marginal improvement over simpler models\nLSTM: Required extensive hyperparameter tuning, opaque predictions, no better than regression\n\nThe regression-based approach achieved a strong balance between accuracy, robustness, and interpretability, making it suitable for operational deployment.\n\n\n\n\nWhat made this model production-ready:\n✅ Automated feature engineering pipeline: Reproducible, testable, version-controlled\ndef engineer_features(df_raw):\n    \"\"\"\n    Transform raw load and weather data into model-ready features.\n    \"\"\"\n    df = df_raw.copy()\n    \n    # Thermal indices\n    df['HDD'] = np.maximum(65 - df['temperature'], 0)\n    df['CDD'] = np.maximum(df['temperature'] - 65, 0)\n    \n    # Lags\n    df['temp_lag1'] = df['temperature'].shift(1)\n    df['temp_lag2'] = df['temperature'].shift(2)\n    \n    # Calendar\n    df['weekday'] = df['timestamp'].dt.dayofweek\n    df['is_weekend'] = df['weekday'].isin([5, 6]).astype(int)\n    df['hour'] = df['timestamp'].dt.hour\n    \n    # Inject realistic noise\n    df['temp_forecast'] = df['temperature'] + np.random.normal(0, 2.5, len(df))\n    \n    return df\n✅ Rolling retraining: Model refit every week with latest data to adapt to structural changes\n✅ Monitoring & alerting: Track forecast errors in real-time, flag when MAPE exceeds thresholds\n✅ Stakeholder communication: Forecast outputs include confidence intervals and scenario analysis (optimistic/pessimistic weather)\nWhat didn’t make it to production:\n❌ LSTM models: Too opaque, required GPUs, marginal performance gain didn’t justify complexity\n❌ Ensemble methods: Stacking multiple models increased maintenance burden without reliability improvement\n❌ Ultra-short-term forecasting (15-min ahead): Operational need was day-ahead; adding complexity for 15-min didn’t add value\n\n\n\n\nThis project reinforced a key principle:\n\nIn operational forecasting, robustness under uncertainty matters more than theoretical optimality under perfect information.\n\nRegression-based models with well-chosen predictors remain highly competitive when combined with: - Realistic assumptions (noisy weather forecasts, not perfect actuals), - Proper evaluation protocols (rolling validation, seasonal breakdowns, statistical testing), - Operational constraints (interpretability, computational feasibility, monitoring).\nBroader lessons:\n\nInject realism early: Training on perfect data creates brittle models\nValidate across seasons: A model that works in Summer might fail in Winter\nPrioritize interpretability: Operators need to understand why a forecast changed\nBeware complexity for complexity’s sake: LSTM didn’t beat regression because the problem structure favored explicit feature engineering\n\nThis is the difference between forecasting as a statistical exercise and forecasting as an operational capability.\n\nTech Stack:\nPython · Statsmodels · Scikit-learn · Pandas · NumPy · Matplotlib · Seaborn\nRepository:\nGitHub – Electricity Demand Forecasting Realesed soon\nRelated Work:\n- Why Feasibility Matters More Than Optimality - Can AI Really Think About Model Design?\n\nForecasting is not about predicting the future.\nIt’s about making decisions today that remain robust tomorrow."
  },
  {
    "objectID": "projects/rl.html",
    "href": "projects/rl.html",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "My work in reinforcement learning is guided by a simple idea:\nlearning to act is fundamentally different from learning to predict.\nWhere supervised models excel at recognizing patterns, reinforcement learning confronts a much harder problem: deciding what to do next when actions have long-term consequences, delayed rewards, and irreversible mistakes. This difference becomes especially visible in environments governed by combinatorial structure and physical constraints.\nThe project presented here explores these limits through a concrete, tangible system:\nan autonomous robot solving a routing problem in the physical world.\n\n\n\n\n\nAt its core, this project is a practical instantiation of the Traveling Salesman Problem (TSP), one of the most studied problems in combinatorial optimization. The objective is deceptively simple: visit a set of locations exactly once, minimize total travel cost, and return to the starting point.\nYet the underlying complexity grows factorially. Even modest problem sizes lead to millions of possible routes, making exhaustive search infeasible in practice. In classical optimization, this motivates mathematical programming, heuristics, or approximation schemes. Here, the challenge was approached from a different angle: can an agent learn such a strategy purely through interaction?\nThis question is not theoretical. It directly reflects how autonomous systems must operate when global information is unavailable or when explicit modeling becomes impractical.\n\n\n\n\nThe environment is implemented on Codey Rocky, a mobile robot capable of autonomous movement and basic sensing. The robot starts from a fixed base, navigates the environment, visits a set of colored targets exactly once, and finally returns to its starting position.\nNo global map is provided. The robot does not plan a full route in advance. Instead, it must make decisions sequentially, relying only on its current perception and internal state. Each action modifies the future space of possibilities.\nBy framing the task this way, a static optimization problem is transformed into a sequential decision-making process, where local actions accumulate into global behavior.\n\n\n\n\nA critical design choice lies in how the environment is represented. Rather than encoding raw sensor data, the environment is abstracted into a finite state space of 128 states. Each state captures the essential information required for decision-making: the robot’s position and orientation, the set of already visited targets, and the remaining objectives.\nThis abstraction is intentionally compact. It preserves the structure of the problem while remaining small enough for learning algorithms to converge. The exercise makes a broader point: in reinforcement learning, state design often matters more than algorithmic sophistication.\nA poorly chosen state space leads to slow learning or unstable behavior, regardless of how advanced the learning algorithm may be.\n\n\n\n\nThe robot’s action space is deliberately constrained. At each step, it can move forward or rotate by fixed angles. These actions reflect physical feasibility rather than mathematical convenience.\nThis restriction plays a dual role. It simplifies the learning problem while simultaneously enforcing realism. The agent cannot “teleport” or perform abstract transitions; every action has a physical cost and a spatial consequence.\n\n\n\n\nIf state design determines learnability, reward design determines behavior.\nThe reward function was carefully constructed to align short-term incentives with long-term goals. Positive rewards are granted for discovering new targets and for completing the full tour. Penalties discourage revisiting already collected items, wandering unnecessarily, or taking excessively long paths.\nThis balance is delicate. Small changes in reward magnitude can radically alter learned behavior, leading to oscillations, local loops, or premature termination. Designing rewards thus becomes an exercise in systems thinking rather than pure mathematics.\nThe process revealed a central weakness of reinforcement learning: desired behavior is never guaranteed, only encouraged.\n\n\n\n\nTwo learning paradigms were explored.\nThe first is tabular Q-learning, where each state–action pair is explicitly stored and updated using the Bellman equation. This approach offers full transparency. Every decision can be traced back to learned values. However, its scalability is fundamentally limited by the size of the state space.\nThe second approach replaces the explicit table with a neural network approximation: Deep Q-Learning (DQN). The network estimates action values directly from state representations, trading interpretability for generalization.\nDespite their conceptual differences, both methods were trained under comparable conditions and evaluated on identical tasks.\n\n\n\n\nDuring training, both algorithms exhibit a similar learning trajectory. Early episodes show rapid improvement as the agent discovers basic strategies. Over time, performance stabilizes and oscillates around a plateau.\nInterestingly, the deep model does not outperform the tabular one. Given the modest state space and deterministic environment, this result is expected. It illustrates a key insight often overlooked in practice: deep learning is not automatically superior. When the problem structure is small and well defined, simpler methods can be just as effective.\n\n\n\n\nBeyond performance metrics, the project exposes deeper limitations of reinforcement learning.\nThe agent explores many trajectories that a classical optimizer would discard immediately. Learning is inherently inefficient in combinatorial spaces. Furthermore, there are no feasibility guarantees: a learned policy may work most of the time, yet fail catastrophically in edge cases.\nPerhaps most importantly, convergence does not imply correctness. A stable policy is not necessarily a good one; it is merely a consistent one under the observed experience.\nThese are not implementation flaws. They are structural properties of reinforcement learning itself.\n\n\n\n\nThis project leads to a broader conclusion:\n\nReinforcement learning is well suited for adaptive control,\nbut poorly suited for structured combinatorial optimization.\n\nFor routing, scheduling, and allocation problems: - explicit optimization provides guarantees and interpretability, - reinforcement learning provides adaptability but little assurance.\nIn practice, the most effective systems often combine both: optimization for structure, learning for adaptation.\n\n\n\n\nThis experiment serves as a concrete reminder that intelligence is not only about learning patterns, but about respecting constraints and understanding consequences.\nBy placing reinforcement learning in a physical, constrained environment, the project highlights both its strengths and its limits. It reinforces the idea that decision systems must be evaluated not by elegance, but by reliability.\n\nTech Stack\nPython · Q-learning · Deep Q-Learning · Neural Networks · Robotics\nRepository\nGitHub – Codey Rocky RL Navigation Released soon\n\nReinforcement learning teaches agents to adapt.\nOptimization teaches systems to remain correct. Both are needed to build intelligent, reliable systems."
  },
  {
    "objectID": "projects/rl.html#reinforcement-learning-for-combinatorial-navigation-codey-rocky-robot",
    "href": "projects/rl.html#reinforcement-learning-for-combinatorial-navigation-codey-rocky-robot",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "At its core, this project is a practical instantiation of the Traveling Salesman Problem (TSP), one of the most studied problems in combinatorial optimization. The objective is deceptively simple: visit a set of locations exactly once, minimize total travel cost, and return to the starting point.\nYet the underlying complexity grows factorially. Even modest problem sizes lead to millions of possible routes, making exhaustive search infeasible in practice. In classical optimization, this motivates mathematical programming, heuristics, or approximation schemes. Here, the challenge was approached from a different angle: can an agent learn such a strategy purely through interaction?\nThis question is not theoretical. It directly reflects how autonomous systems must operate when global information is unavailable or when explicit modeling becomes impractical.\n\n\n\n\nThe environment is implemented on Codey Rocky, a mobile robot capable of autonomous movement and basic sensing. The robot starts from a fixed base, navigates the environment, visits a set of colored targets exactly once, and finally returns to its starting position.\nNo global map is provided. The robot does not plan a full route in advance. Instead, it must make decisions sequentially, relying only on its current perception and internal state. Each action modifies the future space of possibilities.\nBy framing the task this way, a static optimization problem is transformed into a sequential decision-making process, where local actions accumulate into global behavior.\n\n\n\n\nA critical design choice lies in how the environment is represented. Rather than encoding raw sensor data, the environment is abstracted into a finite state space of 128 states. Each state captures the essential information required for decision-making: the robot’s position and orientation, the set of already visited targets, and the remaining objectives.\nThis abstraction is intentionally compact. It preserves the structure of the problem while remaining small enough for learning algorithms to converge. The exercise makes a broader point: in reinforcement learning, state design often matters more than algorithmic sophistication.\nA poorly chosen state space leads to slow learning or unstable behavior, regardless of how advanced the learning algorithm may be.\n\n\n\n\nThe robot’s action space is deliberately constrained. At each step, it can move forward or rotate by fixed angles. These actions reflect physical feasibility rather than mathematical convenience.\nThis restriction plays a dual role. It simplifies the learning problem while simultaneously enforcing realism. The agent cannot “teleport” or perform abstract transitions; every action has a physical cost and a spatial consequence.\n\n\n\n\nIf state design determines learnability, reward design determines behavior.\nThe reward function was carefully constructed to align short-term incentives with long-term goals. Positive rewards are granted for discovering new targets and for completing the full tour. Penalties discourage revisiting already collected items, wandering unnecessarily, or taking excessively long paths.\nThis balance is delicate. Small changes in reward magnitude can radically alter learned behavior, leading to oscillations, local loops, or premature termination. Designing rewards thus becomes an exercise in systems thinking rather than pure mathematics.\nThe process revealed a central weakness of reinforcement learning: desired behavior is never guaranteed, only encouraged.\n\n\n\n\nTwo learning paradigms were explored.\nThe first is tabular Q-learning, where each state–action pair is explicitly stored and updated using the Bellman equation. This approach offers full transparency. Every decision can be traced back to learned values. However, its scalability is fundamentally limited by the size of the state space.\nThe second approach replaces the explicit table with a neural network approximation: Deep Q-Learning (DQN). The network estimates action values directly from state representations, trading interpretability for generalization.\nDespite their conceptual differences, both methods were trained under comparable conditions and evaluated on identical tasks.\n\n\n\n\nDuring training, both algorithms exhibit a similar learning trajectory. Early episodes show rapid improvement as the agent discovers basic strategies. Over time, performance stabilizes and oscillates around a plateau.\nInterestingly, the deep model does not outperform the tabular one. Given the modest state space and deterministic environment, this result is expected. It illustrates a key insight often overlooked in practice: deep learning is not automatically superior. When the problem structure is small and well defined, simpler methods can be just as effective.\n\n\n\n\nBeyond performance metrics, the project exposes deeper limitations of reinforcement learning.\nThe agent explores many trajectories that a classical optimizer would discard immediately. Learning is inherently inefficient in combinatorial spaces. Furthermore, there are no feasibility guarantees: a learned policy may work most of the time, yet fail catastrophically in edge cases.\nPerhaps most importantly, convergence does not imply correctness. A stable policy is not necessarily a good one; it is merely a consistent one under the observed experience.\nThese are not implementation flaws. They are structural properties of reinforcement learning itself.\n\n\n\n\nThis project leads to a broader conclusion:\n\nReinforcement learning is well suited for adaptive control,\nbut poorly suited for structured combinatorial optimization.\n\nFor routing, scheduling, and allocation problems: - explicit optimization provides guarantees and interpretability, - reinforcement learning provides adaptability but little assurance.\nIn practice, the most effective systems often combine both: optimization for structure, learning for adaptation.\n\n\n\n\nThis experiment serves as a concrete reminder that intelligence is not only about learning patterns, but about respecting constraints and understanding consequences.\nBy placing reinforcement learning in a physical, constrained environment, the project highlights both its strengths and its limits. It reinforces the idea that decision systems must be evaluated not by elegance, but by reliability.\n\nTech Stack\nPython · Q-learning · Deep Q-Learning · Neural Networks · Robotics\nRepository\nGitHub – Codey Rocky RL Navigation Released soon\n\nReinforcement learning teaches agents to adapt.\nOptimization teaches systems to remain correct. Both are needed to build intelligent, reliable systems."
  }
]
---
title: "Warehouse Optimization (MILP)"
toc: true
---

# Warehouse Optimization (MILP)

This project presents a **complete, reproducible warehouse optimization framework** designed for **operational realism**, not just mathematical elegance.

It combines **digital twin modeling**, **synthetic data generation**, **sequential MILP optimization**, and a **persistent KPI engine** to evaluate both performance and stability across multiple operational stages.

This is not a toy problem solved once on synthetic data. This is a **production-grade framework** built to handle the messy reality of warehouse operations: capacity constraints, format compatibility, pack sizes, daily arrivals, customer orders, and the constant tension between layout optimization and operational stability.

---

## Context: WA2 Warehouse Operations

### The Challenge

The objective was to optimize the operations of a real warehouse (WA2) by **jointly addressing three interconnected decision problems**:

1. **Product placement (Slotting / Putaway):** Where should incoming inventory be stored?
2. **Order picking:** Which locations should we pick from to fulfill customer orders?
3. **Internal replenishment:** Should we relocate inventory to improve layout quality, and if so, how?

These decisions are deeply coupled:
- **Slotting decisions** today determine picking efficiency tomorrow
- **Picking decisions** create inventory imbalances that trigger replenishment needs
- **Replenishment decisions** improve layout quality but create operational churn

**The key challenge was balancing feasibility, stability, and performance** in a setting with:
- **Limited capacities** (physical shelf space, format restrictions),
- **Multiple product formats** (600ml, 304ml, 400ml, Others) with different pack sizes,
- **Daily operational constraints** (arrivals, orders, capacity limits),
- **Conflicting objectives** (minimize travel distance vs. minimize layout churn vs. maximize space utilization).

### Why This Problem is Hard

Most warehouse optimization research focuses on **single-stage problems** (pure slotting, pure picking, pure replenishment) under simplifying assumptions (infinite capacity, single format, deterministic demand).

**Reality is messier:**

- You can't optimize slotting without considering how it affects picking
- You can't optimize picking without respecting inventory constraints from slotting
- You can't optimize replenishment without accounting for future arrivals and orders
- You can't do any of this if your model produces **infeasible solutions** that violate physical capacity, format compatibility, or pack size constraints

**This framework solves the complete problem** using **sequential MILP** with **state propagation** across stages and days.

---

## Digital Twin & Environment Modeling

A structured warehouse representation was built to ensure optimization decisions reflect **realistic travel distances and topology**.

### Warehouse Structure

**2D Layout:**
- **Blocks:** Racks, aisles, storage positions
- **Paths:** Corridors, intersections, navigation routes
- **Special zones:** Central **Depot** (picking staging area), **recv** (receiving zone for inbound inventory)

**3D Extension:**
- Multiple **levels** (vertical racking)
- Vertical spacing and connectivity constraints
- Realistic accessibility modeling (can't teleport between levels)

### Graph-Based Distance Computation

Two graphs were constructed:

**1. G2D (Planar movement graph):**
- Nodes: All storage locations, depot, receiving
- Edges: Horizontal movement paths with associated costs
- Distance metric: **Shortest path via Dijkstra's algorithm**

**2. G3D (Multi-level graph):**
- Nodes: Storage locations across all vertical levels
- Edges: Horizontal + vertical movement (stairs, lifts)
- Connectivity: Enforces realistic vertical access constraints

**Why this matters:**

Generic warehouse optimization often uses **Euclidean distance** or **Manhattan distance** as a proxy for travel cost.

**Reality:** Warehouse movement follows **corridors, aisles, and specific paths**. A location 10 meters away in Euclidean distance might require 30 meters of actual travel if it's across a blocked aisle.

**Graph-based shortest path** captures this accurately.
```python
import networkx as nx
from scipy.sparse.csgraph import dijkstra

# Build graph from warehouse topology
G2D = nx.Graph()
for corridor in corridors:
    G2D.add_edge(corridor.start, corridor.end, weight=corridor.length)

# Compute all-pairs shortest path distances
distance_matrix = dict(nx.all_pairs_dijkstra_path_length(G2D))

# Use in optimization model
cost_recv_to_loc = distance_matrix['recv'][location_id]
```

This digital twin ensures that optimization decisions reflect **realistic travel distances and topology**, not idealized geometric approximations.

---

## Data Generation & Quality Control

A dedicated `MILPDataFactory` module generates **fully feasible synthetic data** for model validation and scenario testing.

### Generated Data Elements

**Time horizon:**
- **T = 5 days** (rolling operational planning window)

**Product catalog:**
- **|S| = 20 SKUs** with heterogeneous demand patterns
- **Formats F = {600ml, 304ml, 400ml, Others}** (packaging types)
- **Pack sizes:** {12, 25, 1000} units per pack (SKU-specific)
- **Format compatibility:** Each SKU belongs to exactly one format

**Warehouse capacity:**
- **Location capacities Cap(format, loc):** Format-specific storage limits
- **One format per location constraint:** Once a location is assigned a format, only SKUs of that format can be stored there

**Operational data:**
- **External arrivals A(t, recv, s):** Inbound shipments (multiples of pack sizes) arriving at receiving zone
- **Initial inventory I₀:** Starting stock levels across locations (validated for feasibility)
- **Customer orders:** 10-30 orders per day, each with multiple SKU line items
- **Cumulative demand:** Aggregated demand across orders for planning

**Internal cost structure:**
- **Distance matrix C(loc_i, loc_j):** Shortest path costs for internal movements
- **Distance to depot:** Proxy for picking efficiency (closer = faster picking)

### Data Quality: Automated Audit

An automated audit validates data feasibility **before optimization** to avoid solving infeasible models:

**Audit checks:**

**1. Multi-format violation:**
```python
# Check: Each location should have at most one format with positive inventory
for loc in storage_locations:
    formats_present = [f for f in formats if inventory[loc, f] > 0]
    if len(formats_present) > 1:
        violations.append(f"Location {loc} has multiple formats: {formats_present}")
```

**2. Missing capacities:**
```python
# Check: All storage locations must have defined capacities
for loc in storage_locations:
    if loc not in capacity_table:
        violations.append(f"Location {loc} missing capacity definition")
```

**3. Capacity overflows:**
```python
# Check: Initial inventory must not exceed physical capacity
for loc in storage_locations:
    total_inventory = sum(inventory[loc, s] for s in SKUs)
    max_capacity = sum(capacity[loc, f] * format_active[loc, f] for f in formats)
    if total_inventory > max_capacity:
        violations.append(f"Location {loc} inventory {total_inventory} exceeds capacity {max_capacity}")
```

**Audit outputs:**
- `multi_format_locations`: Locations violating one-format rule
- `locations_missing_capacity`: Locations without capacity definitions
- `locations_over_max_capacity`: Locations with inventory exceeding physical limits

**Why this matters:**

Optimization solvers can spend hours exploring infeasible regions if the input data itself is inconsistent. **Validating feasibility upfront** ensures:
- Models solve faster (solver doesn't waste time proving infeasibility)
- Results are operationally meaningful (not artifacts of bad data)
- Debugging is easier (if model is infeasible, it's a modeling issue, not a data issue)

---

## Optimization Models: Sequential MILP

The warehouse system is solved **day by day** using three interconnected MILP models that execute sequentially:

### Daily Workflow

For each day `t`:

**BEFORE → DSLAP → PICK → REPLEN → AFTER**

1. **BEFORE:** Compute KPIs on current warehouse state
2. **DSLAP (Slotting/Putaway):** Decide where to place incoming inventory
3. **PICK (Order Picking):** Decide which locations to pick from to fulfill orders
4. **REPLEN (Internal Replenishment):** Decide whether to relocate inventory for layout improvement
5. **AFTER:** Compute KPIs on updated warehouse state

**State propagation:** Inventory at end of one stage becomes initial inventory for next stage. Inventory at end of day `t` becomes initial inventory for day `t+1`.

This reflects **operational reality**: you can't optimize slotting without knowing the inventory state after yesterday's picking and replenishment.

---

### Model 1: DSLAP (Dynamic Slotting, Layout, and Putaway)

**Objective:** Determine which format is activated on each storage location and how inbound inventory from receiving zone is allocated to storage locations.

#### Decision Variables (day t)

- **E_{t,l,f} ∈ {0,1}:** Binary variable indicating if format `f` is active on location `l` at day `t`
- **U_{t,l} ∈ {0,1}:** Binary variable indicating if location `l` is in use (has any format active)
- **Q_{t,s,l} ≥ 0:** Continuous quantity of SKU `s` moved from receiving to location `l`
- **MQ_{t,s,l} ∈ ℤ₊:** Integer number of packs moved (for SKUs with pack size > 1)
- **I^d_{t,s,l} ≥ 0:** Inventory at **start** of day (after propagation from previous day)
- **I^f_{t,s,l} ≥ 0:** Inventory at **end** of day (after putaway decisions)
- **Y_{t,s,l} ∈ {0,1}:** Binary indicator that SKU `s` has positive inventory on location `l` at end of day
- **W_{t,s,s',l} ∈ {0,1}:** Binary indicator that SKUs `s` and `s'` are both present on location `l` (for similarity penalty)
- **K_{t,l} ∈ {0,1}:** Binary indicator that location `l` is utilized (has at least one SKU)

#### Key Constraints

**1. Depot isolation (no storage at depot):**
```
∀t, ∀s: I^d_{t,s,Depot} = 0, I^f_{t,s,Depot} = 0
```

**2. One format per location (or none):**
```
∀t, ∀l ∈ L_stock: ∑_{f∈F} E_{t,l,f} = U_{t,l}
```
*Interpretation:* Location either has exactly one format active (U=1, one E=1) or is unused (U=0, all E=0)

**3. Inventory presence implies format compatibility:**
```
∀t, ∀l ∈ L_stock, ∀s: Y_{t,s,l} ≤ E_{t,l,fmt(s)}
```
*Interpretation:* Can't store SKU `s` on location `l` unless location's active format matches SKU's format

**4. Capacity constraints (start and end of day):**
```
∀t, ∀l ∈ L_stock: ∑_{s∈S} I^d_{t,s,l} ≤ ∑_{f∈F} Cap(f,l) · E_{t,l,f}
∀t, ∀l ∈ L_stock: ∑_{s∈S} I^f_{t,s,l} ≤ ∑_{f∈F} Cap(f,l) · E_{t,l,f}
```

**5. Putaway availability (can't place more than what's in receiving):**
```
∀t, ∀s: ∑_{l∈L_stock} Q_{t,s,l} ≤ I^d_{t,s,recv}
```

**6. Pack size compliance:**
```
∀t, ∀s with Pack(s) > 1, ∀l ∈ L_stock: Q_{t,s,l} = Pack(s) · MQ_{t,s,l}
```
*Interpretation:* Can only move full packs (e.g., if pack size = 12, can move 0, 12, 24, 36, ... units)

**7. Initial inventory (first day):**
```
I^d_{t₀,s,recv} = I^prev_{s,recv} + A_{t₀,recv,s}
I^d_{t₀,s,l} = I^prev_{s,l}  (∀l ∈ L_stock)
```

**8. State propagation between days:**
```
∀t → t⁺, ∀s, ∀l: I^d_{t⁺,s,l} = I^f_{t,s,l}
```

**9. Inventory balance (end-of-day inventory):**
```
Receiving: I^f_{t,s,recv} = I^d_{t,s,recv} - ∑_{l∈L_stock} Q_{t,s,l}
Storage: I^f_{t,s,l} = I^d_{t,s,l} + Q_{t,s,l}
```

**10. Similarity tracking (for co-location penalty):**
```
∀t, l, ∀s < s': W_{t,s,s',l} ≤ Y_{t,s,l}
∀t, l, ∀s < s': W_{t,s,s',l} ≤ Y_{t,s',l}
∀t, l, ∀s < s': W_{t,s,s',l} ≥ Y_{t,s,l} + Y_{t,s',l} - 1
```
*Interpretation:* W is 1 if and only if both SKUs are present on same location

#### Objective Function

The objective is a **weighted sum** balancing multiple operational goals:
```
min w_move · ∑_{t,s,l∈L_stock} Q_{t,s,l} · C(recv, l)          [Movement cost]
    + w_backlog · ∑_{t,s} I^f_{t,s,recv}                        [Receiving backlog penalty]
    + w_sim · ∑_{t,l} ∑_{s<s'} Sim(s,s') · W_{t,s,s',l} / |L_stock|  [Similarity penalty]
    + w_neardepot · ∑_{t,s,l∈L_stock} demNorm_t(s) · DistDepot(l) · I^f_{t,s,l}  [Proximity to depot]
    + w_occupation · ∑_{t,l∈L_stock} K_{t,l} / |L_stock|        [Space utilization penalty]
```

**Objective components explained:**

1. **Movement cost:** Penalizes long-distance putaway (prefer storing near receiving)
2. **Receiving backlog:** Strongly penalizes inventory stuck in receiving (must clear daily arrivals)
3. **Similarity penalty:** Discourages storing dissimilar SKUs together (reduces picking confusion)
4. **Proximity to depot:** Rewards storing high-demand SKUs near depot (reduces picking distance)
5. **Occupation penalty:** Penalizes spreading inventory across too many locations (encourages consolidation)

**Weight tuning:** These weights encode operational priorities and must be calibrated based on warehouse-specific goals.

---

### Model 2: Picking (Order Fulfillment)

**Objective:** Allocate picks across storage locations to fulfill customer orders while minimizing travel distance.

#### Decision Variables

- **p_{t,s,l,n} ≥ 0:** Quantity of SKU `s` picked from location `l` for order `n` on day `t`
- **I^d_{t,s,l}:** Inventory at start of picking (from DSLAP output)
- **I^f_{t,s,l}:** Inventory at end of picking (after picks executed)

#### Key Constraints

**1. Initial inventory (from DSLAP):**
```
∀s, ∀l ∈ L_no_recv: I^d_{t₀,s,l} = I^prev_{s,l}
```

**2. Order fulfillment (don't exceed demand):**
```
∀t, ∀n, ∀s ∈ lines(n): ∑_{l∈L_no_recv} p_{t,s,l,n} ≤ dem_{t,s,n}
```
*Interpretation:* Can pick up to demanded quantity, but not more (allows partial fulfillment)

**3. Inventory availability (can't pick what you don't have):**
```
∀t, ∀s, ∀l ∈ L_no_recv: ∑_n p_{t,s,l,n} ≤ I^d_{t,s,l}
```

**4. Inventory balance:**
```
∀t, ∀s, ∀l ∈ L_no_recv: I^f_{t,s,l} = I^d_{t,s,l} - ∑_n p_{t,s,l,n}
```

**5. State propagation:**
```
∀t → t⁺, ∀s, ∀l ∈ L_no_recv: I^d_{t⁺,s,l} = I^f_{t,s,l}
```

#### Objective Function
```
min -w_pick · ∑_{t,s,l,n} p_{t,s,l,n}  [Maximize volume served]
    + w_dist · ∑_{t,s,l,n} p_{t,s,l,n} · DistDepot(l)  [Minimize travel distance]
```

**Trade-off:** The negative term maximizes served volume (fill as many orders as possible), while the distance term pushes picks toward locations closer to depot when multiple locations can satisfy the same demand.

---

### Model 3: Replenishment (Internal Relocation)

**Objective:** At day `t`, relocate inventory flows `q_{s,l_f,l_t}` on explicit arcs `(l_f → l_t)` to improve layout quality without excessive operational churn.

**Why this is hard:** Without restrictions, replenishment would consider all possible movements: `|S| × |L|²` decision variables (e.g., 20 SKUs × 100 locations × 100 locations = 200,000 variables).

**Solution:** Restrict arc set to **Top-K nearest neighbors** via distance matrix `C(l_f, l_t)`.
```python
# Build restricted arc set
ARCS = []
for loc_from in storage_locations:
    # Find K nearest locations
    neighbors = sorted(storage_locations, key=lambda l: distance_matrix[loc_from][l])[:K]
    for loc_to in neighbors:
        if loc_from != loc_to:
            ARCS.append((loc_from, loc_to))
```

**Result:** Arc set reduced from `|L|²` to `|L| × K` (e.g., 100² = 10,000 → 100 × 10 = 1,000).

#### Decision Variables

- **q_{s,l_f,l_t} ≥ 0:** Quantity of SKU `s` moved from location `l_f` to location `l_t`
- **MQ_{s,l_f,l_t} ∈ ℤ₊:** Integer number of packs moved (for pack-size SKUs)
- **E_{l,f} ∈ {0,1}:** Format active on location `l` (can change during replenishment)
- **U_l ∈ {0,1}:** Location in use
- **I^d_{s,l}:** Inventory before replenishment
- **I^f_{s,l}:** Inventory after replenishment
- **Y_{s,l} ∈ {0,1}:** SKU presence indicator
- **W_{s,s',l} ∈ {0,1}:** Co-location indicator (for similarity penalty)
- **Z_{s,l_f,l_t} ∈ {0,1}:** Binary activation variable for movement arc

#### Key Constraints

**1. Initial inventory:**
```
∀s, ∀l: I^d_{s,l} = I^prev_{s,l}
```

**2. One format per location:**
```
∀l: ∑_{f∈F} E_{l,f} = U_l
```

**3. Format locking (detected formats stay fixed):**
```
∀l with f⋆(l) ≠ ∅: E_{l,f⋆(l)} = U_l, E_{l,f} = 0 (f ≠ f⋆(l))
```
*Interpretation:* If location already has inventory of format `f`, lock that format (prevents format switching chaos)

**4. Inventory balance:**
```
∀s, ∀l: I^f_{s,l} = I^d_{s,l} + ∑_{l_f ∈ IN(l)} q_{s,l_f,l} - ∑_{l_t ∈ OUT(l)} q_{s,l,l_t}
```

**5. Outflow availability (can't send more than you have):**
```
∀s, ∀l_f: ∑_{l_t ∈ OUT(l_f)} q_{s,l_f,l_t} ≤ I^d_{s,l_f}
```

**6. Format compatibility for movements:**
```
∀s, ∀(l_f, l_t) ∈ ARCS: q_{s,l_f,l_t} ≤ M_{l_f} · E_{l_t,fmt(s)}
```
*Interpretation:* Can only move SKU to location if destination format matches SKU format

**7. Capacity constraints:**
```
∀l: ∑_s I^f_{s,l} ≤ ∑_f Cap(f,l) · E_{l,f}
```

**8. Pack size compliance:**
```
∀s with Pack(s) > 1, ∀(l_f, l_t) ∈ ARCS: q_{s,l_f,l_t} = Pack(s) · MQ_{s,l_f,l_t}
```

**9. Movement activation:**
```
∀s, ∀(l_f, l_t) ∈ ARCS: q_{s,l_f,l_t} ≤ M_{l_f} · Z_{s,l_f,l_t}
```

**10. Maximum actions limit (prevent excessive churn):**
```
∑_{s,(l_f,l_t)∈ARCS} Z_{s,l_f,l_t} ≤ 20
```
*Interpretation:* Limit total number of replenishment moves per day (operational constraint)

#### Objective Function
```
min w_neardepot · ∑_s ∑_l demNorm_t(s) · DistDepot(l) · I^f_{s,l}  [Proximity to depot]
    + w_sim · ∑_l ∑_{s<s'} Sim(s,s') · W_{s,s',l}                   [Similarity penalty]
    + w_move · ∑_s ∑_{(l_f,l_t)∈ARCS} q_{s,l_f,l_t} · C(l_f, l_t)    [Movement cost]
    + w_actions · ∑_{s,(l_f,l_t)∈ARCS} Z_{s,l_f,l_t}                 [Action count penalty]
```

**Trade-off:** Balance layout improvement (near depot, similarity) against operational disruption (movement cost, number of actions).

---

## Orchestration & KPI Tracking

Each day follows the same pipeline with **persistent KPI computation**:

### Daily Execution Sequence
```
BEFORE → DSLAP → PICK → REPLEN → AFTER
```

**KPI computation logic:**

KPIs are computed **before and after each optimization stage**. Critically, **if a stage does not modify a variable, the corresponding KPI is preserved** from the previous stage.

**Why this matters:**

If picking doesn't change layout (only reduces inventory), then layout-related KPIs (weighted distance, dispersion, similarity) should remain unchanged. Recomputing them would introduce numerical noise and obscure which stage actually drove performance changes.

**Implementation:**
```python
def compute_kpis_with_persistence(stage_name, variables_changed, current_state, previous_kpis):
    """
    Compute KPIs for current stage, persisting unchanged metrics from previous stage.
    """
    kpis = {}
    
    if 'inventory' in variables_changed:
        kpis['weighted_distance'] = compute_weighted_distance(current_state)
        kpis['dispersion'] = compute_dispersion(current_state)
        kpis['occupation'] = compute_occupation(current_state)
    else:
        # Persist from previous stage
        kpis['weighted_distance'] = previous_kpis['weighted_distance']
        kpis['dispersion'] = previous_kpis['dispersion']
        kpis['occupation'] = previous_kpis['occupation']
    
    if 'format_assignment' in variables_changed:
        kpis['similarity_penalty'] = compute_similarity(current_state)
    else:
        kpis['similarity_penalty'] = previous_kpis['similarity_penalty']
    
    return kpis
```

### Tracked KPIs

**Layout quality:**
- **Weighted distance to depot:** `∑_{s,l} demand_weight(s) · distance(depot, l) · inventory(s, l)`
- **Dispersion:** Number of distinct locations holding inventory
- **Occupation rate:** `total_inventory / total_available_capacity`
- **Similarity penalty:** `∑_{l} ∑_{s<s'} Sim(s,s') · colocated(s, s', l)`

**Operational metrics:**
- **Layout churn:** `∑_{s,l} |I^f_{s,l} - I^d_{s,l}|` (total inventory movement within day)
- **Number of relocation actions:** Count of executed movements (DSLAP + REPLEN)
- **Total movement cost:** `∑ quantity_moved · distance`

**Picking performance:**
- **Turnover:** Fraction of inventory picked per day
- **Workload balance:** Coefficient of variation of picks across zones/locations
- **Fill rate:** `total_picked / total_demanded`

**Capacity utilization:**
- **Average utilization:** `mean(inventory / capacity)` across active locations
- **Peak utilization:** `max(inventory / capacity)`
- **Underutilized locations:** Count of locations with utilization < threshold

---

## Results & Insights

### Experimental Scenario

**Configuration:**
- Horizon: T = 5 days
- SKUs: 20 products with heterogeneous demand (Poisson-distributed)
- Formats: 4 types with pack sizes {12, 25, 1000}
- Orders: 10-30 per day with 2-5 line items each
- Arrivals: Poisson-distributed, aligned with pack sizes
- Solver: PuLP with CBC (timeout 300s per stage)

### Quantitative Results

**Layout quality improvements (Day 1 → Day 5):**
- **Weighted distance to depot:** ↓ 18% for high-demand SKUs
- **Similarity penalty:** ↓ 25% (fewer dissimilar SKUs co-located)
- **Dispersion:** ↓ 12% (inventory more consolidated)

**Operational costs:**
- **Relocation actions:** Avg 15 moves/day (within operational limit of 20)
- **Movement cost:** Controlled (weight tuning prevents excessive churn)
- **Receiving backlog:** 0 (all daily arrivals cleared)

**Picking efficiency:**
- **Average pick distance:** ↓ 22% (high-demand items closer to depot)
- **Workload balance (CV):** ↓ 15% (picks distributed more evenly across zones)
- **Fill rate:** 95%+ (high order fulfillment with partial picks when necessary)

### Qualitative Insights

**1. Trade-offs become explicit:**

Unlike black-box heuristics, MILP formulations force explicit trade-offs via objective weights:
- Increase `w_neardepot` → SKUs migrate toward depot, but movement cost increases
- Increase `w_actions` → Fewer relocations, but layout quality degrades slower
- Increase `w_backlog` → Receiving clears faster, but putaway distances may increase

**This transparency is operationally valuable:** Warehouse managers can see *why* certain decisions were made and adjust priorities accordingly.

**2. Feasibility is non-negotiable:**

All solutions respect:
- Physical capacity constraints
- Format compatibility rules
- Pack size requirements
- Inventory conservation laws

**This distinguishes MILP from heuristics** that might produce "good" solutions that violate operational constraints in subtle ways.

**3. Stability matters as much as optimality:**

Early experiments with aggressive replenishment (high `w_neardepot`, low `w_actions`) achieved excellent layout scores but created **operational chaos**:
- 50+ relocations per day
- Inventory constantly moving
- Pickers unable to rely on stable locations

**Tuning replenishment conservatively** (limit 20 actions/day, higher movement cost) achieved 90% of layout quality improvement with 1/3 the operational disruption.

**This is the essence of practical optimization:** Not finding the theoretical optimum, but finding solutions that **operators trust and can execute reliably**.

---

## Implementation: Reproducibility & Exports

The pipeline systematically exports all artifacts for **reproducibility and auditability**:

### Graph & Topology Exports
```
locations2d.csv          # 2D node coordinates, types (Depot, recv, storage, path)
arcs2d.csv              # 2D edges with distances
locations3d.csv          # 3D node coordinates (with vertical levels)
arcs3d.csv              # 3D edges (horizontal + vertical movements)
```

### MILP Input Data
```
time_periods.csv         # Days in planning horizon
skus.csv                # Product catalog (SKU, format, pack size, demand patterns)
formats.csv             # Format definitions
capacities.csv          # Cap(format, location)
arrivals.csv            # A(t, recv, s) - inbound shipments
initial_inventory.csv    # I₀(s, l) - starting state (validated feasible)
orders.csv              # Customer orders (order ID, day, SKU, quantity)
distance_matrix.csv      # C(l_i, l_j) - shortest path costs
```

### Optimization Outputs
```
base_Q.csv              # Putaway decisions: Q(t, s, l)
base_p.csv              # Picking decisions: p(t, s, l, n)
base_q.csv              # Replenishment flows: q(s, l_f, l_t)
base_I_f.csv            # End-of-day inventory: I^f(t, s, l)
```

### KPI Results
```
kpis_results.csv        # Structured KPIs:
                        # - Stage (BEFORE, AFTER_DSLAP, AFTER_PICK, AFTER_REPLEN)
                        # - Day
                        # - Metric name
                        # - Value
                        # - Delta from previous stage
```

**Why this matters for production deployment:**

1. **Auditability:** Every optimization run is fully reproducible
2. **Debugging:** Can trace exactly which stage/day caused KPI changes
3. **Validation:** Can verify solver outputs against operational constraints
4. **Continuous improvement:** Can A/B test different weight configurations systematically

---

## Current Limitations & Technical Challenges

### 1. Arc Set Dependency in Replenishment

**Problem:** Replenishment depends on completeness of arc set. If `ARCS` is too sparse (Top-K too small), some beneficial relocations become impossible.

**Manifestation:** If location A has excess inventory and location B near depot is underutilized, but `(A, B) ∉ ARCS`, replenishment cannot improve layout.

**Mitigation strategies:**
- Increase K (more neighbors)
- Use zone-based arcs (all locations within same zone are mutually connected)
- Hybrid: Top-K + strategic arcs (e.g., all → depot zone)

### 2. Objective Weight Calibration

**Problem:** If `w_move = 0` and `w_neardepot = 0`, then `q = 0` is optimal (no replenishment). Conversely, if `w_actions` too low, model relocates excessively.

**Current approach:** Manual tuning via sensitivity analysis.

**Better approach (future work):**
- Multi-objective optimization (Pareto frontier exploration)
- Reinforcement learning to learn weight policies from historical performance
- Stakeholder elicitation (conjoint analysis to infer implicit preferences)

### 3. Computational Complexity: Similarity Variables

**Problem:** Similarity tracking uses `W_{s,s',l}` variables: `O(|S|² × |L|)` binary variables (e.g., 20² × 100 = 40,000 variables).

**Impact:** Scales poorly for large product catalogs.

**Mitigation strategies:**
- **SKU clustering:** Group similar products, track similarity at cluster level
- **Approximation:** Sample SKU pairs for similarity penalty instead of exhaustive enumeration
- **Decomposition:** Track similarity only for active locations (locations with inventory)

### 4. Solver Performance at Scale

**Current:** CBC (open-source) solves instances with T=5, |S|=20, |L|=100 in ~60s per stage.

**Projected:** Real warehouse with T=7, |S|=500, |L|=2000 would exceed computational budget.

**Scalability strategies:**
- **Decomposition:** Solve by warehouse zone (with coupling constraints)
- **Rolling horizon:** Optimize days 1-3 with high fidelity, days 4-5 with aggregation
- **Warm start:** Use previous day's solution as MIP start for next day
- **Commercial solver:** Gurobi, CPLEX significantly faster than CBC for large MILP

---

## Future Work: Next Steps

### 1. Multi-Objective Optimization

Current approach uses **weighted sum** of objectives. This requires manual weight tuning and obscures trade-off structure.

**Proposed:** Generate **Pareto frontier** using ε-constraint method:
```python
# Fix all objectives except one, vary bounds on fixed objectives
for epsilon in [0.1, 0.2, 0.3, ...]:
    model = MILP_model()
    model.objective = minimize(near_depot_cost)
    model.add_constraint(move_cost <= epsilon * baseline_move_cost)
    model.add_constraint(actions_count <= epsilon * baseline_actions)
    solutions.append(model.solve())

# Present Pareto frontier to stakeholders for preference elicitation
```

### 2. Demand Scenario Integration

Current models use **deterministic demand** (known orders for day `t`).

**Reality:** Day `t+1, t+2, ...` demands are uncertain.

**Proposed:** Stochastic programming or robust optimization:
```python
# Generate demand scenarios
scenarios = generate_demand_scenarios(historical_data, n_scenarios=100)

# Two-stage stochastic MILP:
# Stage 1: Slotting decisions (here-and-now)
# Stage 2: Picking decisions (wait-and-see, per scenario)
model.objective = first_stage_cost + expected_value(second_stage_cost, scenarios)
```

**Benefit:** Slotting decisions hedge against demand uncertainty, not just optimize for expected demand.

### 3. SKU Clustering for Dimensionality Reduction

For warehouses with hundreds of SKUs, track similarity and co-location at **cluster level**:
```python
# K-means clustering on (demand_pattern, physical_attributes)
clusters = cluster_skus(skus, n_clusters=10)

# Reformulate similarity: W_{c,c',l} for clusters c, c'
# Variables reduced from O(|S|²) to O(|C|²)
```

### 4. Baseline Comparison: Random Admissible Orchestrator

**Current:** No benchmark to validate that MILP is actually better than simpler heuristics.

**Proposed:** Implement **random admissible policy**:
- Slotting: Place arrivals randomly at locations with compatible format and available capacity
- Picking: Pick randomly from locations with inventory
- Replenishment: Randomly relocate up to 20 moves per day

**Comparison:** MILP vs. Random on same data over 30 runs.

**Hypothesis:** MILP should achieve 20-30% better weighted distance with comparable operational churn.

---

## Conclusion: Why This Framework Matters

This project provides a **complete, reproducible foundation** for warehouse optimization that respects operational reality:

✅ **Feasibility guaranteed:** All solutions respect capacity, format compatibility, pack sizes, inventory conservation

✅ **Operational realism:** Models sequential decision-making (slotting → picking → replenishment), not isolated optimization

✅ **Transparency:** Explicit objective trade-offs, interpretable decisions, full auditability

✅ **Stability-aware:** Balances layout quality improvement against operational churn

✅ **Production-ready infrastructure:** Automated data generation, validation, orchestration, KPI tracking, exports

**This is not an academic exercise.**

This is a framework designed to **actually run in production warehouses**, where:
- Operators need to trust recommendations
- Feasibility failures are catastrophic
- Stability matters as much as optimality
- Decisions must be explainable to non-technical stakeholders

---

## Key Takeaway

> A good MILP model is not the one that finds the mathematically optimal solution.
>
> It's the one that produces **feasible, explainable, and stable decisions** day after day, even when reality deviates from assumptions.

**This framework achieves that.**

---

**Tech Stack:**  
Python · PuLP · Gurobi · Pandas · NumPy · NetworkX · Plotly · Matplotlib

**Repository:**  
[GitHub – WA2 Warehouse Optimization](https://github.com/MBalogogGLemuel) *Realesed soon*

**Related Work:**  
- [Why Feasibility Matters More Than Optimality](/blog/feasibility-over-optimality)
- [Why Optimization Problems Expose the Limits of AI Reasoning](/blog/optimization-ai-limits)

---

*Optimization is not about finding the perfect solution.*  
*It's about finding solutions that survive contact with operational reality.*
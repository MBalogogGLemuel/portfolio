---
title: "Projects"
toc: true
toc-depth: 2
page-class: projects-page
---

- [Electrical Engineering & Telecom](#Telecom)
- [Forecasting](#forecasting)
- [Deterministic Optimization](#optimisation)
- [Stochastic Optimization](#stochastic)
- [Reinforcement Learning](#rl)
- [Data Engineering & MLOps](#dataeng)
- [Natural Language Processing & LLM](#llm)
- [Deep Learning & Computer Vision](#dl-cv)

::: {.justify}

This page presents my technical projects organized by **area of specialization**.

Each section features **selected projects** with their **context**, **methodological approach**, **results**, and **links to code** when available.

---

## Telecom & Electrical Engineering {#Telecom}

::: {.cards-grid}

::: {.project-card}

<img class="project-thumb" src="../../assets/monopol3d.PNG" alt="Antenna array optimization using ANN">

<div class="project-body">

### P1 - Beamforming Optimization of a Planar Antenna Array using Artificial Neural Networks (ANN)

**Context.**  
Design of a smart planar antenna (4×5 monopole array) and training of a **multilayer perceptron (MLP)** to predict the **feeding law (amplitudes/phases)** to form a beam in a target direction.

**Approach.**
- Analytical baseline: **Dolph–Chebyshev** (sidelobe control).
- ML model: **ANN / MLP with backpropagation**, **MSE criterion**.
- Evaluation chain: **MATLAB (NNSTART) → CST Microwave Studio** (co-simulation and control).

**Key Results.**
- **Directivity gain ≈ +0.2 dB** compared to the Dolph–Chebyshev method.
- **Computation time ≈ 1.27 s** for feeding law generation.
- Network training time: **≈ 27 h 59 min 58 s** (backpropagation).

**Technologies.**  
MATLAB R2018a · Neural Network Toolbox (NNSTART) · CST Microwave Studio · Array Antennas · Beamforming · Heuristic Optimization · MLP

**Deliverables.**
- Detailed technical report (Scientific article)
- MATLAB scripts (training and inference)
- Electromagnetic models and simulations in CST
- 2D and 3D radiation patterns

<div class="project-links">
[Read report (PDF)](../../assets/OptimreseauxantennesRNA/Synthese-Optim_reseaux_antennes_planaires.pdf){.btn .btn-primary}
[GitHub source code](https://github.com/MBalogogGLemuel/Optimisation-reseaux-antenne-RNA){.btn .btn-outline}

</div>

:::


::: {.project-card}

<img class="project-thumb" src="../../assets/_site/assets/Detection_par_finger_printing/images/point_mesure.png" alt="Wi-Fi fingerprinting detection">

<div class="project-body">

### P2 - Indoor Wi-Fi Localization via RSSI Fingerprinting  
**Machine Learning (ANN/MLP) vs PSO Comparison**

**Summary.**  
Indoor localization prototype in a **10 m × 10 m area** with **4 Wi-Fi access points**.  
I formulated the problem as **supervised regression**: \((RSSI_1,\dots,RSSI_4)\rightarrow(x,y)\), then compared a **neural network (MLP)** to **PSO** in accuracy and latency.



**Objective.**
- Estimate position **(x, y)** from Wi-Fi RSSI (fingerprinting)
- Optimize the **accuracy / computation time** tradeoff (real-time telecom constraint)

**Methodology.**
- **Experimental setup**: **100 m²** area, **2.4 GHz**, 4 APs at corners
- **Dataset**: **50 reference points** (fingerprinting database)
- **ML Model**: **MLP (ANN)**, regression, **MSE** loss
- **Baseline**: **PSO** (60 particles, \(c_1=c_2=2\), \(w\in[0.4,0.9]\), 1000 iterations)
- **Condition**: LOS (controlled protocol)


**Key Results.**
- **ANN (MLP)**: **2.5729 m** mean error | **0.206288 s**
- **PSO**: **2.8612 m** mean error | **1.360301 s**
- **ANN Gain**: **≈ +11%** more accurate | **≈ ×6.6** faster (inference)

**Why this is strong for a recruiter (Telecom × Data).**
- Reformulation of a radio problem into an **exploitable ML pipeline**
- Rigorous analysis of **latency vs accuracy** tradeoffs
- Structured comparison **ML vs heuristic optimization**
- Directly transferable approach to **IoT / smart buildings / radio analytics**


**Stack & Skills.**
- Wi-Fi Fingerprinting (RSSI), radio propagation (FSPL/Friis)
- Supervised regression, MLP, MSE, evaluation
- PSO (tuning, convergence, computational cost)
- MATLAB + LaTeX reporting

<div class="project-links">
[Report (PDF)](../../assets/Detectionparfingerprinting/Synthese-Detection_par_fingerprinting.pdf){.btn .btn-primary}
[GitHub](https://github.com/MBalogogGLemuel/Detection-par-fingerprinting){.btn .btn-outline}

</div>

:::

:::




## Forecasting {#forecasting}

Development of **time series forecasting systems applied to real energy systems**, with particular emphasis on **statistical robustness**, **incorporation of exogenous variables**, and **rigorous comparison between classical and deep learning approaches**.

::: {cards-grid}

::: {.project-card}

<img class="project-thumb" src="../../assets/jcplforecast.png" alt="JCPL energy forecasting">

<div class="project-body">

### P1 — Regional Energy Consumption Forecasting (Statistical Approach)

**Context**  
**Daily energy consumption forecasting** project (peak load) for regions served by **JCP&L (New Jersey)**, in a **medium-term energy planning** context.  
The objective is to anticipate demand fluctuations while maintaining **high interpretability** of explanatory mechanisms.

**Methodological Approach**  
The approach is based on classical time series modeling (Naive, ETS, SARIMAX) explicitly integrating:

- **energy seasonality**,  
- **long-term trends**,  
- **exogenous climate and socio-demographic variables**.

Models are evaluated in a strict **rolling time validation** framework, ensuring realistic out-of-sample performance estimation.

**Key Results**
- Stable and consistent regional-scale forecasts  
- Clear identification of seasonal components and exogenous effects  
- Construction of a **reliable operational baseline**  

**Added Value**  
This project illustrates the value of well-specified statistical models for **energy decision-making** and **operational planning**, when transparency and traceability of assumptions are essential.

**Tech**: `R` · `Statistical Forecasting` · `Time Series` · `Power BI` · `Tableau` 

<div class="project-actions">
<a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Case Study</a>
</div>

</div>

:::

::: {.project-card}

<img class="project-thumb" src="../../assets/mrcforecast.png" alt="Multi-region Quebec energy forecasting via deep learning">

<div class="project-body">

### P2 — Multivariate Energy Forecasting via Deep Learning

**Context**  
Extension of the previous project toward a **data-driven approach**, aiming to capture **complex nonlinear relationships** between energy consumption, weather conditions, and regional dynamics.  
The framework is **multi-regional forecasting**, at the scale of Quebec's administrative regions, characterized by heterogeneous behaviors and partially non-stationary series.

**Methodological Approach**  
Modeling relies on sequential architectures (RNN-LSTM, TCN), systematically compared to traditional statistical models (SARIMA), to:

- exploit **sliding time windows**,  
- integrate **stabilizing transformations** of the target variable,  
- automatically learn dynamic dependencies between variables.

Particular attention is paid to **comparative evaluation** of approaches to measure the real gain brought by deep learning.

**Key Results**
- Performance improvement in certain high-variability regions  
- Better capture of delayed effects and nonlinearities  
- Robust behavior on recent out-of-sample periods  
- Confirmation of the **persistent relevance of traditional statistical models**  

**Added Value**  
This work highlights the **strengths and limitations of deep learning applied to energy forecasting**, and proposes a pragmatic framework to arbitrate between **predictive performance**, **temporal stability**, and **operational complexity**.

**Tech**: `Python` · `Pytorch` · `TensorFlow` · `Scikit-learn` · `Pandas` · `NumPy` · `Matplotlib` · `Power BI`

<div class="project-actions">
<a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Comparative Study</a>
</div>

</div>

:::

:::










## Deterministic Optimization {#optimisation}

Design and analysis of **deterministic optimization models** for real problems of planning, resource allocation, and project management, with particular emphasis on **translating operational constraints into exploitable mathematical formulations** and on **analyzing cost–time–resource tradeoffs**.

::: {cards-grid}

::: {.project-card}

<img class="project-thumb" src="../../assets/warehouse-optim4.png" alt="Deterministic optimization of an industrial logistics warehouse">

<div class="project-body">

### P1 — Integrated Deterministic Optimization of a Logistics Warehouse (MILP)

**Context**  
Recent industrial project aiming at operational optimization of a multi-zone, multi-format, multi-level logistics warehouse, in a real context of **high flow variability**, **strict physical constraints**, and **daily operational pressure**.  
The project is conducted in a confidential framework; the elements presented here are deliberately **abstract and anonymized**.

The overall objective is to simultaneously improve:

- the efficiency of **stock placement**,  
- **order picking** performance,  
- and the consistency of **internal replenishments**,  
while maintaining **day-to-day operational feasibility**.

**Methodological Approach**  
The problem is formulated as a **sequential deterministic optimization** structured around three complementary sub-models:

- **Placement (Putaway / Slotting)**: format activation decisions per location and allocation of incoming arrivals, under capacity, compatibility, and functional proximity constraints.
- **Order Picking**: optimal allocation of pickings to maximize service while minimizing internal movements.
- **Internal Replenishment**: controlled stock relocation via an admissible subset of arcs, to limit combinatorics and movement costs.

These models are **orchestrated sequentially at daily scale**, with consistent propagation of inventory states between steps.

**Evaluation and Performance Indicators**  
An indicator engine allows evaluating the impact of decisions before and after each step, notably:

- weighted average distance to central point,
- occupancy rate and stock dispersion,
- co-location penalties (similarity),
- volume and cost of internal movements,
- layout stability (churn).

This approach enables a **clear quantitative reading of operational tradeoffs** induced by each decision.

**Key Results**

- Measurable improvement in accessibility of high-demand items  
- Reduction of unnecessary internal movements  
- Better layout stability under dynamic flows  
- Reproducible framework to test realistic operational scenarios  

**Added Value**  
This project illustrates a complete implementation of **deterministic optimization applied to a real logistics system**, combining mathematical modeling, decision orchestration, and performance analysis.  
It constitutes a solid foundation for future extensions toward **stochastic** approaches or **reinforcement learning**, while remaining operationally exploitable.

**Tech**: `Python` · `GulP/Gurobi` · `Plotly` · `NetworkX` · `Pandas` · `NumPy` · `Matplotlib` · `Power BI`

<div class="project-actions">
<a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Industrial Study</a>
</div>

</div>

:::


::: {.project-card}

<img class="project-thumb" src="../../assets/optim-project1.png" alt="Scheduling and acceleration of software project">

<div class="project-body">

### P2 — Scheduling and Acceleration of a Complex Software Project

**Context**  
Planning problem for software development composed of several interdependent sub-projects, with strict precedence constraints.  
The initial objective is to **minimize total project duration**, then explore **acceleration strategies under budget constraints**, integrating partial or total outsourcing decisions.

**Methodological Approach**  
The problem is formulated as a **deterministic scheduling model**, progressively enriched by:
- **explicit precedence constraints** between tasks,
- **binary decision variables** to represent outsourcing,
- **global deadline constraints** imposed by the client,
- linear mechanisms to model **non-uniform costs**, **conditional discounts**, and **partial delivery thresholds** (e.g., 75% of tasks delivered before target date).

Each model adjustment aims to preserve **linearity**, while increasing decision realism.

**Key Results**
- Significant reduction in delivery time under cost constraints  
- Identification of critical tasks to outsource as priority  
- Fine analysis of tradeoffs between **acceleration cost** and **deadline compliance**  

**Added Value**  
This project illustrates a complete **incremental modeling** approach, typical of real industrial contexts, where requirements evolve and require robust, explainable, and adaptable models.

**Tech**: `Excel Solver` · `Gams` · `IBM Cplex` · `PowerBi`

<div class="project-actions">
<a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Case Study</a>
</div>

</div>

:::

::: {.project-card}

<img class="project-thumb" src="../../assets/optim-locomotive2.png" alt="Dynamic locomotive fleet allocation">

<div class="project-body">

### P3 — Dynamic Multi-Site Locomotive Fleet Allocation

**Context**  
Fleet management problem for locomotives circulating between several interconnected sites (A–B–C), with imposed departure and arrival schedules and **minimum capacity requirements per trip**.  
The challenge is to **minimize the total number of locomotives needed**, while ensuring operational network feasibility.

**Methodological Approach**  
The system is modeled as a **multi-period deterministic allocation** problem, integrating:
- **minimum capacity constraints per trip**,
- **flow balance equations** ensuring consistency of availability per site,
- **cyclic conditions** ensuring fleet stability over the time horizon.

The model explicitly captures the temporal dynamics of resources and interdependencies between local and global decisions.

**Key Results**
- Determination of **minimum fleet required** to satisfy all trips  
- Clear visualization of flows and resource tensions  
- Highlighting of critical periods in terms of availability  

**Added Value**  
This work highlights the power of deterministic models for **logistics network planning**, when priority is given to robustness and feasibility rather than stochasticity.

<div class="project-actions">
<a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Operational Analysis</a>
</div>

**Tech**: `Python` · `Gams` · `IBM Cplex` · `PowerBi` · `Excel Solver`

</div>

:::

::: {.project-card}

<img class="project-thumb" src="../../assets/optim-maintenance3.png" alt="Industrial maintenance planning">

<div class="project-body">

### P4 — Optimal Maintenance Planning for Power Plants

**Context**  
Maintenance planning problem for several power plants over a multi-month horizon, under **team availability constraints**, **authorized launch windows**, and **calendar-dependent costs**.  
Two objectives are studied: **total cost minimization** and **earliest completion of all maintenance**, regardless of budget.

**Methodological Approach**  
The problem is formulated as a **linear optimization model in binary variables**, integrating:
- uniqueness constraints on start month per plant,
- global personnel capacity constraints per period,
- temporal relationships linking start month to completion date,
- adjustments imposing **ordering or synchronization relationships** between certain plants.

This approach allows rapid testing of several strategic scenarios.

**Key Results**
- Measured reduction in maintenance costs under realistic constraints  
- Clear comparison between **cost-minimal** and **time-minimal** strategies  
- Identification of human resource bottlenecks  

**Added Value**  
This project demonstrates how deterministic optimization can serve as a strategic decision support tool in critical industrial contexts, where resources are limited and operational consequences major.

**Tech**: `Excel Solver` · `Gams` · `IBM Cplex` · `PowerBi` 

<div class="project-actions">
<a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Decision Study</a>
</div>

</div>

:::
 
:::

## Stochastic Optimization {#stochastic}

Design of **optimization models under uncertainty** aimed at supporting decision-making when key parameters (demand, performance, environment) are **neither deterministic nor perfectly observable**.  
The projects below emphasize **probabilistic modeling**, **risk–performance tradeoff**, and **value of information**.

::: {.cards-grid}

::: {.project-card}

<img class="project-thumb" src="../../assets/bayesian-optim1.png" alt="Bayesian optimization for experimental systems">

<div class="project-body">

### P1 — Bayesian Optimization for Expensive Experimental Systems

**Context**  
This project addresses an optimization problem where the objective function is **unknown**, **noisy**, and **expensive to evaluate**, typical of industrial or experimental environments.  
The case study focuses on optimizing the **mechanical strength of a polymer**, depending on continuous physico-chemical parameters (plasticizer proportion x₁ ∈ [0,1] and curing time x₂ ∈ [0,5] hours), for which each evaluation represents a real cost (time, resources, laboratory tests).

The objective is to identify the optimal formulation while **minimizing the number of experiments required**, while explicitly managing the uncertainty inherent in measurements.

**Methodological Approach**  
The strategy relies on **sequential Bayesian optimization**, structured around:

- a **probabilistic surrogate model** (Gaussian process) providing an estimate of expected performance μ(x) and associated uncertainty σ(x) at each point in the input space,  
- an **acquisition function** (Expected Improvement) allowing balancing exploration (uncertain areas) and exploitation (promising areas),  
- an **iterative update** of the model from new observations simulated via the synthetic function:  

  ```
  f(x₁,x₂) = P_strength·exp[-(x₁-μ₁)² + (x₂-μ₂)²)/(2σ²)] + I(x₁,x₂) + ε
  ```
  where I(x₁,x₂) = -0.5·sin(3πx₁)·cos(2πx₂) captures nonlinear effects.

This approach enables **drastically reducing the number of experiments required** (10 iterations vs 100+ via exhaustive grid) while converging toward promising regions of the decision space.

**Key Results**

- **Optimal solution identified**: x₁* = 0.510, x₂* = 2.048 with strength of **93.87 MPa**  
- **Fast convergence**: stabilization after only 10 iterations  
- **Explicit quantification of uncertainty** associated with decisions (σ(x) exploited in acquisition function)  
- **Concrete illustration of value of information**: each observation reduces global uncertainty  

**Added Value**  
The project demonstrates how Bayesian stochastic optimization constitutes a robust alternative to deterministic methods when data is scarce, expensive, or noisy, while remaining **interpretable and decision-oriented**.  
Direct applications include material formulation (polymers, alloys, composites), chemical process optimization, manufacturing parameter tuning, and hyperparameter optimization in machine learning.

**Tech:** `Python` · `R` · `NumPy` · `SciPy` · `Matplotlib`

<div class="project-actions">
<a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Experimental Study</a>
</div>

</div>

:::


::: {.project-card}

<img class="project-thumb" src="../../assets/mdp-treasure2.png" alt="Optimal decision under uncertainty in risky environment">

<div class="project-body">

### P2 — Optimal Decision Under Uncertainty and Probabilistic Scenarios

**Context**  
This project falls within a **sequential decision-making under risk** framework, where action outcomes depend on probabilistic transitions and random shocks.  
The modeled scenario is that of a **treasure hunter navigating dense jungle**, who must reach an ancient temple (reward +1000) while **minimizing exposure to unfavorable scenarios**: cliffs (75% success / 25% fall, penalty -100), rivers (80% / 20%, -75), jungles (90% / 10%, -50), and negative absorbing states (Valley of Death: -150, Predators: -200).

The objective is to maximize expected value over the decision horizon, while explicitly managing time cost (-10 per action) and risk aversion.

**Methodological Approach**  
Modeling relies on explicit stochastic formulation as a **Markov Decision Process (MDP)**, integrating:

- **probabilistic scenarios** representing possible outcomes of each decision, with transitions P(s'|s,a) varying by terrain,  
- an **expected value function** V*(s) = max_a Σ_s' P(s'|s,a)[R(s,a,s') + γ·V*(s')] integrating rewards, penalties, and time cost,  
- a **discount factor** γ = 0.9 translating temporal preference and risk aversion.

Analysis includes both **exact resolution** (Value Iteration, convergence in 13 iterations) and **linear approximation of value function** via TD Learning, allowing study of tradeoffs between precision and computational complexity:
```
V(s) ≈ θ₁·f₁(s) + θ₂·f₂(s) + θ₃·f₃(s)
```
with f₁ = distance to treasure, f₂ = proximity to dangers, f₃ = safe state indicator.

**Key Results**

- **Exact optimal policy identified**: S→J1→T (optimal values V*(S)=1666.37, V*(J1)=1862.64)  
- **Highlighting of strategies** avoiding states with high expected loss (cliffs, river)  
- **Marked sensitivity** to discount factor: high γ favors risk-taking for final gain  
- **Capacity of linear approximations** (TD Learning, 1000 episodes) to capture global structure: θ₃=1799.45 (safe states valued), θ₂=1619.82 (avoid dangers), θ₁=-48.75 (minimize distance)  

**Added Value**  
This work illustrates the foundations of multi-stage stochastic optimization and highlights central issues in decision-making under uncertainty: **anticipation, prudence, and risk management**.  
Direct applications include robotic path planning, financial portfolio management (risk/return), search and rescue strategies, logistics optimization under uncertainty, and autonomous navigation.

**Tech:** `Python` · `R` · `Pandas` · `NetworkX` · `NumPy` · `Matplotlib`

<div class="project-actions">
<a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Decision Analysis</a>
</div>

</div>

:::

::: {.project-card}

<img class="project-thumb" src="../../assets/pricing-policy3.png" alt="Optimal selling policy under price uncertainty">

<div class="project-body">

### P3 — Optimal Selling Policy Under Price Uncertainty

**Context**  
**Dynamic pricing** problem in a context of stochastic uncertainty about future asset price evolution.  
A seller holds an asset that can be sold at any time. Prices fluctuate randomly according to different stochastic processes (random walk, AR(3) autoregressive model). The objective is to identify the **selling policy maximizing expected profit**, arbitrating between immediate sale (exploitation) and waiting for a better price (exploration).

**Methodological Approach**  
Three classes of parametric policies are compared via **Monte Carlo simulation** (1000–5000 realizations):

1. **"Sell-low" policy**: sell as soon as p_t ≥ θ_low (simple threshold rule)  
2. **"High-low" policy**: sell if p_t ≥ θ_high or p_t ≤ θ_low (stop-loss + take-profit)  
3. **"Tracking" policy**: sell if p_t ≥ α·p̄_t + (1-α)·θ_track (adaptive moving average)

Policies are evaluated under different **stochastic price models**:

- **Random walk**: p_{t+1} = p_t + ε, ε ~ N(0,σ²)  
- **AR(3) model**: p_{t+1} = θ₀·p_t + θ₁·p_{t-1} + θ₂·p_{t-2} + ε (captures autocorrelation)

Optimization focuses on identifying the best parameters (θ_low, θ_high, θ_track, α) for each policy and each model.

**Key Results**

- **"Tracking" policy systematically winning** across all tested scenarios:  

  • Random walk (N=5000): $45.55 (vs $45.22 for others)  
  • AR(3) with autocorrelation: $41.59 (vs $41.28–$41.59)  

- **Stable performance** across different stochastic models (robustness)  
- **Impact of autocorrelation**: profits reduced by ~10% under AR(3) ($41 vs $46)  
- **Best adaptive parameters**: α ≈ 0.7–0.9 depending on context  

**Added Value**  
The project demonstrates the importance of **adaptive policies** facing stochastic uncertainty, and illustrates the **simulation-based evaluation** methodology to compare decision strategies.  
Direct applications include algorithmic trading, perishable inventory management, dynamic pricing (hospitality, transport), online auctions, and yield management.

**Tech:** `Python` · `R` · `Monte Carlo Simulation` · `NumPy` · `Pandas` · `Matplotlib`

<div class="project-actions">
<a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Strategy Study</a>
</div>

</div>

:::

::: {.project-card}

<img class="project-thumb" src="../../assets/newsvendor4.png" alt="Newsvendor problem with salvage">

<div class="project-body">

### P4 — Optimal Inventory Management Under Demand Uncertainty

**Context**  
Classic **inventory optimization under uncertainty** problem, extended to include salvage value of unsold items (price s per unit, 0 < s < r).  
A vendor must determine the **optimal quantity to order** (x*) before observing random demand D, knowing they incur a purchase cost c per unit, sell at price r, and can salvage unsold items at price s. The objective is to **maximize expected profit** by arbitrating between shortage risk (lost opportunity) and surplus risk (storage cost / devaluation).

**Methodological Approach**  
The problem is formulated as an **analytical stochastic optimization model**:

```
max_x E[r·min(x,D) + s·max(0,x-D) - c·x]
```

Resolution via gradient calculation and first-order optimality condition leads to the **critical quantile formula**:

```
F(x*) = (c - r) / (s - r)
⟹ x* = F⁻¹[(c - r) / (s - r)]
```

Two approaches are compared:
1. **Optimal stochastic solution** (x* = 125 units for D ~ U(50,150), c=10, r=25, s=5)  
2. **Deterministic solution** with demand fixed at E[D] = 100 (x*_det = 100 units)  

The gap between the two quantifies the **Value of the Stochastic Solution (VSS)**: VSS = Π(x*) - Π(x*_det) = 1100 - 1500 = -$400.

A **stochastic gradient algorithm** is also implemented to empirically validate convergence toward x* ≈ 124 units.

**Key Results**

- **Analytical optimal solution**: x* = 125 units (expected profit $1100)  
- **VSS = -$400**: stochastic solution presents loss risk compared to deterministic demand fixed at 100, illustrating the **importance of forecast quality**  
- **Empirical convergence** of stochastic gradient toward x* ≈ 124 units  
- **Insensitivity to distribution bounds**: formula remains valid for D ∈ [a,b] with truncated distribution  

**Added Value**  
This project illustrates fundamental concepts of stochastic optimization: **cost/benefit balance under uncertainty**, **value of perfect information**, and **sensitivity analysis** to parameters (c, r, s, distribution of D).  
Direct applications include inventory management (retail, manufacturing), capacity planning, and any decision situation before observing random demand.

**Tech:** `Python` · `R` · `NumPy` · `Matplotlib`

<div class="project-actions">
<a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Theoretical Analysis</a>
</div>

</div>

:::

:::











## Reinforcement Learning {#rl}

Application of **reinforcement learning** to a **sequential control and combinatorial optimization** problem, in a simulated and constrained environment, with complete implementation in **native Python** and standard scientific ecosystem libraries.

::: {cards-grid}

::: {.project-card}

<img class="project-thumb" src="../../assets/rlrocky1.png" alt="Bayesian optimization for experimental systems">


<div class="project-body">

### Reinforcement Learning for Autonomous Robot Navigation (Codey Rocky)

**Context**  
This project addresses a variant of the **Traveling Salesman Problem (TSP)** in a discrete environment, formulated as a **sequential decision problem under constraints**.  
An autonomous robot (Codey Rocky) must:

- visit a set of colored items **exactly once**,  
- **minimize total distance traveled**,  
- then **return to starting point**,  
all within a deliberately limited state space to make learning tractable.

The problem is **NP-hard**, with rapid combinatorial explosion of possible trajectories as the number of targets increases.

**Methodological Approach**  
The environment is modeled as a **Markov Decision Process (MDP)** comprising:

- a discrete state space (128 states) although in reality, the space should normally be continuous (white zones on the map),
- a restricted set of actions (forward, rotations),
- a reward function combining objectives and penalties.

Two approaches are implemented and compared:

- **Tabular Q-learning**  
  Learning via direct Q-table update, serving as an interpretable reference.

- **Deep Q-learning (DQN)**  
  Value function approximation via a dense neural network, trained by backpropagation.

Policies are trained using an **ε-greedy** strategy, with explicit control of learning rate and discount factor.

**Key Results**

- Convergence of both approaches after comparable number of episodes  
- Similar final performance in moderate state space  
- Empirical validation of deep RL limitations when structural complexity remains low  

**Critical Analysis**  
The project highlights that:

- deep learning does not systematically bring significant gain versus well-posed tabular methods,
- state space structure is determinant to justify use of neural networks,
- stability and interpretability remain key criteria in constrained environments.

**Added Value**  
This work illustrates a **rigorous implementation of reinforcement learning**, from environment modeling to comparative algorithm analysis, highlighting the **real conditions for deep RL relevance**.

**Tech:** `Python` · `PyTorch` · `TensorFlow` · `NumPy` · `Pandas` · `Matplotlib`  ·  `Robotics`

<div class="project-links">
<a class="btn btn-sm btn-primary" href="https://youtu.be/R0ZM-iSKcVE" target="_blank">Démonstration vidéo</a>


</div>

:::

:::

## Data Engineering & MLOps {#dataeng}

Design and implementation of **data and AI model pipelines fully deployed in production**, with particular attention to **cloud scalability**, **experimental reproducibility**, and integration of **advanced paradigms like federated learning**.

::: {cards-grid}

::: {.project-card}

<img class="project-thumb" src="../../assets/ecoenergy1.png" alt="AWS MLOps platform and federated learning">

<div class="project-body">

### P1 — Cloud-Native MLOps Platform for Smart Energy Monitoring

**Context**  
End-to-end project aimed at designing an **industrial MLOps platform fully deployed on AWS cloud**, for analysis and forecasting of energy consumption of distributed systems.  
The operational framework imposes strong constraints: **heterogeneous data**, **multi-site deployment**, **data security**, and **need for real-time predictions**.

The objective is to cover the entire **lifecycle of a production AI application**, from data ingestion to model serving, while integrating a **federated learning** approach to preserve local data confidentiality.

**Cloud Architecture & Deployment**  
The application is deployed **end-to-end on AWS**, with a service-oriented architecture:

- **Storage & Data**
  - Historical and intermediate data stored on **Amazon S3**
  - Clear separation between raw data, features, and model artifacts  

- **Compute & Orchestration**
  - Containerized services orchestrated via **AWS Kubernetes (EKS)**  
  - Distributed training and inference, with GPU support  
  - Automatic horizontal scalability according to load  

- **Serving & Integration**
  - Models exposed via **API endpoints**  
  - Asynchronous triggers via **AWS Lambda** for inference and updates  
  - Architecture ready for real-time and batch use cases  

**MLOps & Experimentation**
- Experiment, metrics, and model version tracking  
- Status management (experimentation, validation, production)  
- Strict separation between data, training, and deployment pipelines  

**Federated Learning (Key Element)**  
A **federated learning** mechanism was implemented to:

- Train models **without centralizing sensitive data**  
- Aggregate weights learned locally on different nodes  
- Reduce risks related to data confidentiality and governance  
- Simulate multi-site industrial scenarios (buildings, regions, equipment)

This approach constitutes a **innovative lever** for energy and industrial systems, where regulatory and operational constraints limit data centralization.

**Key Results**
- AI application **fully deployed in production on AWS**  
- Functional MLOps pipeline from storage to prediction API  
- Validation of operation in distributed environment  
- Successful integration of federated learning in cloud architecture  

**Added Value**  
This project demonstrates **advanced mastery of Data Engineering and cloud-native MLOps**, as well as ability to integrate **recent research paradigms (federated learning)** into realistic industrial architectures.  
It illustrates the complete transition **from modeling to industrialization**, with a system and long-term vision.

**Tech:** `AWS (S3, Lambda, EKS, Endpoints)` · `Fast Api` · `Nginx` · `Data Engineering` · `MLOps` · `Docker` · `Kubernetes` · `GPU Computing` · `Federated Learning` 

<div class="project-actions">
<a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Cloud & MLOps Platform</a>
</div>

</div>

::: 
:::





## Natural Language Processing (NLP) & LLM {#llm}

Advanced analysis of **large-scale text corpora** to extract **latent semantic structures**, compare international media discourses, and understand the evolution of narratives over time.

::: {.cards-grid}

::: {.project-card}

<img class="project-thumb" src="../../assets/nlp1.png" alt="Text analysis and Topic Modeling on COVID-19">

<div class="project-body">

### P1 — Comparative Analysis of International Media Discourses (COVID-19)

**Context**  
Large-scale academic project conducted at HEC Montreal aimed at analyzing and comparing **English-language media discourses** from several countries and continents around a common theme: the **COVID-19 pandemic**.  
The objective is to identify **how the same global event is treated differently according to geopolitical, cultural, and temporal contexts**, beyond simple factual content.

**Methodological Approach**  
The approach relies on a complete **natural language processing** pipeline, combining:

- **in-depth exploratory analysis** of texts (length, temporal distribution, geographic origin),  
- **rigorous linguistic preprocessing** (cleaning, entity normalization, lexical variant management, noise reduction),  
- **unsupervised topic modeling** allowing extraction of dominant subjects without a priori assumptions.

Analysis is structured **by key time periods** (beginning, peak, and late phase of the pandemic), to study the evolution of media narratives over time.

**Key Results**
- Highlighting of **distinct themes according to pandemic phases** (initial reaction, public policies, socio-economic impacts, variants, easing of measures)  
- Marked differences between regions in **topic prioritization** (public health, governance, economy, society)  
- Topic Modeling's ability to reveal **latent discursive dynamics**, difficult to observe by manual reading  

**Challenges and Lessons**
- Strong heterogeneity of sources (editorial styles, article sizes, vocabulary)  
- Critical importance of **preprocessing and linguistic normalization** to obtain exploitable results  
- Necessity of constant arbitration between **semantic richness** and **statistical stability** of models  

**Added Value**  
This project demonstrates **advanced mastery of modern NLP techniques**, with ability to transform unstructured texts into **interpretable analytical indicators**, useful for:
- media analysis,  
- strategic intelligence,  
- social and decision sciences research.

It also constitutes a solid foundation for extensions toward **sentiment analysis**, **large language models (LLMs)**, and fine study of **multi-country narrative dynamics**.

**Tech:** NLP · Topic Modeling · Text Mining · SpaCy · Gensim · NLTK · Transformers · Semantic Visualization

<div class="project-actions">
<a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Session Project</a>
</div>

</div>

:::

:::



## Machine Learning & Deep Learning & Computer Vision {#dl-cv}




::: {.project-card}

<img class="project-thumb" src="../../assets/airline1.png" alt="Airline reliability prediction">

<div class="project-body">

### P1 — Airline Reliability Prediction for Commercial Routes

**Context**  
Predictive modeling project aimed at estimating **airline reliability for given routes**, based on vast history of North American commercial flights.  
Reliability is operationally defined from criteria combining **delays**, **cancellations**, and **temporal variability**, in a context where user decisions are highly sensitive to uncertainty.

The objective is to provide **robust decision support**, enabling comparison of airlines according to **risk associated with a specific route**, taking into account spatio-temporal context.

**Modeling Approach**  
The approach relies on structured supervised modeling:

- Formalization of the problem as **reliability class prediction** (low, medium, high)  
- Integration of temporal, geographic, and operational variables  
- Comparison of several classification model families  
- Analysis of tradeoffs between **global accuracy**, **stability**, and **interpretability**  
- Rigorous evaluation in a framework respecting data temporal structure  

Particular attention is paid to **class imbalance** and its impact on prediction quality.

**Key Results**

- Ability to clearly differentiate reliability levels by airlines and routes  
- Highlighting of dominant temporal and geographic factors  
- Significant improvement over naive ranking approaches  
- Stable and consistent models on out-of-sample periods  

**Added Value**  
This project illustrates a **concrete application of large-scale predictive modeling**, with strong connection between **statistics**, **machine learning**, and **user decision-making**.  
It highlights the ability to transform massive data into **exploitable reliability indicators**.

**Tech:** `Supervised Learning` · `Classification` · `Predictive Modeling` · `Imbalance Evaluation` · `Python` · `Scikit-learn` · `XGBoost` · `Pandas` · `Matplotlib`

</div>

:::



::: {.project-card}

<img class="project-thumb" src="../../assets/montreal2.png" alt="Accident risk at Montreal intersections">

<div class="project-body">

### P2 — Spatial Modeling of Accident Risk at Montreal Intersections

**Context**  
Project applied to **urban road safety**, aimed at modeling and classifying **accident risk level at intersections in Montreal city**.  
Road accidents present **strong spatial dependence**: nearby intersections often share common characteristics related to urban environment, traffic, and infrastructure.

The objective is to produce a **robust ranking of intersections by dangerousness**, to support development and prevention decisions.

**Modeling Approach**  
The approach combines statistical modeling and spatial dependence:

- Modeling of accident numbers via **adapted count models**  
- Introduction of **spatial lag variables**  
- Construction of neighborhood structures (distance, radius, k-nearest neighbors)  
- Model comparison:
  - classical count models,
  - random effects models,
  - spatial models explicitly integrating geographic dependence  

Evaluation relies on quantitative metrics and on **intersection ranking stability**.

**Key Results**

- Clear performance improvement through spatial dependence integration  
- Identification of high-risk urban zones  
- Consistent and interpretable intersection ranking  
- Results directly exploitable for urban planning  

**Added Value**  
This project demonstrates **advanced mastery of applied spatial modeling**, at the interface between **machine learning**, **statistics**, and **urban data**.  
It highlights the added value of structural models for problems with **strong societal impact**.

**Tech:** `Spatial Modeling` · `Count Statistics` · `Statistical Learning` · `Geographic Data` 

</div>

:::


::: {.project-card}

<img class="project-thumb" src="../../assets/bixi3.jpg" alt="Analysis of BIXI bike solicitation in Montreal">

<div class="project-body">

### P3 — Modeling BIXI Network Solicitation in Montreal

**Context**  
Project applied to **urban mobility**, aimed at analyzing and modeling **BIXI trip duration and intensity** in Montreal, based on data from multiple stations distributed across the urban territory.  
Observations present a **natural hierarchical structure**: trips are nested within stations, themselves located in different boroughs, inducing **significant intra-station correlations**.

The objective is to understand **how network solicitation varies according to spatial and temporal context**, and to quantify heterogeneity between stations.

**Modeling Approach**  
The approach relies on progressive statistical modeling:

- Base modeling via a **global linear model**, integrating temporal covariates (weekday vs weekend)  
- Highlighting limitations of observation independence assumption  
- Introduction of **mixed effects models**, with:
  - **random intercepts by station**,  
  - then **conditional random effects for weekends**,  
- Formal statistical tests (likelihood ratios, Wald tests) to evaluate:
  - significance of random effects,  
  - inter-station variability,  
  - relevance of hierarchical structures.

Particular attention is paid to estimation and interpretation of **intra-station correlation**, key indicator of trip dependence.

**Key Results**
- Highlighting of **high intra-station correlation**, invalidating independence assumption  
- Significant variability in average trip duration between boroughs  
- Positive global effect of weekend on trip duration  
- Low inter-station variability of "weekend" effect, justifying a more parsimonious model  

**Added Value**  
This project demonstrates **advanced mastery of hierarchical models and mixed effects**, essential for analyzing correlated urban data.  
It highlights the importance of **adapted structural modeling** to avoid biased conclusions in shared mobility systems.

**Tech:** `Linear Mixed Models` · `Advanced Statistics` · `Urban Mobility Data` · `Hierarchical Analysis` · `R` · `lme4` · `ggplot2`

</div>

:::




:::



<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Projects – Georges Lemuel Balogog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e31584831b205ffbb2d98406f31c2a5b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    const path = window.location.pathname;

    const frUrl = path.replace("/portfolio/en/", "/portfolio/fr/");
    const enUrl = path; // already EN

    const links = Array.from(document.querySelectorAll(".navbar a.nav-link"));
    const frLink = links.find(a => a.textContent.trim() === "FR");
    const enLink = links.find(a => a.textContent.trim() === "EN");

    if (frLink) frLink.setAttribute("href", frUrl);
    if (enLink) enLink.setAttribute("href", enUrl);
  });
</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Georges Lemuel Balogog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-blog" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Blog</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-blog">    
        <li>
    <a class="dropdown-item" href="./blog/index.html">
 <span class="dropdown-text">All posts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./blog/2026-ia-illusion-thinking.html">
 <span class="dropdown-text">IA Illusion of Thought</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./blog/2026-ia-optimization-limits.html">
 <span class="dropdown-text">IA Limits of Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./blog/feasibility-over-optimality.html">
 <span class="dropdown-text">Why Feasibility Matters More Than Optimality</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./cv.html"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./contact.html"> 
<span class="menu-text">Contact</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="./#"> 
<span class="menu-text">FR</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./#"> 
<span class="menu-text">EN</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/MBalogogGLemuel"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/balogog-georges-6810b9118/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:georges.balogog@yahoo.fr"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Projects</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<ul>
<li><a href="#Telecom">Electrical Engineering &amp; Telecom</a></li>
<li><a href="#forecasting">Forecasting</a></li>
<li><a href="#optimisation">Deterministic Optimization</a></li>
<li><a href="#stochastic">Stochastic Optimization</a></li>
<li><a href="#rl">Reinforcement Learning</a></li>
<li><a href="#dataeng">Data Engineering &amp; MLOps</a></li>
<li><a href="#llm">Natural Language Processing &amp; LLM</a></li>
<li><a href="#dl-cv">Deep Learning &amp; Computer Vision</a></li>
</ul>
<div class="justify">
<p>This page presents my technical projects organized by <strong>area of specialization</strong>.</p>
<p>Each section features <strong>selected projects</strong> with their <strong>context</strong>, <strong>methodological approach</strong>, <strong>results</strong>, and <strong>links to code</strong> when available.</p>
<hr>
<section id="Telecom" class="level2">
<h2 class="anchored" data-anchor-id="Telecom">Telecom &amp; Electrical Engineering</h2>
<div class="cards-grid">
<div class="project-card">
<p><img class="project-thumb" src="../../assets/monopol3d.PNG" alt="Antenna array optimization using ANN"></p>
<section id="p1---beamforming-optimization-of-a-planar-antenna-array-using-artificial-neural-networks-ann" class="level3 project-body">
<h3 class="anchored">P1 - Beamforming Optimization of a Planar Antenna Array using Artificial Neural Networks (ANN)</h3>
<p><strong>Context.</strong><br>
Design of a smart planar antenna (4×5 monopole array) and training of a <strong>multilayer perceptron (MLP)</strong> to predict the <strong>feeding law (amplitudes/phases)</strong> to form a beam in a target direction.</p>
<p><strong>Approach.</strong> - Analytical baseline: <strong>Dolph–Chebyshev</strong> (sidelobe control). - ML model: <strong>ANN / MLP with backpropagation</strong>, <strong>MSE criterion</strong>. - Evaluation chain: <strong>MATLAB (NNSTART) → CST Microwave Studio</strong> (co-simulation and control).</p>
<p><strong>Key Results.</strong> - <strong>Directivity gain ≈ +0.2 dB</strong> compared to the Dolph–Chebyshev method. - <strong>Computation time ≈ 1.27 s</strong> for feeding law generation. - Network training time: <strong>≈ 27 h 59 min 58 s</strong> (backpropagation).</p>
<p><strong>Technologies.</strong><br>
MATLAB R2018a · Neural Network Toolbox (NNSTART) · CST Microwave Studio · Array Antennas · Beamforming · Heuristic Optimization · MLP</p>
<p><strong>Deliverables.</strong> - Detailed technical report (Scientific article) - MATLAB scripts (training and inference) - Electromagnetic models and simulations in CST - 2D and 3D radiation patterns</p>
<div class="project-links">
<p><a href="../../assets/OptimreseauxantennesRNA/Synthese-Optim_reseaux_antennes_planaires.pdf" class="btn btn-primary">Read report (PDF)</a> <a href="https://github.com/MBalogogGLemuel/Optimisation-reseaux-antenne-RNA" class="btn btn-outline">GitHub source code</a></p>
</div>
<p>:::</p>
<div class="project-card">
<p><img class="project-thumb" src="../../assets/_site/assets/Detection_par_finger_printing/images/point_mesure.png" alt="Wi-Fi fingerprinting detection"></p>
<div class="project-body">
<section id="p2---indoor-wi-fi-localization-via-rssi-fingerprinting" class="level3">
<h3 class="anchored" data-anchor-id="p2---indoor-wi-fi-localization-via-rssi-fingerprinting">P2 - Indoor Wi-Fi Localization via RSSI Fingerprinting</h3>
<p><strong>Machine Learning (ANN/MLP) vs PSO Comparison</strong></p>
<p><strong>Summary.</strong><br>
Indoor localization prototype in a <strong>10 m × 10 m area</strong> with <strong>4 Wi-Fi access points</strong>.<br>
I formulated the problem as <strong>supervised regression</strong>: ((RSSI_1,,RSSI_4)(x,y)), then compared a <strong>neural network (MLP)</strong> to <strong>PSO</strong> in accuracy and latency.</p>
<p><strong>Objective.</strong> - Estimate position <strong>(x, y)</strong> from Wi-Fi RSSI (fingerprinting) - Optimize the <strong>accuracy / computation time</strong> tradeoff (real-time telecom constraint)</p>
<p><strong>Methodology.</strong> - <strong>Experimental setup</strong>: <strong>100 m²</strong> area, <strong>2.4 GHz</strong>, 4 APs at corners - <strong>Dataset</strong>: <strong>50 reference points</strong> (fingerprinting database) - <strong>ML Model</strong>: <strong>MLP (ANN)</strong>, regression, <strong>MSE</strong> loss - <strong>Baseline</strong>: <strong>PSO</strong> (60 particles, (c_1=c_2=2), (w), 1000 iterations) - <strong>Condition</strong>: LOS (controlled protocol)</p>
<p><strong>Key Results.</strong> - <strong>ANN (MLP)</strong>: <strong>2.5729 m</strong> mean error | <strong>0.206288 s</strong> - <strong>PSO</strong>: <strong>2.8612 m</strong> mean error | <strong>1.360301 s</strong> - <strong>ANN Gain</strong>: <strong>≈ +11%</strong> more accurate | <strong>≈ ×6.6</strong> faster (inference)</p>
<p><strong>Why this is strong for a recruiter (Telecom × Data).</strong> - Reformulation of a radio problem into an <strong>exploitable ML pipeline</strong> - Rigorous analysis of <strong>latency vs accuracy</strong> tradeoffs - Structured comparison <strong>ML vs heuristic optimization</strong> - Directly transferable approach to <strong>IoT / smart buildings / radio analytics</strong></p>
<p><strong>Stack &amp; Skills.</strong> - Wi-Fi Fingerprinting (RSSI), radio propagation (FSPL/Friis) - Supervised regression, MLP, MSE, evaluation - PSO (tuning, convergence, computational cost) - MATLAB + LaTeX reporting</p>
<div class="project-links">
<p><a href="../../assets/Detectionparfingerprinting/Synthese-Detection_par_fingerprinting.pdf" class="btn btn-primary">Report (PDF)</a> <a href="https://github.com/MBalogogGLemuel/Detection-par-fingerprinting" class="btn btn-outline">GitHub</a></p>
</div>
<p>:::</p>
<p>:::</p>
</section>
<section id="forecasting" class="level2">
<h2 class="anchored" data-anchor-id="forecasting">Forecasting</h2>
<p>Development of <strong>time series forecasting systems applied to real energy systems</strong>, with particular emphasis on <strong>statistical robustness</strong>, <strong>incorporation of exogenous variables</strong>, and <strong>rigorous comparison between classical and deep learning approaches</strong>.</p>
<div class="{cards-grid}">
<div class="project-card">
<p><img class="project-thumb" src="../../assets/jcplforecast.png" alt="JCPL energy forecasting"></p>
<section id="p1-regional-energy-consumption-forecasting-statistical-approach" class="level3 project-body">
<h3 class="anchored" data-anchor-id="p1-regional-energy-consumption-forecasting-statistical-approach">P1 — Regional Energy Consumption Forecasting (Statistical Approach)</h3>
<p><strong>Context</strong><br>
<strong>Daily energy consumption forecasting</strong> project (peak load) for regions served by <strong>JCP&amp;L (New Jersey)</strong>, in a <strong>medium-term energy planning</strong> context.<br>
The objective is to anticipate demand fluctuations while maintaining <strong>high interpretability</strong> of explanatory mechanisms.</p>
<p><strong>Methodological Approach</strong><br>
The approach is based on classical time series modeling (Naive, ETS, SARIMAX) explicitly integrating:</p>
<ul>
<li><strong>energy seasonality</strong>,<br>
</li>
<li><strong>long-term trends</strong>,<br>
</li>
<li><strong>exogenous climate and socio-demographic variables</strong>.</li>
</ul>
<p>Models are evaluated in a strict <strong>rolling time validation</strong> framework, ensuring realistic out-of-sample performance estimation.</p>
<p><strong>Key Results</strong> - Stable and consistent regional-scale forecasts<br>
- Clear identification of seasonal components and exogenous effects<br>
- Construction of a <strong>reliable operational baseline</strong></p>
<p><strong>Added Value</strong><br>
This project illustrates the value of well-specified statistical models for <strong>energy decision-making</strong> and <strong>operational planning</strong>, when transparency and traceability of assumptions are essential.</p>
<p><strong>Tech</strong>: <code>R</code> · <code>Statistical Forecasting</code> · <code>Time Series</code> · <code>Power BI</code> · <code>Tableau</code></p>
<div class="project-actions">
<p><a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Case Study</a></p>
</div>
</section>
</div>
<div class="project-card">
<p><img class="project-thumb" src="../../assets/mrcforecast.png" alt="Multi-region Quebec energy forecasting via deep learning"></p>
<section id="p2-multivariate-energy-forecasting-via-deep-learning" class="level3 project-body">
<h3 class="anchored" data-anchor-id="p2-multivariate-energy-forecasting-via-deep-learning">P2 — Multivariate Energy Forecasting via Deep Learning</h3>
<p><strong>Context</strong><br>
Extension of the previous project toward a <strong>data-driven approach</strong>, aiming to capture <strong>complex nonlinear relationships</strong> between energy consumption, weather conditions, and regional dynamics.<br>
The framework is <strong>multi-regional forecasting</strong>, at the scale of Quebec’s administrative regions, characterized by heterogeneous behaviors and partially non-stationary series.</p>
<p><strong>Methodological Approach</strong><br>
Modeling relies on sequential architectures (RNN-LSTM, TCN), systematically compared to traditional statistical models (SARIMA), to:</p>
<ul>
<li>exploit <strong>sliding time windows</strong>,<br>
</li>
<li>integrate <strong>stabilizing transformations</strong> of the target variable,<br>
</li>
<li>automatically learn dynamic dependencies between variables.</li>
</ul>
<p>Particular attention is paid to <strong>comparative evaluation</strong> of approaches to measure the real gain brought by deep learning.</p>
<p><strong>Key Results</strong> - Performance improvement in certain high-variability regions<br>
- Better capture of delayed effects and nonlinearities<br>
- Robust behavior on recent out-of-sample periods<br>
- Confirmation of the <strong>persistent relevance of traditional statistical models</strong></p>
<p><strong>Added Value</strong><br>
This work highlights the <strong>strengths and limitations of deep learning applied to energy forecasting</strong>, and proposes a pragmatic framework to arbitrate between <strong>predictive performance</strong>, <strong>temporal stability</strong>, and <strong>operational complexity</strong>.</p>
<p><strong>Tech</strong>: <code>Python</code> · <code>Pytorch</code> · <code>TensorFlow</code> · <code>Scikit-learn</code> · <code>Pandas</code> · <code>NumPy</code> · <code>Matplotlib</code> · <code>Power BI</code></p>
<div class="project-actions">
<p><a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Comparative Study</a></p>
</div>
</section>
</div>
</div>
</section>
<section id="optimisation" class="level2">
<h2 class="anchored" data-anchor-id="optimisation">Deterministic Optimization</h2>
<p>Design and analysis of <strong>deterministic optimization models</strong> for real problems of planning, resource allocation, and project management, with particular emphasis on <strong>translating operational constraints into exploitable mathematical formulations</strong> and on <strong>analyzing cost–time–resource tradeoffs</strong>.</p>
<div class="{cards-grid}">
<div class="project-card">
<p><img class="project-thumb" src="../../assets/warehouse-optim4.png" alt="Deterministic optimization of an industrial logistics warehouse"></p>
<section id="p1-integrated-deterministic-optimization-of-a-logistics-warehouse-milp" class="level3 project-body">
<h3 class="anchored" data-anchor-id="p1-integrated-deterministic-optimization-of-a-logistics-warehouse-milp">P1 — Integrated Deterministic Optimization of a Logistics Warehouse (MILP)</h3>
<p><strong>Context</strong><br>
Recent industrial project aiming at operational optimization of a multi-zone, multi-format, multi-level logistics warehouse, in a real context of <strong>high flow variability</strong>, <strong>strict physical constraints</strong>, and <strong>daily operational pressure</strong>.<br>
The project is conducted in a confidential framework; the elements presented here are deliberately <strong>abstract and anonymized</strong>.</p>
<p>The overall objective is to simultaneously improve:</p>
<ul>
<li>the efficiency of <strong>stock placement</strong>,<br>
</li>
<li><strong>order picking</strong> performance,<br>
</li>
<li>and the consistency of <strong>internal replenishments</strong>,<br>
while maintaining <strong>day-to-day operational feasibility</strong>.</li>
</ul>
<p><strong>Methodological Approach</strong><br>
The problem is formulated as a <strong>sequential deterministic optimization</strong> structured around three complementary sub-models:</p>
<ul>
<li><strong>Placement (Putaway / Slotting)</strong>: format activation decisions per location and allocation of incoming arrivals, under capacity, compatibility, and functional proximity constraints.</li>
<li><strong>Order Picking</strong>: optimal allocation of pickings to maximize service while minimizing internal movements.</li>
<li><strong>Internal Replenishment</strong>: controlled stock relocation via an admissible subset of arcs, to limit combinatorics and movement costs.</li>
</ul>
<p>These models are <strong>orchestrated sequentially at daily scale</strong>, with consistent propagation of inventory states between steps.</p>
<p><strong>Evaluation and Performance Indicators</strong><br>
An indicator engine allows evaluating the impact of decisions before and after each step, notably:</p>
<ul>
<li>weighted average distance to central point,</li>
<li>occupancy rate and stock dispersion,</li>
<li>co-location penalties (similarity),</li>
<li>volume and cost of internal movements,</li>
<li>layout stability (churn).</li>
</ul>
<p>This approach enables a <strong>clear quantitative reading of operational tradeoffs</strong> induced by each decision.</p>
<p><strong>Key Results</strong></p>
<ul>
<li>Measurable improvement in accessibility of high-demand items<br>
</li>
<li>Reduction of unnecessary internal movements<br>
</li>
<li>Better layout stability under dynamic flows<br>
</li>
<li>Reproducible framework to test realistic operational scenarios</li>
</ul>
<p><strong>Added Value</strong><br>
This project illustrates a complete implementation of <strong>deterministic optimization applied to a real logistics system</strong>, combining mathematical modeling, decision orchestration, and performance analysis.<br>
It constitutes a solid foundation for future extensions toward <strong>stochastic</strong> approaches or <strong>reinforcement learning</strong>, while remaining operationally exploitable.</p>
<p><strong>Tech</strong>: <code>Python</code> · <code>GulP/Gurobi</code> · <code>Plotly</code> · <code>NetworkX</code> · <code>Pandas</code> · <code>NumPy</code> · <code>Matplotlib</code> · <code>Power BI</code></p>
<div class="project-actions">
<p><a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Industrial Study</a></p>
</div>
</section>
</div>
<div class="project-card">
<p><img class="project-thumb" src="../../assets/optim-project1.png" alt="Scheduling and acceleration of software project"></p>
<section id="p2-scheduling-and-acceleration-of-a-complex-software-project" class="level3 project-body">
<h3 class="anchored" data-anchor-id="p2-scheduling-and-acceleration-of-a-complex-software-project">P2 — Scheduling and Acceleration of a Complex Software Project</h3>
<p><strong>Context</strong><br>
Planning problem for software development composed of several interdependent sub-projects, with strict precedence constraints.<br>
The initial objective is to <strong>minimize total project duration</strong>, then explore <strong>acceleration strategies under budget constraints</strong>, integrating partial or total outsourcing decisions.</p>
<p><strong>Methodological Approach</strong><br>
The problem is formulated as a <strong>deterministic scheduling model</strong>, progressively enriched by: - <strong>explicit precedence constraints</strong> between tasks, - <strong>binary decision variables</strong> to represent outsourcing, - <strong>global deadline constraints</strong> imposed by the client, - linear mechanisms to model <strong>non-uniform costs</strong>, <strong>conditional discounts</strong>, and <strong>partial delivery thresholds</strong> (e.g., 75% of tasks delivered before target date).</p>
<p>Each model adjustment aims to preserve <strong>linearity</strong>, while increasing decision realism.</p>
<p><strong>Key Results</strong> - Significant reduction in delivery time under cost constraints<br>
- Identification of critical tasks to outsource as priority<br>
- Fine analysis of tradeoffs between <strong>acceleration cost</strong> and <strong>deadline compliance</strong></p>
<p><strong>Added Value</strong><br>
This project illustrates a complete <strong>incremental modeling</strong> approach, typical of real industrial contexts, where requirements evolve and require robust, explainable, and adaptable models.</p>
<p><strong>Tech</strong>: <code>Excel Solver</code> · <code>Gams</code> · <code>IBM Cplex</code> · <code>PowerBi</code></p>
<div class="project-actions">
<p><a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Case Study</a></p>
</div>
</section>
</div>
<div class="project-card">
<p><img class="project-thumb" src="../../assets/optim-locomotive2.png" alt="Dynamic locomotive fleet allocation"></p>
<section id="p3-dynamic-multi-site-locomotive-fleet-allocation" class="level3 project-body">
<h3 class="anchored" data-anchor-id="p3-dynamic-multi-site-locomotive-fleet-allocation">P3 — Dynamic Multi-Site Locomotive Fleet Allocation</h3>
<p><strong>Context</strong><br>
Fleet management problem for locomotives circulating between several interconnected sites (A–B–C), with imposed departure and arrival schedules and <strong>minimum capacity requirements per trip</strong>.<br>
The challenge is to <strong>minimize the total number of locomotives needed</strong>, while ensuring operational network feasibility.</p>
<p><strong>Methodological Approach</strong><br>
The system is modeled as a <strong>multi-period deterministic allocation</strong> problem, integrating: - <strong>minimum capacity constraints per trip</strong>, - <strong>flow balance equations</strong> ensuring consistency of availability per site, - <strong>cyclic conditions</strong> ensuring fleet stability over the time horizon.</p>
<p>The model explicitly captures the temporal dynamics of resources and interdependencies between local and global decisions.</p>
<p><strong>Key Results</strong> - Determination of <strong>minimum fleet required</strong> to satisfy all trips<br>
- Clear visualization of flows and resource tensions<br>
- Highlighting of critical periods in terms of availability</p>
<p><strong>Added Value</strong><br>
This work highlights the power of deterministic models for <strong>logistics network planning</strong>, when priority is given to robustness and feasibility rather than stochasticity.</p>
<div class="project-actions">
<p><a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Operational Analysis</a></p>
</div>
<p><strong>Tech</strong>: <code>Python</code> · <code>Gams</code> · <code>IBM Cplex</code> · <code>PowerBi</code> · <code>Excel Solver</code></p>
</section>
</div>
<div class="project-card">
<p><img class="project-thumb" src="../../assets/optim-maintenance3.png" alt="Industrial maintenance planning"></p>
<section id="p4-optimal-maintenance-planning-for-power-plants" class="level3 project-body">
<h3 class="anchored" data-anchor-id="p4-optimal-maintenance-planning-for-power-plants">P4 — Optimal Maintenance Planning for Power Plants</h3>
<p><strong>Context</strong><br>
Maintenance planning problem for several power plants over a multi-month horizon, under <strong>team availability constraints</strong>, <strong>authorized launch windows</strong>, and <strong>calendar-dependent costs</strong>.<br>
Two objectives are studied: <strong>total cost minimization</strong> and <strong>earliest completion of all maintenance</strong>, regardless of budget.</p>
<p><strong>Methodological Approach</strong><br>
The problem is formulated as a <strong>linear optimization model in binary variables</strong>, integrating: - uniqueness constraints on start month per plant, - global personnel capacity constraints per period, - temporal relationships linking start month to completion date, - adjustments imposing <strong>ordering or synchronization relationships</strong> between certain plants.</p>
<p>This approach allows rapid testing of several strategic scenarios.</p>
<p><strong>Key Results</strong> - Measured reduction in maintenance costs under realistic constraints<br>
- Clear comparison between <strong>cost-minimal</strong> and <strong>time-minimal</strong> strategies<br>
- Identification of human resource bottlenecks</p>
<p><strong>Added Value</strong><br>
This project demonstrates how deterministic optimization can serve as a strategic decision support tool in critical industrial contexts, where resources are limited and operational consequences major.</p>
<p><strong>Tech</strong>: <code>Excel Solver</code> · <code>Gams</code> · <code>IBM Cplex</code> · <code>PowerBi</code></p>
<div class="project-actions">
<p><a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Decision Study</a></p>
</div>
</section>
</div>
</div>
</section>
<section id="stochastic" class="level2">
<h2 class="anchored" data-anchor-id="stochastic">Stochastic Optimization</h2>
<p>Design of <strong>optimization models under uncertainty</strong> aimed at supporting decision-making when key parameters (demand, performance, environment) are <strong>neither deterministic nor perfectly observable</strong>.<br>
The projects below emphasize <strong>probabilistic modeling</strong>, <strong>risk–performance tradeoff</strong>, and <strong>value of information</strong>.</p>
<div class="cards-grid">
<div class="project-card">
<p><img class="project-thumb" src="../../assets/bayesian-optim1.png" alt="Bayesian optimization for experimental systems"></p>
<section id="p1-bayesian-optimization-for-expensive-experimental-systems" class="level3 project-body">
<h3 class="anchored" data-anchor-id="p1-bayesian-optimization-for-expensive-experimental-systems">P1 — Bayesian Optimization for Expensive Experimental Systems</h3>
<p><strong>Context</strong><br>
This project addresses an optimization problem where the objective function is <strong>unknown</strong>, <strong>noisy</strong>, and <strong>expensive to evaluate</strong>, typical of industrial or experimental environments.<br>
The case study focuses on optimizing the <strong>mechanical strength of a polymer</strong>, depending on continuous physico-chemical parameters (plasticizer proportion x₁ ∈ [0,1] and curing time x₂ ∈ [0,5] hours), for which each evaluation represents a real cost (time, resources, laboratory tests).</p>
<p>The objective is to identify the optimal formulation while <strong>minimizing the number of experiments required</strong>, while explicitly managing the uncertainty inherent in measurements.</p>
<p><strong>Methodological Approach</strong><br>
The strategy relies on <strong>sequential Bayesian optimization</strong>, structured around:</p>
<ul>
<li><p>a <strong>probabilistic surrogate model</strong> (Gaussian process) providing an estimate of expected performance μ(x) and associated uncertainty σ(x) at each point in the input space,<br>
</p></li>
<li><p>an <strong>acquisition function</strong> (Expected Improvement) allowing balancing exploration (uncertain areas) and exploitation (promising areas),<br>
</p></li>
<li><p>an <strong>iterative update</strong> of the model from new observations simulated via the synthetic function:</p>
<pre><code>f(x₁,x₂) = P_strength·exp[-(x₁-μ₁)² + (x₂-μ₂)²)/(2σ²)] + I(x₁,x₂) + ε</code></pre>
<p>where I(x₁,x₂) = -0.5·sin(3πx₁)·cos(2πx₂) captures nonlinear effects.</p></li>
</ul>
<p>This approach enables <strong>drastically reducing the number of experiments required</strong> (10 iterations vs 100+ via exhaustive grid) while converging toward promising regions of the decision space.</p>
<p><strong>Key Results</strong></p>
<ul>
<li><strong>Optimal solution identified</strong>: x₁* = 0.510, x₂* = 2.048 with strength of <strong>93.87 MPa</strong><br>
</li>
<li><strong>Fast convergence</strong>: stabilization after only 10 iterations<br>
</li>
<li><strong>Explicit quantification of uncertainty</strong> associated with decisions (σ(x) exploited in acquisition function)<br>
</li>
<li><strong>Concrete illustration of value of information</strong>: each observation reduces global uncertainty</li>
</ul>
<p><strong>Added Value</strong><br>
The project demonstrates how Bayesian stochastic optimization constitutes a robust alternative to deterministic methods when data is scarce, expensive, or noisy, while remaining <strong>interpretable and decision-oriented</strong>.<br>
Direct applications include material formulation (polymers, alloys, composites), chemical process optimization, manufacturing parameter tuning, and hyperparameter optimization in machine learning.</p>
<p><strong>Tech:</strong> <code>Python</code> · <code>R</code> · <code>NumPy</code> · <code>SciPy</code> · <code>Matplotlib</code></p>
<div class="project-actions">
<p><a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Experimental Study</a></p>
</div>
</section>
</div>
<div class="project-card">
<p><img class="project-thumb" src="../../assets/mdp-treasure2.png" alt="Optimal decision under uncertainty in risky environment"></p>
<section id="p2-optimal-decision-under-uncertainty-and-probabilistic-scenarios" class="level3 project-body">
<h3 class="anchored" data-anchor-id="p2-optimal-decision-under-uncertainty-and-probabilistic-scenarios">P2 — Optimal Decision Under Uncertainty and Probabilistic Scenarios</h3>
<p><strong>Context</strong><br>
This project falls within a <strong>sequential decision-making under risk</strong> framework, where action outcomes depend on probabilistic transitions and random shocks.<br>
The modeled scenario is that of a <strong>treasure hunter navigating dense jungle</strong>, who must reach an ancient temple (reward +1000) while <strong>minimizing exposure to unfavorable scenarios</strong>: cliffs (75% success / 25% fall, penalty -100), rivers (80% / 20%, -75), jungles (90% / 10%, -50), and negative absorbing states (Valley of Death: -150, Predators: -200).</p>
<p>The objective is to maximize expected value over the decision horizon, while explicitly managing time cost (-10 per action) and risk aversion.</p>
<p><strong>Methodological Approach</strong><br>
Modeling relies on explicit stochastic formulation as a <strong>Markov Decision Process (MDP)</strong>, integrating:</p>
<ul>
<li><strong>probabilistic scenarios</strong> representing possible outcomes of each decision, with transitions P(s’|s,a) varying by terrain,<br>
</li>
<li>an <strong>expected value function</strong> V*(s) = max_a Σ_s’ P(s’|s,a)[R(s,a,s’) + γ·V*(s’)] integrating rewards, penalties, and time cost,<br>
</li>
<li>a <strong>discount factor</strong> γ = 0.9 translating temporal preference and risk aversion.</li>
</ul>
<p>Analysis includes both <strong>exact resolution</strong> (Value Iteration, convergence in 13 iterations) and <strong>linear approximation of value function</strong> via TD Learning, allowing study of tradeoffs between precision and computational complexity:</p>
<pre><code>V(s) ≈ θ₁·f₁(s) + θ₂·f₂(s) + θ₃·f₃(s)</code></pre>
<p>with f₁ = distance to treasure, f₂ = proximity to dangers, f₃ = safe state indicator.</p>
<p><strong>Key Results</strong></p>
<ul>
<li><strong>Exact optimal policy identified</strong>: S→J1→T (optimal values V<em>(S)=1666.37, V</em>(J1)=1862.64)<br>
</li>
<li><strong>Highlighting of strategies</strong> avoiding states with high expected loss (cliffs, river)<br>
</li>
<li><strong>Marked sensitivity</strong> to discount factor: high γ favors risk-taking for final gain<br>
</li>
<li><strong>Capacity of linear approximations</strong> (TD Learning, 1000 episodes) to capture global structure: θ₃=1799.45 (safe states valued), θ₂=1619.82 (avoid dangers), θ₁=-48.75 (minimize distance)</li>
</ul>
<p><strong>Added Value</strong><br>
This work illustrates the foundations of multi-stage stochastic optimization and highlights central issues in decision-making under uncertainty: <strong>anticipation, prudence, and risk management</strong>.<br>
Direct applications include robotic path planning, financial portfolio management (risk/return), search and rescue strategies, logistics optimization under uncertainty, and autonomous navigation.</p>
<p><strong>Tech:</strong> <code>Python</code> · <code>R</code> · <code>Pandas</code> · <code>NetworkX</code> · <code>NumPy</code> · <code>Matplotlib</code></p>
<div class="project-actions">
<p><a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Decision Analysis</a></p>
</div>
</section>
</div>
<div class="project-card">
<p><img class="project-thumb" src="../../assets/pricing-policy3.png" alt="Optimal selling policy under price uncertainty"></p>
<section id="p3-optimal-selling-policy-under-price-uncertainty" class="level3 project-body">
<h3 class="anchored" data-anchor-id="p3-optimal-selling-policy-under-price-uncertainty">P3 — Optimal Selling Policy Under Price Uncertainty</h3>
<p><strong>Context</strong><br>
<strong>Dynamic pricing</strong> problem in a context of stochastic uncertainty about future asset price evolution.<br>
A seller holds an asset that can be sold at any time. Prices fluctuate randomly according to different stochastic processes (random walk, AR(3) autoregressive model). The objective is to identify the <strong>selling policy maximizing expected profit</strong>, arbitrating between immediate sale (exploitation) and waiting for a better price (exploration).</p>
<p><strong>Methodological Approach</strong><br>
Three classes of parametric policies are compared via <strong>Monte Carlo simulation</strong> (1000–5000 realizations):</p>
<ol type="1">
<li><strong>“Sell-low” policy</strong>: sell as soon as p_t ≥ θ_low (simple threshold rule)<br>
</li>
<li><strong>“High-low” policy</strong>: sell if p_t ≥ θ_high or p_t ≤ θ_low (stop-loss + take-profit)<br>
</li>
<li><strong>“Tracking” policy</strong>: sell if p_t ≥ α·p̄_t + (1-α)·θ_track (adaptive moving average)</li>
</ol>
<p>Policies are evaluated under different <strong>stochastic price models</strong>:</p>
<ul>
<li><strong>Random walk</strong>: p_{t+1} = p_t + ε, ε ~ N(0,σ²)<br>
</li>
<li><strong>AR(3) model</strong>: p_{t+1} = θ₀·p_t + θ₁·p_{t-1} + θ₂·p_{t-2} + ε (captures autocorrelation)</li>
</ul>
<p>Optimization focuses on identifying the best parameters (θ_low, θ_high, θ_track, α) for each policy and each model.</p>
<p><strong>Key Results</strong></p>
<ul>
<li><p><strong>“Tracking” policy systematically winning</strong> across all tested scenarios:</p>
<p>• Random walk (N=5000): $45.55 (vs $45.22 for others)<br>
• AR(3) with autocorrelation: $41.59 (vs $41.28–$41.59)</p></li>
<li><p><strong>Stable performance</strong> across different stochastic models (robustness)<br>
</p></li>
<li><p><strong>Impact of autocorrelation</strong>: profits reduced by ~10% under AR(3) ($41 vs $46)<br>
</p></li>
<li><p><strong>Best adaptive parameters</strong>: α ≈ 0.7–0.9 depending on context</p></li>
</ul>
<p><strong>Added Value</strong><br>
The project demonstrates the importance of <strong>adaptive policies</strong> facing stochastic uncertainty, and illustrates the <strong>simulation-based evaluation</strong> methodology to compare decision strategies.<br>
Direct applications include algorithmic trading, perishable inventory management, dynamic pricing (hospitality, transport), online auctions, and yield management.</p>
<p><strong>Tech:</strong> <code>Python</code> · <code>R</code> · <code>Monte Carlo Simulation</code> · <code>NumPy</code> · <code>Pandas</code> · <code>Matplotlib</code></p>
<div class="project-actions">
<p><a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Strategy Study</a></p>
</div>
</section>
</div>
<div class="project-card">
<p><img class="project-thumb" src="../../assets/newsvendor4.png" alt="Newsvendor problem with salvage"></p>
<section id="p4-optimal-inventory-management-under-demand-uncertainty" class="level3 project-body">
<h3 class="anchored" data-anchor-id="p4-optimal-inventory-management-under-demand-uncertainty">P4 — Optimal Inventory Management Under Demand Uncertainty</h3>
<p><strong>Context</strong><br>
Classic <strong>inventory optimization under uncertainty</strong> problem, extended to include salvage value of unsold items (price s per unit, 0 &lt; s &lt; r).<br>
A vendor must determine the <strong>optimal quantity to order</strong> (x*) before observing random demand D, knowing they incur a purchase cost c per unit, sell at price r, and can salvage unsold items at price s. The objective is to <strong>maximize expected profit</strong> by arbitrating between shortage risk (lost opportunity) and surplus risk (storage cost / devaluation).</p>
<p><strong>Methodological Approach</strong><br>
The problem is formulated as an <strong>analytical stochastic optimization model</strong>:</p>
<pre><code>max_x E[r·min(x,D) + s·max(0,x-D) - c·x]</code></pre>
<p>Resolution via gradient calculation and first-order optimality condition leads to the <strong>critical quantile formula</strong>:</p>
<pre><code>F(x*) = (c - r) / (s - r)
⟹ x* = F⁻¹[(c - r) / (s - r)]</code></pre>
<p>Two approaches are compared: 1. <strong>Optimal stochastic solution</strong> (x* = 125 units for D ~ U(50,150), c=10, r=25, s=5)<br>
2. <strong>Deterministic solution</strong> with demand fixed at E[D] = 100 (x*_det = 100 units)</p>
<p>The gap between the two quantifies the <strong>Value of the Stochastic Solution (VSS)</strong>: VSS = Π(x<em>) - Π(x</em>_det) = 1100 - 1500 = -$400.</p>
<p>A <strong>stochastic gradient algorithm</strong> is also implemented to empirically validate convergence toward x* ≈ 124 units.</p>
<p><strong>Key Results</strong></p>
<ul>
<li><strong>Analytical optimal solution</strong>: x* = 125 units (expected profit $1100)<br>
</li>
<li><strong>VSS = -$400</strong>: stochastic solution presents loss risk compared to deterministic demand fixed at 100, illustrating the <strong>importance of forecast quality</strong><br>
</li>
<li><strong>Empirical convergence</strong> of stochastic gradient toward x* ≈ 124 units<br>
</li>
<li><strong>Insensitivity to distribution bounds</strong>: formula remains valid for D ∈ [a,b] with truncated distribution</li>
</ul>
<p><strong>Added Value</strong><br>
This project illustrates fundamental concepts of stochastic optimization: <strong>cost/benefit balance under uncertainty</strong>, <strong>value of perfect information</strong>, and <strong>sensitivity analysis</strong> to parameters (c, r, s, distribution of D).<br>
Direct applications include inventory management (retail, manufacturing), capacity planning, and any decision situation before observing random demand.</p>
<p><strong>Tech:</strong> <code>Python</code> · <code>R</code> · <code>NumPy</code> · <code>Matplotlib</code></p>
<div class="project-actions">
<p><a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Theoretical Analysis</a></p>
</div>
</section>
</div>
</div>
</section>
<section id="rl" class="level2">
<h2 class="anchored" data-anchor-id="rl">Reinforcement Learning</h2>
<p>Application of <strong>reinforcement learning</strong> to a <strong>sequential control and combinatorial optimization</strong> problem, in a simulated and constrained environment, with complete implementation in <strong>native Python</strong> and standard scientific ecosystem libraries.</p>
<div class="{cards-grid}">
<div class="project-card">
<p><img class="project-thumb" src="../../assets/rlrocky1.png" alt="Bayesian optimization for experimental systems"></p>
<div class="project-body">
<section id="reinforcement-learning-for-autonomous-robot-navigation-codey-rocky" class="level3">
<h3 class="anchored" data-anchor-id="reinforcement-learning-for-autonomous-robot-navigation-codey-rocky">Reinforcement Learning for Autonomous Robot Navigation (Codey Rocky)</h3>
<p><strong>Context</strong><br>
This project addresses a variant of the <strong>Traveling Salesman Problem (TSP)</strong> in a discrete environment, formulated as a <strong>sequential decision problem under constraints</strong>.<br>
An autonomous robot (Codey Rocky) must:</p>
<ul>
<li>visit a set of colored items <strong>exactly once</strong>,<br>
</li>
<li><strong>minimize total distance traveled</strong>,<br>
</li>
<li>then <strong>return to starting point</strong>,<br>
all within a deliberately limited state space to make learning tractable.</li>
</ul>
<p>The problem is <strong>NP-hard</strong>, with rapid combinatorial explosion of possible trajectories as the number of targets increases.</p>
<p><strong>Methodological Approach</strong><br>
The environment is modeled as a <strong>Markov Decision Process (MDP)</strong> comprising:</p>
<ul>
<li>a discrete state space (128 states) although in reality, the space should normally be continuous (white zones on the map),</li>
<li>a restricted set of actions (forward, rotations),</li>
<li>a reward function combining objectives and penalties.</li>
</ul>
<p>Two approaches are implemented and compared:</p>
<ul>
<li><p><strong>Tabular Q-learning</strong><br>
Learning via direct Q-table update, serving as an interpretable reference.</p></li>
<li><p><strong>Deep Q-learning (DQN)</strong><br>
Value function approximation via a dense neural network, trained by backpropagation.</p></li>
</ul>
<p>Policies are trained using an <strong>ε-greedy</strong> strategy, with explicit control of learning rate and discount factor.</p>
<p><strong>Key Results</strong></p>
<ul>
<li>Convergence of both approaches after comparable number of episodes<br>
</li>
<li>Similar final performance in moderate state space<br>
</li>
<li>Empirical validation of deep RL limitations when structural complexity remains low</li>
</ul>
<p><strong>Critical Analysis</strong><br>
The project highlights that:</p>
<ul>
<li>deep learning does not systematically bring significant gain versus well-posed tabular methods,</li>
<li>state space structure is determinant to justify use of neural networks,</li>
<li>stability and interpretability remain key criteria in constrained environments.</li>
</ul>
<p><strong>Added Value</strong><br>
This work illustrates a <strong>rigorous implementation of reinforcement learning</strong>, from environment modeling to comparative algorithm analysis, highlighting the <strong>real conditions for deep RL relevance</strong>.</p>
<p><strong>Tech:</strong> <code>Python</code> · <code>PyTorch</code> · <code>TensorFlow</code> · <code>NumPy</code> · <code>Pandas</code> · <code>Matplotlib</code> · <code>Robotics</code></p>
<div class="project-links">
<p><a class="btn btn-sm btn-primary" href="https://youtu.be/R0ZM-iSKcVE" target="_blank">Démonstration vidéo</a></p>
</div>
<p>:::</p>
<p>:::</p>
</section>
<section id="dataeng" class="level2">
<h2 class="anchored" data-anchor-id="dataeng">Data Engineering &amp; MLOps</h2>
<p>Design and implementation of <strong>data and AI model pipelines fully deployed in production</strong>, with particular attention to <strong>cloud scalability</strong>, <strong>experimental reproducibility</strong>, and integration of <strong>advanced paradigms like federated learning</strong>.</p>
<div class="{cards-grid}">
<div class="project-card">
<p><img class="project-thumb" src="../../assets/ecoenergy1.png" alt="AWS MLOps platform and federated learning"></p>
<section id="p1-cloud-native-mlops-platform-for-smart-energy-monitoring" class="level3 project-body">
<h3 class="anchored" data-anchor-id="p1-cloud-native-mlops-platform-for-smart-energy-monitoring">P1 — Cloud-Native MLOps Platform for Smart Energy Monitoring</h3>
<p><strong>Context</strong><br>
End-to-end project aimed at designing an <strong>industrial MLOps platform fully deployed on AWS cloud</strong>, for analysis and forecasting of energy consumption of distributed systems.<br>
The operational framework imposes strong constraints: <strong>heterogeneous data</strong>, <strong>multi-site deployment</strong>, <strong>data security</strong>, and <strong>need for real-time predictions</strong>.</p>
<p>The objective is to cover the entire <strong>lifecycle of a production AI application</strong>, from data ingestion to model serving, while integrating a <strong>federated learning</strong> approach to preserve local data confidentiality.</p>
<p><strong>Cloud Architecture &amp; Deployment</strong><br>
The application is deployed <strong>end-to-end on AWS</strong>, with a service-oriented architecture:</p>
<ul>
<li><strong>Storage &amp; Data</strong>
<ul>
<li>Historical and intermediate data stored on <strong>Amazon S3</strong></li>
<li>Clear separation between raw data, features, and model artifacts</li>
</ul></li>
<li><strong>Compute &amp; Orchestration</strong>
<ul>
<li>Containerized services orchestrated via <strong>AWS Kubernetes (EKS)</strong><br>
</li>
<li>Distributed training and inference, with GPU support<br>
</li>
<li>Automatic horizontal scalability according to load</li>
</ul></li>
<li><strong>Serving &amp; Integration</strong>
<ul>
<li>Models exposed via <strong>API endpoints</strong><br>
</li>
<li>Asynchronous triggers via <strong>AWS Lambda</strong> for inference and updates<br>
</li>
<li>Architecture ready for real-time and batch use cases</li>
</ul></li>
</ul>
<p><strong>MLOps &amp; Experimentation</strong> - Experiment, metrics, and model version tracking<br>
- Status management (experimentation, validation, production)<br>
- Strict separation between data, training, and deployment pipelines</p>
<p><strong>Federated Learning (Key Element)</strong><br>
A <strong>federated learning</strong> mechanism was implemented to:</p>
<ul>
<li>Train models <strong>without centralizing sensitive data</strong><br>
</li>
<li>Aggregate weights learned locally on different nodes<br>
</li>
<li>Reduce risks related to data confidentiality and governance<br>
</li>
<li>Simulate multi-site industrial scenarios (buildings, regions, equipment)</li>
</ul>
<p>This approach constitutes a <strong>innovative lever</strong> for energy and industrial systems, where regulatory and operational constraints limit data centralization.</p>
<p><strong>Key Results</strong> - AI application <strong>fully deployed in production on AWS</strong><br>
- Functional MLOps pipeline from storage to prediction API<br>
- Validation of operation in distributed environment<br>
- Successful integration of federated learning in cloud architecture</p>
<p><strong>Added Value</strong><br>
This project demonstrates <strong>advanced mastery of Data Engineering and cloud-native MLOps</strong>, as well as ability to integrate <strong>recent research paradigms (federated learning)</strong> into realistic industrial architectures.<br>
It illustrates the complete transition <strong>from modeling to industrialization</strong>, with a system and long-term vision.</p>
<p><strong>Tech:</strong> <code>AWS (S3, Lambda, EKS, Endpoints)</code> · <code>Fast Api</code> · <code>Nginx</code> · <code>Data Engineering</code> · <code>MLOps</code> · <code>Docker</code> · <code>Kubernetes</code> · <code>GPU Computing</code> · <code>Federated Learning</code></p>
<div class="project-actions">
<p><a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Cloud &amp; MLOps Platform</a></p>
</div>
</section>
</div>
</div>
</section>
<section id="llm" class="level2">
<h2 class="anchored" data-anchor-id="llm">Natural Language Processing (NLP) &amp; LLM</h2>
<p>Advanced analysis of <strong>large-scale text corpora</strong> to extract <strong>latent semantic structures</strong>, compare international media discourses, and understand the evolution of narratives over time.</p>
<div class="cards-grid">
<div class="project-card">
<p><img class="project-thumb" src="../../assets/nlp1.png" alt="Text analysis and Topic Modeling on COVID-19"></p>
<section id="p1-comparative-analysis-of-international-media-discourses-covid-19" class="level3 project-body">
<h3 class="anchored" data-anchor-id="p1-comparative-analysis-of-international-media-discourses-covid-19">P1 — Comparative Analysis of International Media Discourses (COVID-19)</h3>
<p><strong>Context</strong><br>
Large-scale academic project conducted at HEC Montreal aimed at analyzing and comparing <strong>English-language media discourses</strong> from several countries and continents around a common theme: the <strong>COVID-19 pandemic</strong>.<br>
The objective is to identify <strong>how the same global event is treated differently according to geopolitical, cultural, and temporal contexts</strong>, beyond simple factual content.</p>
<p><strong>Methodological Approach</strong><br>
The approach relies on a complete <strong>natural language processing</strong> pipeline, combining:</p>
<ul>
<li><strong>in-depth exploratory analysis</strong> of texts (length, temporal distribution, geographic origin),<br>
</li>
<li><strong>rigorous linguistic preprocessing</strong> (cleaning, entity normalization, lexical variant management, noise reduction),<br>
</li>
<li><strong>unsupervised topic modeling</strong> allowing extraction of dominant subjects without a priori assumptions.</li>
</ul>
<p>Analysis is structured <strong>by key time periods</strong> (beginning, peak, and late phase of the pandemic), to study the evolution of media narratives over time.</p>
<p><strong>Key Results</strong> - Highlighting of <strong>distinct themes according to pandemic phases</strong> (initial reaction, public policies, socio-economic impacts, variants, easing of measures)<br>
- Marked differences between regions in <strong>topic prioritization</strong> (public health, governance, economy, society)<br>
- Topic Modeling’s ability to reveal <strong>latent discursive dynamics</strong>, difficult to observe by manual reading</p>
<p><strong>Challenges and Lessons</strong> - Strong heterogeneity of sources (editorial styles, article sizes, vocabulary)<br>
- Critical importance of <strong>preprocessing and linguistic normalization</strong> to obtain exploitable results<br>
- Necessity of constant arbitration between <strong>semantic richness</strong> and <strong>statistical stability</strong> of models</p>
<p><strong>Added Value</strong><br>
This project demonstrates <strong>advanced mastery of modern NLP techniques</strong>, with ability to transform unstructured texts into <strong>interpretable analytical indicators</strong>, useful for: - media analysis,<br>
- strategic intelligence,<br>
- social and decision sciences research.</p>
<p>It also constitutes a solid foundation for extensions toward <strong>sentiment analysis</strong>, <strong>large language models (LLMs)</strong>, and fine study of <strong>multi-country narrative dynamics</strong>.</p>
<p><strong>Tech:</strong> NLP · Topic Modeling · Text Mining · SpaCy · Gensim · NLTK · Transformers · Semantic Visualization</p>
<div class="project-actions">
<p><a class="btn btn-sm btn-primary" href="#" aria-disabled="true">Session Project</a></p>
</div>
</section>
</div>
</div>
</section>
<section id="dl-cv" class="level2">
<h2 class="anchored" data-anchor-id="dl-cv">Machine Learning &amp; Deep Learning &amp; Computer Vision</h2>
<div class="project-card">
<p><img class="project-thumb" src="../../assets/airline1.png" alt="Airline reliability prediction"></p>
<section id="p1-airline-reliability-prediction-for-commercial-routes" class="level3 project-body">
<h3 class="anchored" data-anchor-id="p1-airline-reliability-prediction-for-commercial-routes">P1 — Airline Reliability Prediction for Commercial Routes</h3>
<p><strong>Context</strong><br>
Predictive modeling project aimed at estimating <strong>airline reliability for given routes</strong>, based on vast history of North American commercial flights.<br>
Reliability is operationally defined from criteria combining <strong>delays</strong>, <strong>cancellations</strong>, and <strong>temporal variability</strong>, in a context where user decisions are highly sensitive to uncertainty.</p>
<p>The objective is to provide <strong>robust decision support</strong>, enabling comparison of airlines according to <strong>risk associated with a specific route</strong>, taking into account spatio-temporal context.</p>
<p><strong>Modeling Approach</strong><br>
The approach relies on structured supervised modeling:</p>
<ul>
<li>Formalization of the problem as <strong>reliability class prediction</strong> (low, medium, high)<br>
</li>
<li>Integration of temporal, geographic, and operational variables<br>
</li>
<li>Comparison of several classification model families<br>
</li>
<li>Analysis of tradeoffs between <strong>global accuracy</strong>, <strong>stability</strong>, and <strong>interpretability</strong><br>
</li>
<li>Rigorous evaluation in a framework respecting data temporal structure</li>
</ul>
<p>Particular attention is paid to <strong>class imbalance</strong> and its impact on prediction quality.</p>
<p><strong>Key Results</strong></p>
<ul>
<li>Ability to clearly differentiate reliability levels by airlines and routes<br>
</li>
<li>Highlighting of dominant temporal and geographic factors<br>
</li>
<li>Significant improvement over naive ranking approaches<br>
</li>
<li>Stable and consistent models on out-of-sample periods</li>
</ul>
<p><strong>Added Value</strong><br>
This project illustrates a <strong>concrete application of large-scale predictive modeling</strong>, with strong connection between <strong>statistics</strong>, <strong>machine learning</strong>, and <strong>user decision-making</strong>.<br>
It highlights the ability to transform massive data into <strong>exploitable reliability indicators</strong>.</p>
<p><strong>Tech:</strong> <code>Supervised Learning</code> · <code>Classification</code> · <code>Predictive Modeling</code> · <code>Imbalance Evaluation</code> · <code>Python</code> · <code>Scikit-learn</code> · <code>XGBoost</code> · <code>Pandas</code> · <code>Matplotlib</code></p>
</section>
</div>
<div class="project-card">
<p><img class="project-thumb" src="../../assets/montreal2.png" alt="Accident risk at Montreal intersections"></p>
<section id="p2-spatial-modeling-of-accident-risk-at-montreal-intersections" class="level3 project-body">
<h3 class="anchored" data-anchor-id="p2-spatial-modeling-of-accident-risk-at-montreal-intersections">P2 — Spatial Modeling of Accident Risk at Montreal Intersections</h3>
<p><strong>Context</strong><br>
Project applied to <strong>urban road safety</strong>, aimed at modeling and classifying <strong>accident risk level at intersections in Montreal city</strong>.<br>
Road accidents present <strong>strong spatial dependence</strong>: nearby intersections often share common characteristics related to urban environment, traffic, and infrastructure.</p>
<p>The objective is to produce a <strong>robust ranking of intersections by dangerousness</strong>, to support development and prevention decisions.</p>
<p><strong>Modeling Approach</strong><br>
The approach combines statistical modeling and spatial dependence:</p>
<ul>
<li>Modeling of accident numbers via <strong>adapted count models</strong><br>
</li>
<li>Introduction of <strong>spatial lag variables</strong><br>
</li>
<li>Construction of neighborhood structures (distance, radius, k-nearest neighbors)<br>
</li>
<li>Model comparison:
<ul>
<li>classical count models,</li>
<li>random effects models,</li>
<li>spatial models explicitly integrating geographic dependence</li>
</ul></li>
</ul>
<p>Evaluation relies on quantitative metrics and on <strong>intersection ranking stability</strong>.</p>
<p><strong>Key Results</strong></p>
<ul>
<li>Clear performance improvement through spatial dependence integration<br>
</li>
<li>Identification of high-risk urban zones<br>
</li>
<li>Consistent and interpretable intersection ranking<br>
</li>
<li>Results directly exploitable for urban planning</li>
</ul>
<p><strong>Added Value</strong><br>
This project demonstrates <strong>advanced mastery of applied spatial modeling</strong>, at the interface between <strong>machine learning</strong>, <strong>statistics</strong>, and <strong>urban data</strong>.<br>
It highlights the added value of structural models for problems with <strong>strong societal impact</strong>.</p>
<p><strong>Tech:</strong> <code>Spatial Modeling</code> · <code>Count Statistics</code> · <code>Statistical Learning</code> · <code>Geographic Data</code></p>
</section>
</div>
<div class="project-card">
<p><img class="project-thumb" src="../../assets/bixi3.jpg" alt="Analysis of BIXI bike solicitation in Montreal"></p>
<section id="p3-modeling-bixi-network-solicitation-in-montreal" class="level3 project-body">
<h3 class="anchored" data-anchor-id="p3-modeling-bixi-network-solicitation-in-montreal">P3 — Modeling BIXI Network Solicitation in Montreal</h3>
<p><strong>Context</strong><br>
Project applied to <strong>urban mobility</strong>, aimed at analyzing and modeling <strong>BIXI trip duration and intensity</strong> in Montreal, based on data from multiple stations distributed across the urban territory.<br>
Observations present a <strong>natural hierarchical structure</strong>: trips are nested within stations, themselves located in different boroughs, inducing <strong>significant intra-station correlations</strong>.</p>
<p>The objective is to understand <strong>how network solicitation varies according to spatial and temporal context</strong>, and to quantify heterogeneity between stations.</p>
<p><strong>Modeling Approach</strong><br>
The approach relies on progressive statistical modeling:</p>
<ul>
<li>Base modeling via a <strong>global linear model</strong>, integrating temporal covariates (weekday vs weekend)<br>
</li>
<li>Highlighting limitations of observation independence assumption<br>
</li>
<li>Introduction of <strong>mixed effects models</strong>, with:
<ul>
<li><strong>random intercepts by station</strong>,<br>
</li>
<li>then <strong>conditional random effects for weekends</strong>,<br>
</li>
</ul></li>
<li>Formal statistical tests (likelihood ratios, Wald tests) to evaluate:
<ul>
<li>significance of random effects,<br>
</li>
<li>inter-station variability,<br>
</li>
<li>relevance of hierarchical structures.</li>
</ul></li>
</ul>
<p>Particular attention is paid to estimation and interpretation of <strong>intra-station correlation</strong>, key indicator of trip dependence.</p>
<p><strong>Key Results</strong> - Highlighting of <strong>high intra-station correlation</strong>, invalidating independence assumption<br>
- Significant variability in average trip duration between boroughs<br>
- Positive global effect of weekend on trip duration<br>
- Low inter-station variability of “weekend” effect, justifying a more parsimonious model</p>
<p><strong>Added Value</strong><br>
This project demonstrates <strong>advanced mastery of hierarchical models and mixed effects</strong>, essential for analyzing correlated urban data.<br>
It highlights the importance of <strong>adapted structural modeling</strong> to avoid biased conclusions in shared mobility systems.</p>
<p><strong>Tech:</strong> <code>Linear Mixed Models</code> · <code>Advanced Statistics</code> · <code>Urban Mobility Data</code> · <code>Hierarchical Analysis</code> · <code>R</code> · <code>lme4</code> · <code>ggplot2</code></p>
</section>
</div>
<p>:::</p>


</section>
</div>
</div>
</div>
</section>
</div>
</div>
</section>
</div>
</div>
</section>
</div>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/mbalogogglemuel\.github\.io\/portfolio\/en\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>